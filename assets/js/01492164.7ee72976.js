"use strict";(globalThis.webpackChunkphysical_ai_robotics_website=globalThis.webpackChunkphysical_ai_robotics_website||[]).push([[294],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}},9820:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter_4_ml_algorithms","title":"Chapter 4: Machine Learning and AI Algorithms for Humanoid Robotics","description":"4.1 Introduction to Machine Learning in Humanoid Robotics","source":"@site/docs/chapter_4_ml_algorithms.md","sourceDirName":".","slug":"/chapter_4_ml_algorithms","permalink":"/physical-ai-robotics-website/docs/chapter_4_ml_algorithms","draft":false,"unlisted":false,"editUrl":"https://github.com/muaazasif/physical-ai-robotics-website/docs/chapter_4_ml_algorithms.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Visualization and Animation for Humanoid Robotics AI","permalink":"/physical-ai-robotics-website/docs/chapter_3_visualization_animation"},"next":{"title":"Chapter 5: Control Architectures and System Integration for Humanoid Robotics","permalink":"/physical-ai-robotics-website/docs/chapter_5_control_architecture"}}');var s=i(4848),o=i(8453);const a={},r="Chapter 4: Machine Learning and AI Algorithms for Humanoid Robotics",l={},c=[{value:"4.1 Introduction to Machine Learning in Humanoid Robotics",id:"41-introduction-to-machine-learning-in-humanoid-robotics",level:2},{value:"Types of Machine Learning in Humanoid Robotics",id:"types-of-machine-learning-in-humanoid-robotics",level:3},{value:"4.2 Theoretical Foundations of ML for Robotics",id:"42-theoretical-foundations-of-ml-for-robotics",level:2},{value:"4.2.1 Markov Decision Processes (MDPs)",id:"421-markov-decision-processes-mdps",level:3},{value:"4.2.2 Deep Learning in Robotics",id:"422-deep-learning-in-robotics",level:3},{value:"4.2.3 Learning from Demonstration (LfD)",id:"423-learning-from-demonstration-lfd",level:3},{value:"4.3 Reinforcement Learning for Humanoid Control",id:"43-reinforcement-learning-for-humanoid-control",level:2},{value:"4.3.1 Deep Q-Networks (DQN) and Extensions",id:"431-deep-q-networks-dqn-and-extensions",level:3},{value:"4.3.2 Policy Gradient Methods",id:"432-policy-gradient-methods",level:3},{value:"4.4 Imitation Learning and Behavior Cloning",id:"44-imitation-learning-and-behavior-cloning",level:2},{value:"4.5 Deep Learning for Perception and Control",id:"45-deep-learning-for-perception-and-control",level:2},{value:"4.5.1 Vision-Based Control",id:"451-vision-based-control",level:3},{value:"4.6 ROS2 Integration with Machine Learning",id:"46-ros2-integration-with-machine-learning",level:2},{value:"4.7 Laboratory Exercises",id:"47-laboratory-exercises",level:2},{value:"Lab Exercise 1: Reinforcement Learning Environment Setup",id:"lab-exercise-1-reinforcement-learning-environment-setup",level:3},{value:"Lab Exercise 2: Imitation Learning from Demonstrations",id:"lab-exercise-2-imitation-learning-from-demonstrations",level:3},{value:"Lab Exercise 3: Vision-Based Control System",id:"lab-exercise-3-vision-based-control-system",level:3},{value:"4.8 Advanced Topics in ML for Humanoid Robotics",id:"48-advanced-topics-in-ml-for-humanoid-robotics",level:2},{value:"4.8.1 Multi-Task Learning",id:"481-multi-task-learning",level:3},{value:"4.8.2 Transfer Learning",id:"482-transfer-learning",level:3},{value:"4.8.3 Meta-Learning",id:"483-meta-learning",level:3},{value:"4.8.4 Safe Learning",id:"484-safe-learning",level:3},{value:"4.9 Challenges and Considerations",id:"49-challenges-and-considerations",level:2},{value:"4.9.1 Data Efficiency",id:"491-data-efficiency",level:3},{value:"4.9.2 Real-Time Performance",id:"492-real-time-performance",level:3},{value:"4.9.3 Safety and Robustness",id:"493-safety-and-robustness",level:3},{value:"4.9.4 Interpretability",id:"494-interpretability",level:3},{value:"4.10 Summary",id:"410-summary",level:2},{value:"4.11 Discussion Questions",id:"411-discussion-questions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-4-machine-learning-and-ai-algorithms-for-humanoid-robotics",children:"Chapter 4: Machine Learning and AI Algorithms for Humanoid Robotics"})}),"\n",(0,s.jsx)(n.h2,{id:"41-introduction-to-machine-learning-in-humanoid-robotics",children:"4.1 Introduction to Machine Learning in Humanoid Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Machine learning (ML) and artificial intelligence (AI) are revolutionizing humanoid robotics by enabling robots to learn from experience, adapt to new situations, and perform complex tasks that are difficult to program explicitly. Unlike traditional robotics approaches that rely on pre-defined rules and mathematical models, ML algorithms allow humanoid robots to improve their performance through interaction with the environment."}),"\n",(0,s.jsx)(n.p,{children:"The application of ML in humanoid robotics faces unique challenges:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-dimensional state and action spaces"}),": Humanoid robots have many degrees of freedom"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time constraints"}),": Control decisions must be made rapidly to maintain balance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety requirements"}),": Learning algorithms must ensure robot and human safety"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Continuous action domains"}),": Most robot actions are continuous rather than discrete"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sim-to-real transfer"}),": Models trained in simulation must work in the real world"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"types-of-machine-learning-in-humanoid-robotics",children:"Types of Machine Learning in Humanoid Robotics"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Supervised Learning"}),": Learning from labeled data (e.g., pose estimation, object recognition)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reinforcement Learning"}),": Learning through trial and error with rewards (e.g., locomotion, manipulation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unsupervised Learning"}),": Discovering patterns in unlabeled data (e.g., behavior clustering, anomaly detection)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Imitation Learning"}),": Learning by observing and mimicking demonstrations (e.g., skill acquisition)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"42-theoretical-foundations-of-ml-for-robotics",children:"4.2 Theoretical Foundations of ML for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"421-markov-decision-processes-mdps",children:"4.2.1 Markov Decision Processes (MDPs)"}),"\n",(0,s.jsx)(n.p,{children:"MDPs provide the mathematical framework for sequential decision-making in robotics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Space (S)"}),": All possible states the robot can be in"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Space (A)"}),": All possible actions the robot can take"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transition Model (T)"}),": Probability of transitioning to next state given current state and action"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward Function (R)"}),": Immediate reward for taking an action in a state"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Discount Factor (\u03b3)"}),": Weight for future rewards"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots, states often include joint positions, velocities, sensor readings, and environment information. Actions include joint torques, desired positions, or high-level commands."}),"\n",(0,s.jsx)(n.h3,{id:"422-deep-learning-in-robotics",children:"4.2.2 Deep Learning in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Deep learning has transformed robotics by enabling end-to-end learning of complex behaviors:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Convolutional Neural Networks (CNNs)"})," for perception: Processing visual and sensory data\n",(0,s.jsx)(n.strong,{children:"Recurrent Neural Networks (RNNs)"})," for temporal sequences: Learning from time-series sensor data\n",(0,s.jsx)(n.strong,{children:"Deep Reinforcement Learning (DRL)"})," for control: Learning complex motor skills"]}),"\n",(0,s.jsx)(n.h3,{id:"423-learning-from-demonstration-lfd",children:"4.2.3 Learning from Demonstration (LfD)"}),"\n",(0,s.jsx)(n.p,{children:"LfD allows robots to learn skills by observing human demonstrations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Behavioral Cloning"}),": Directly mapping observations to actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Inverse Optimal Control"}),": Learning the underlying reward function"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamics Modeling"}),": Learning the movement dynamics"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"43-reinforcement-learning-for-humanoid-control",children:"4.3 Reinforcement Learning for Humanoid Control"}),"\n",(0,s.jsx)(n.h3,{id:"431-deep-q-networks-dqn-and-extensions",children:"4.3.1 Deep Q-Networks (DQN) and Extensions"}),"\n",(0,s.jsx)(n.p,{children:"DQN and its variants have shown success in learning humanoid behaviors:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nDeep Q-Network implementation for humanoid robot control\nDemonstrates basic DQN training for simple locomotion task\n"""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nfrom collections import deque\nimport gym\n\nclass DQN(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=256):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, hidden_size)\n        self.fc4 = nn.Linear(hidden_size, action_size)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\nclass HumanoidDQNAgent:\n    def __init__(self, state_size, action_size, lr=1e-3):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=10000)\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = lr\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        \n        # Neural networks\n        self.q_network = DQN(state_size, action_size).to(self.device)\n        self.target_network = DQN(state_size, action_size).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        \n        # Update target network\n        self.update_target_network()\n        \n    def update_target_network(self):\n        """Update target network with current network weights"""\n        self.target_network.load_state_dict(self.q_network.state_dict())\n    \n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay memory"""\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def act(self, state, training=True):\n        """Choose action using epsilon-greedy policy"""\n        if training and np.random.random() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n    \n    def replay(self, batch_size=32):\n        """Train the model on a batch of experiences"""\n        if len(self.memory) < batch_size:\n            return\n        \n        batch = random.sample(self.memory, batch_size)\n        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)\n        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)\n        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)\n        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)\n        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)\n        \n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (0.99 * next_q_values * ~dones)\n        \n        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\nclass SimpleHumanoidEnv:\n    """Simple simulation environment for humanoid learning"""\n    def __init__(self):\n        self.state_size = 12  # 6 joint positions + 6 joint velocities\n        self.action_size = 4  # 4 discretized actions: move forward, backward, balance, step\n        self.reset()\n    \n    def reset(self):\n        """Reset environment to initial state"""\n        # Random initial joint positions and velocities\n        self.state = np.random.uniform(-0.1, 0.1, self.state_size)\n        self.time_step = 0\n        self.max_steps = 200\n        return self.state\n    \n    def step(self, action):\n        """Execute action and return (next_state, reward, done, info)"""\n        # Simple physics simulation\n        joint_positions = self.state[:6]\n        joint_velocities = self.state[6:]\n        \n        # Apply action (simplified)\n        if action == 0:  # Move forward\n            joint_velocities[0] += 0.05\n        elif action == 1:  # Move backward\n            joint_velocities[0] -= 0.05\n        elif action == 2:  # Balance\n            # Try to keep joints near neutral position\n            joint_positions *= 0.95\n        elif action == 3:  # Step\n            joint_positions[1] += 0.1  # Simple stepping motion\n        \n        # Update state with some physics\n        new_positions = joint_positions + joint_velocities * 0.02\n        new_velocities = joint_velocities * 0.98  # Damping\n        \n        self.state = np.concatenate([new_positions, new_velocities])\n        \n        # Calculate reward (encourage balanced, forward motion)\n        balance_reward = 1.0 - min(abs(self.state[0]), 1.0)  # Stay balanced\n        forward_reward = max(0, self.state[0]) * 0.1  # Move forward\n        stability_reward = max(0, 1.0 - np.std(self.state[6:]))  # Stable movement\n        \n        reward = balance_reward + forward_reward + stability_reward\n        \n        self.time_step += 1\n        done = self.time_step >= self.max_steps or abs(self.state[0]) > 1.5\n        \n        return self.state, reward, done, {}\n\ndef train_dqn_agent():\n    """Train DQN agent on simple humanoid environment"""\n    env = SimpleHumanoidEnv()\n    agent = HumanoidDQNAgent(env.state_size, env.action_size)\n    \n    episodes = 1000\n    scores = deque(maxlen=100)\n    \n    for e in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        \n        for time_step in range(env.max_steps):\n            action = agent.act(state)\n            next_state, reward, done, _ = env.step(action)\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n            \n            if done:\n                break\n        \n        scores.append(total_reward)\n        \n        # Train the agent\n        if len(agent.memory) > 32:\n            agent.replay(32)\n        \n        # Update target network every 100 episodes\n        if e % 100 == 0:\n            agent.update_target_network()\n        \n        if e % 100 == 0:\n            avg_score = np.mean(scores)\n            print(f"Episode {e}, Average Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.3f}")\n    \n    print("Training completed!")\n\nif __name__ == "__main__":\n    train_dqn_agent()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"432-policy-gradient-methods",children:"4.3.2 Policy Gradient Methods"}),"\n",(0,s.jsx)(n.p,{children:"Policy gradient methods optimize the policy directly, making them suitable for continuous control:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nPolicy Gradient implementation for humanoid continuous control\nUses Actor-Critic method for stable learning\n"""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=256):\n        super(ActorNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, hidden_size)\n        \n        # Mean and std of action distribution\n        self.action_mean = nn.Linear(hidden_size, action_size)\n        self.action_std = nn.Linear(hidden_size, action_size)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        \n        mean = torch.tanh(self.action_mean(x))  # Actions in [-1, 1]\n        std = torch.clamp(torch.exp(self.action_std(x)), 0.01, 1.0)  # Clamped std\n        \n        return mean, std\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, state_size, hidden_size=256):\n        super(CriticNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, hidden_size)\n        self.fc4 = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\nclass PPOAgent:\n    def __init__(self, state_size, action_size, lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=4):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        \n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n        \n        self.actor = ActorNetwork(state_size, action_size).to(self.device)\n        self.critic = CriticNetwork(state_size).to(self.device)\n        \n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n        \n    def select_action(self, state):\n        """Select action using current policy"""\n        state = torch.FloatTensor(state).to(self.device)\n        \n        with torch.no_grad():\n            mean, std = self.actor(state)\n            dist = Normal(mean, std)\n            action = dist.sample()\n            action_logprob = dist.log_prob(action)\n        \n        return action.cpu().numpy(), action_logprob.cpu().numpy()\n    \n    def update(self, memory):\n        """Update policy using PPO algorithm"""\n        # Monte Carlo estimate of state rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n        \n        # Normalizing the rewards\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n        \n        # Convert to tensors\n        old_states = torch.FloatTensor(memory.states).to(self.device)\n        old_actions = torch.FloatTensor(memory.actions).to(self.device)\n        old_logprobs = torch.FloatTensor(memory.logprobs).to(self.device)\n        \n        # Optimize policy for K epochs\n        for _ in range(self.k_epochs):\n            # Evaluate old actions and values\n            mean, std = self.actor(old_states)\n            dist = Normal(mean, std)\n            logprobs = dist.log_prob(old_actions)\n            state_values = self.critic(old_states)\n            \n            # Calculate ratios\n            ratios = torch.exp(logprobs - old_logprobs.detach())\n            \n            # Calculate advantages\n            advantages = rewards - state_values.detach()\n            \n            # Calculate surrogate losses\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n            \n            # Actor loss\n            actor_loss = -torch.min(surr1, surr2).mean()\n            \n            # Critic loss\n            critic_loss = F.mse_loss(state_values.squeeze(), rewards)\n            \n            # Update networks\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n            \n            self.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic_optimizer.step()\n\nclass Memory:\n    def __init__(self):\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = []\n        self.is_terminals = []\n    \n    def clear_memory(self):\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.is_terminals[:]\n\nclass ContinuousHumanoidEnv:\n    """Environment with continuous action space"""\n    def __init__(self):\n        self.state_size = 12  # 6 joint positions + 6 velocities\n        self.action_size = 6  # Continuous joint control\n        self.reset()\n    \n    def reset(self):\n        self.state = np.random.uniform(-0.1, 0.1, self.state_size)\n        self.time_step = 0\n        self.max_steps = 200\n        return self.state\n    \n    def step(self, action):\n        # Scale action to reasonable ranges\n        action_scaled = np.clip(action, -1.0, 1.0) * 0.1  # Small control steps\n        \n        # Simple integration of joint control\n        joint_positions = self.state[:6]\n        joint_velocities = self.state[6:]\n        \n        new_velocities = joint_velocities + action_scaled * 0.02\n        new_positions = joint_positions + new_velocities * 0.02\n        \n        self.state = np.concatenate([new_positions, new_velocities])\n        \n        # Reward based on balance and movement\n        balance_reward = 1.0 - min(np.abs(self.state[0:3]).max(), 1.0)\n        stability_reward = 1.0 - np.std(self.state[6:]) * 2.0\n        \n        reward = balance_reward + stability_reward * 0.5\n        \n        self.time_step += 1\n        done = self.time_step >= self.max_steps or np.abs(self.state[:3]).max() > 1.5\n        \n        return self.state, reward, done, {}\n\ndef train_ppo_agent():\n    """Train PPO agent on continuous humanoid environment"""\n    env = ContinuousHumanoidEnv()\n    agent = PPOAgent(env.state_size, env.action_size)\n    memory = Memory()\n    \n    episodes = 1000\n    max_timesteps = 200\n    update_timestep = 2000  # Update policy every 2000 timesteps\n    \n    timestep = 0\n    \n    for i_episode in range(episodes):\n        state = env.reset()\n        \n        for t in range(max_timesteps):\n            timestep += 1\n            \n            # Select action\n            action, action_logprob = agent.select_action(state)\n            \n            # Take action\n            state, reward, done, _ = env.step(action)\n            \n            # Store in memory\n            memory.states.append(state)\n            memory.actions.append(action)\n            memory.logprobs.append(action_logprob)\n            memory.rewards.append(reward)\n            memory.is_terminals.append(done)\n            \n            if done:\n                break\n        \n        # Update policy if enough timesteps have passed\n        if timestep % update_timestep == 0:\n            agent.update(memory)\n            memory.clear_memory()\n            timestep = 0\n        \n        if i_episode % 100 == 0:\n            print(f"Episode {i_episode}, Completed")\n\nif __name__ == "__main__":\n    train_ppo_agent()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"44-imitation-learning-and-behavior-cloning",children:"4.4 Imitation Learning and Behavior Cloning"}),"\n",(0,s.jsx)(n.p,{children:"Imitation learning enables robots to learn complex behaviors by observing demonstrations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nImitation Learning implementation for humanoid skill acquisition\nDemonstrates behavior cloning from expert demonstrations\n"""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nimport os\n\nclass ImitationNet(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=256):\n        super(ImitationNet, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, hidden_size)\n        self.fc4 = nn.Linear(hidden_size, action_size)\n        \n        # Add batch normalization for stable training\n        self.bn1 = nn.BatchNorm1d(hidden_size)\n        self.bn2 = nn.BatchNorm1d(hidden_size)\n        self.bn3 = nn.BatchNorm1d(hidden_size)\n        \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.fc1(x)))\n        x = torch.relu(self.bn2(self.fc2(x)))\n        x = torch.relu(self.bn3(self.fc3(x)))\n        return self.fc4(x)  # Output continuous actions\n\nclass ImitationLearningAgent:\n    def __init__(self, state_size, action_size, lr=1e-3):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        \n        self.net = ImitationNet(state_size, action_size).to(self.device)\n        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        \n    def train(self, states, actions, epochs=100, batch_size=64):\n        """Train the imitation learning model"""\n        dataset = torch.utils.data.TensorDataset(\n            torch.FloatTensor(states).to(self.device),\n            torch.FloatTensor(actions).to(self.device)\n        )\n        \n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        \n        self.net.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch_states, batch_actions in dataloader:\n                self.optimizer.zero_grad()\n                \n                predicted_actions = self.net(batch_states)\n                loss = self.criterion(predicted_actions, batch_actions)\n                \n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n            \n            if epoch % 20 == 0:\n                print(f"Epoch {epoch}, Loss: {total_loss/len(dataloader):.6f}")\n    \n    def predict(self, state):\n        """Predict action for given state"""\n        self.net.eval()\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            action = self.net(state_tensor).cpu().numpy()[0]\n        return action\n    \n    def save_model(self, path):\n        """Save trained model"""\n        torch.save(self.net.state_dict(), path)\n    \n    def load_model(self, path):\n        """Load trained model"""\n        self.net.load_state_dict(torch.load(path, map_location=self.device))\n\ndef generate_demonstration_data(env, num_demonstrations=1000):\n    """Generate demonstration data from random expert-like actions"""\n    states = []\n    actions = []\n    \n    for _ in range(num_demonstrations):\n        state = env.reset()\n        action = np.random.uniform(-0.1, 0.1, env.action_size)  # Random expert action\n        \n        next_state, reward, done, _ = env.step(action)\n        \n        states.append(state)\n        actions.append(action)\n        \n        if done:\n            env.reset()\n    \n    return np.array(states), np.array(actions)\n\ndef train_imitation_model():\n    """Train imitation learning model"""\n    env = ContinuousHumanoidEnv()  # Using the environment from previous example\n    \n    # Generate demonstration data\n    print("Generating demonstration data...")\n    states, actions = generate_demonstration_data(env, num_demonstrations=5000)\n    \n    # Create and train agent\n    agent = ImitationLearningAgent(env.state_size, env.action_size)\n    \n    print("Training imitation model...")\n    agent.train(states, actions, epochs=200)\n    \n    # Save the trained model\n    agent.save_model("imitation_model.pth")\n    print("Model saved as imitation_model.pth")\n    \n    # Test the trained model\n    print("\\nTesting trained model...")\n    test_env = ContinuousHumanoidEnv()\n    state = test_env.reset()\n    total_reward = 0\n    \n    for step in range(100):\n        action = agent.predict(state)\n        state, reward, done, _ = test_env.step(action)\n        total_reward += reward\n        if done:\n            break\n    \n    print(f"Test episode reward: {total_reward}")\n\nif __name__ == "__main__":\n    train_imitation_model()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"45-deep-learning-for-perception-and-control",children:"4.5 Deep Learning for Perception and Control"}),"\n",(0,s.jsx)(n.h3,{id:"451-vision-based-control",children:"4.5.1 Vision-Based Control"}),"\n",(0,s.jsx)(n.p,{children:"Deep learning enables robots to use visual information for control:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nVision-based control using CNNs for humanoid robotics\nDemonstrates end-to-end learning from camera images\n"""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport cv2\nimport random\n\nclass VisionNet(nn.Module):\n    def __init__(self, action_size):\n        super(VisionNet, self).__init__()\n        \n        # Convolutional layers for visual processing\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        \n        # Calculate the size of flattened feature map\n        # Assuming input image is 84x84x3\n        self.feature_size = self._get_conv_output_size((3, 84, 84))\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(self.feature_size, 512)\n        self.fc2 = nn.Linear(512, action_size)\n        \n    def _get_conv_output_size(self, shape):\n        """Calculate the output size of the convolutional layers"""\n        x = torch.zeros(1, *shape)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        return int(np.prod(x.size()))\n    \n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        \n        x = x.view(x.size(0), -1)  # Flatten\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nclass VisionControlAgent:\n    def __init__(self, action_size, lr=1e-4):\n        self.action_size = action_size\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        \n        self.net = VisionNet(action_size).to(self.device)\n        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        \n    def preprocess_image(self, image):\n        """Preprocess camera image for network input"""\n        # Convert to grayscale and resize\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        resized = cv2.resize(gray, (84, 84))\n        \n        # Normalize pixel values\n        normalized = resized / 255.0\n        \n        # Add batch and channel dimensions\n        return np.expand_dims(normalized, axis=[0, -1]).transpose(0, 3, 1, 2)\n    \n    def select_action(self, image):\n        """Select action based on input image"""\n        self.net.eval()\n        with torch.no_grad():\n            processed_image = self.preprocess_image(image)\n            image_tensor = torch.FloatTensor(processed_image).to(self.device)\n            action_values = self.net(image_tensor)\n            action = torch.argmax(action_values, dim=1).item()\n        return action\n\ndef simulate_camera_image():\n    """Simulate camera image for vision-based control"""\n    # Create a synthetic image with simple patterns\n    image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n    \n    # Add some geometric shapes to provide visual features\n    cv2.rectangle(image, (50, 50), (100, 100), (255, 0, 0), -1)  # Blue square\n    cv2.circle(image, (150, 150), 30, (0, 255, 0), -1)  # Green circle\n    cv2.line(image, (0, 200), (224, 200), (0, 0, 255), 2)  # Red line\n    \n    return image\n\ndef test_vision_control():\n    """Test vision-based control system"""\n    agent = VisionControlAgent(action_size=4)  # 4 actions: forward, backward, left, right\n    \n    print("Testing vision-based control...")\n    \n    # Simulate a few steps of vision-based control\n    for step in range(10):\n        # Simulate camera input\n        camera_image = simulate_camera_image()\n        \n        # Select action based on image\n        action = agent.select_action(camera_image)\n        \n        action_names = ["Move Forward", "Move Backward", "Turn Left", "Turn Right"]\n        print(f"Step {step + 1}: Action selected - {action_names[action]}")\n        \n        # In a real implementation, this would control the robot\n        # and receive new images as feedback\n\nif __name__ == "__main__":\n    test_vision_control()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"46-ros2-integration-with-machine-learning",children:"4.6 ROS2 Integration with Machine Learning"}),"\n",(0,s.jsx)(n.p,{children:"Integrating machine learning models with ROS2 for real-time humanoid robotics:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nROS2 node for machine learning inference in humanoid robotics\nIntegrates trained models with ROS2 for real-time control\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float64MultiArray\nfrom sensor_msgs.msg import JointState, Image\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\nimport cv2\n\nclass MLInferenceNode(Node):\n    def __init__(self):\n        super().__init__(\'ml_inference_node\')\n        \n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n        \n        # Subscribers\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            \'/joint_states\',\n            self.joint_state_callback,\n            10\n        )\n        \n        self.camera_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.camera_callback,\n            10\n        )\n        \n        # Publishers\n        self.action_pub = self.create_publisher(Float64MultiArray, \'/robot_actions\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        \n        # Initialize ML model (example: simple policy network)\n        self.policy_network = self.load_policy_model()\n        \n        # Robot state\n        self.current_joint_positions = []\n        self.current_joint_velocities = []\n        self.camera_image = None\n        \n        # Timer for inference\n        self.timer = self.create_timer(0.05, self.inference_loop)  # 20 Hz\n        \n        self.get_logger().info(\'ML Inference Node Started\')\n\n    def load_policy_model(self):\n        """Load trained policy model"""\n        # In a real implementation, this would load a saved model\n        # For example: return torch.load(\'trained_policy.pth\')\n        \n        # For this example, create a simple dummy model\n        class DummyPolicy:\n            def predict(self, state):\n                # Dummy policy: return random action\n                return np.random.uniform(-1, 1, 6).astype(np.float32)\n        \n        return DummyPolicy()\n    \n    def joint_state_callback(self, msg):\n        """Update joint state from sensor feedback"""\n        self.current_joint_positions = list(msg.position)\n        self.current_joint_velocities = list(msg.velocity)\n    \n    def camera_callback(self, msg):\n        """Process camera image"""\n        try:\n            self.camera_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f\'Could not convert image: {e}\')\n    \n    def prepare_state_vector(self):\n        """Prepare state vector for ML inference"""\n        # Combine joint positions and velocities\n        if (len(self.current_joint_positions) > 0 and \n            len(self.current_joint_velocities) > 0):\n            \n            # Example: use first 12 joint states for a 6-DOF system\n            joint_data = (self.current_joint_positions[:6] + \n                         self.current_joint_velocities[:6])\n            return np.array(joint_data, dtype=np.float32)\n        else:\n            # Return neutral pose if no joint data available\n            return np.zeros(12, dtype=np.float32)\n    \n    def inference_loop(self):\n        """Main inference loop"""\n        # Prepare state for inference\n        state = self.prepare_state_vector()\n        \n        # Get action from policy\n        action = self.policy_network.predict(state)\n        \n        # Publish action\n        action_msg = Float64MultiArray()\n        action_msg.data = action.tolist()\n        self.action_pub.publish(action_msg)\n        \n        # Also publish as Twist for basic navigation\n        twist_msg = Twist()\n        twist_msg.linear.x = action[0] * 0.5  # Scale action to velocity\n        twist_msg.linear.y = action[1] * 0.5\n        twist_msg.angular.z = action[2] * 0.5\n        self.cmd_vel_pub.publish(twist_msg)\n        \n        self.get_logger().debug(f\'Inference: {action[:3]}\')  # Log first 3 actions\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ml_node = MLInferenceNode()\n    \n    try:\n        rclpy.spin(ml_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ml_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"47-laboratory-exercises",children:"4.7 Laboratory Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"lab-exercise-1-reinforcement-learning-environment-setup",children:"Lab Exercise 1: Reinforcement Learning Environment Setup"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Set up and train a reinforcement learning agent for basic humanoid locomotion."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Equipment Required"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Computer with Python and PyTorch installed"}),"\n",(0,s.jsx)(n.li,{children:"Mujoco or PyBullet physics simulator"}),"\n",(0,s.jsx)(n.li,{children:"ROS2 environment"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Install and configure MuJoCo or PyBullet simulator"}),"\n",(0,s.jsx)(n.li,{children:"Create humanoid robot model with appropriate sensors and actuators"}),"\n",(0,s.jsx)(n.li,{children:"Implement reward function for locomotion (forward progress, energy efficiency, balance)"}),"\n",(0,s.jsx)(n.li,{children:"Train DDPG or PPO agent for walking"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate trained agent in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Analyze learning curves and robot behavior"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"lab-exercise-2-imitation-learning-from-demonstrations",children:"Lab Exercise 2: Imitation Learning from Demonstrations"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Implement imitation learning to teach a humanoid robot a manipulation skill."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Record human demonstrations of a simple task (e.g., reaching, grasping)"}),"\n",(0,s.jsx)(n.li,{children:"Preprocess demonstration data to extract relevant features"}),"\n",(0,s.jsx)(n.li,{children:"Implement behavior cloning network"}),"\n",(0,s.jsx)(n.li,{children:"Train on demonstration dataset"}),"\n",(0,s.jsx)(n.li,{children:"Test learned policy on physical or simulated robot"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate performance and compare to expert demonstrations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"lab-exercise-3-vision-based-control-system",children:"Lab Exercise 3: Vision-Based Control System"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Develop a vision-based control system for humanoid navigation."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up camera system and image acquisition"}),"\n",(0,s.jsx)(n.li,{children:"Implement CNN-based obstacle detection"}),"\n",(0,s.jsx)(n.li,{children:"Create navigation policy using visual input"}),"\n",(0,s.jsx)(n.li,{children:"Integrate with ROS2 navigation stack"}),"\n",(0,s.jsx)(n.li,{children:"Test in simulated and real environments"}),"\n",(0,s.jsx)(n.li,{children:"Analyze robustness to different lighting conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"48-advanced-topics-in-ml-for-humanoid-robotics",children:"4.8 Advanced Topics in ML for Humanoid Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"481-multi-task-learning",children:"4.8.1 Multi-Task Learning"}),"\n",(0,s.jsx)(n.p,{children:"Training robots to perform multiple tasks simultaneously using shared representations."}),"\n",(0,s.jsx)(n.h3,{id:"482-transfer-learning",children:"4.8.2 Transfer Learning"}),"\n",(0,s.jsx)(n.p,{children:"Applying knowledge learned in one domain to similar but different tasks."}),"\n",(0,s.jsx)(n.h3,{id:"483-meta-learning",children:"4.8.3 Meta-Learning"}),"\n",(0,s.jsx)(n.p,{children:"Enabling robots to learn new skills quickly from few examples."}),"\n",(0,s.jsx)(n.h3,{id:"484-safe-learning",children:"4.8.4 Safe Learning"}),"\n",(0,s.jsx)(n.p,{children:"Ensuring safety during the learning process in real-world deployments."}),"\n",(0,s.jsx)(n.h2,{id:"49-challenges-and-considerations",children:"4.9 Challenges and Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"491-data-efficiency",children:"4.9.1 Data Efficiency"}),"\n",(0,s.jsx)(n.p,{children:"ML algorithms often require extensive training data. For humanoid robots, this can mean thousands of hours of interaction. Techniques like domain randomization and sim-to-real transfer help address this."}),"\n",(0,s.jsx)(n.h3,{id:"492-real-time-performance",children:"4.9.2 Real-Time Performance"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots require fast decision-making. Efficient neural network architectures and optimization techniques are crucial for real-time performance."}),"\n",(0,s.jsx)(n.h3,{id:"493-safety-and-robustness",children:"4.9.3 Safety and Robustness"}),"\n",(0,s.jsx)(n.p,{children:"ML-based controllers must be robust to unexpected situations and ensure safety. This includes proper handling of edge cases and failure modes."}),"\n",(0,s.jsx)(n.h3,{id:"494-interpretability",children:"4.9.4 Interpretability"}),"\n",(0,s.jsx)(n.p,{children:"Understanding and interpreting ML-based behaviors is important for debugging and trust. Techniques like attention mechanisms and explainable AI help."}),"\n",(0,s.jsx)(n.h2,{id:"410-summary",children:"4.10 Summary"}),"\n",(0,s.jsx)(n.p,{children:"Machine learning and AI algorithms are transforming humanoid robotics by enabling adaptive, learning-based control systems. This chapter has covered fundamental concepts including reinforcement learning, imitation learning, and deep learning applications specifically for humanoid robots. The examples demonstrate practical implementations of these concepts with code examples and integration strategies."}),"\n",(0,s.jsx)(n.p,{children:"The future of humanoid robotics lies in the seamless integration of learning algorithms that enable robots to continuously improve their performance and adapt to new situations. As these systems become more sophisticated, they will play an increasingly important role in practical applications."}),"\n",(0,s.jsx)(n.h2,{id:"411-discussion-questions",children:"4.11 Discussion Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the main advantages of reinforcement learning over traditional control methods for humanoid robotics?"}),"\n",(0,s.jsx)(n.li,{children:"How can we ensure safety when deploying learning-based controllers on physical humanoid robots?"}),"\n",(0,s.jsx)(n.li,{children:"What are the challenges of applying transfer learning from simulation to real robots?"}),"\n",(0,s.jsx)(n.li,{children:"How might multi-modal learning (visual, tactile, proprioceptive) improve humanoid robot performance?"}),"\n",(0,s.jsx)(n.li,{children:"What role does hardware design play in enabling effective machine learning for humanoid robots?"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);
"use strict";(globalThis.webpackChunkphysical_ai_robotics_website=globalThis.webpackChunkphysical_ai_robotics_website||[]).push([[9],{5644:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter_2_physical_ai","title":"Chapter 2: Physical AI for Humanoid Robotics","description":"2.1 Introduction to Physical AI in Humanoid Robotics","source":"@site/docs/chapter_2_physical_ai.md","sourceDirName":".","slug":"/chapter_2_physical_ai","permalink":"/physical-ai-robotics-website/docs/chapter_2_physical_ai","draft":false,"unlisted":false,"editUrl":"https://github.com/muaazasif/physical-ai-robotics-website/docs/chapter_2_physical_ai.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to Humanoid Robotics","permalink":"/physical-ai-robotics-website/docs/chapter_1_introduction"},"next":{"title":"Chapter 3: Visualization and Animation for Humanoid Robotics AI","permalink":"/physical-ai-robotics-website/docs/chapter_3_visualization_animation"}}');var o=t(4848),a=t(8453);const s={},r="Chapter 2: Physical AI for Humanoid Robotics",l={},c=[{value:"2.1 Introduction to Physical AI in Humanoid Robotics",id:"21-introduction-to-physical-ai-in-humanoid-robotics",level:2},{value:"Key Components of Physical AI in Humanoid Robotics",id:"key-components-of-physical-ai-in-humanoid-robotics",level:3},{value:"2.2 Theoretical Foundations",id:"22-theoretical-foundations",level:2},{value:"2.2.1 Robot Kinematics and Dynamics",id:"221-robot-kinematics-and-dynamics",level:3},{value:"2.2.2 Control Theory in Physical AI",id:"222-control-theory-in-physical-ai",level:3},{value:"2.2.3 Sensor Fusion and State Estimation",id:"223-sensor-fusion-and-state-estimation",level:3},{value:"2.2.4 Motion Planning for Humanoid Robots",id:"224-motion-planning-for-humanoid-robots",level:3},{value:"2.3 Simple Examples",id:"23-simple-examples",level:2},{value:"Example 1: Balance Control with Inverted Pendulum Model",id:"example-1-balance-control-with-inverted-pendulum-model",level:3},{value:"Example 2: Basic Path Planning with Potential Fields",id:"example-2-basic-path-planning-with-potential-fields",level:3},{value:"2.4 ROS2 Python Implementation Examples",id:"24-ros2-python-implementation-examples",level:2},{value:"Example 3: ROS2 Node for Joint Control",id:"example-3-ros2-node-for-joint-control",level:3},{value:"Example 4: ROS2 Perception Node for Object Detection",id:"example-4-ros2-perception-node-for-object-detection",level:3},{value:"2.5 Laboratory Exercises",id:"25-laboratory-exercises",level:2},{value:"Lab Exercise 1: Balance Control Implementation",id:"lab-exercise-1-balance-control-implementation",level:3},{value:"Lab Exercise 2: Inverse Kinematics for Arm Control",id:"lab-exercise-2-inverse-kinematics-for-arm-control",level:3},{value:"Lab Exercise 3: Path Planning and Navigation",id:"lab-exercise-3-path-planning-and-navigation",level:3},{value:"Lab Exercise 4: Sensor Fusion for State Estimation",id:"lab-exercise-4-sensor-fusion-for-state-estimation",level:3},{value:"2.6 Summary",id:"26-summary",level:2},{value:"2.7 Discussion Questions",id:"27-discussion-questions",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-2-physical-ai-for-humanoid-robotics",children:"Chapter 2: Physical AI for Humanoid Robotics"})}),"\n",(0,o.jsx)(e.h2,{id:"21-introduction-to-physical-ai-in-humanoid-robotics",children:"2.1 Introduction to Physical AI in Humanoid Robotics"}),"\n",(0,o.jsx)(e.p,{children:"Physical AI represents the integration of artificial intelligence with physical systems, enabling robots to perceive, reason, and act in the real world. For humanoid robotics, Physical AI encompasses the algorithms and systems that allow human-like robots to understand their environment, plan movements, and interact with objects and humans safely and effectively."}),"\n",(0,o.jsx)(e.p,{children:"Unlike traditional AI that operates in virtual environments, Physical AI must contend with real-world constraints such as physics, sensor noise, actuator limitations, and the need for real-time responses. Humanoid robots face additional challenges due to their complex multi-degree-of-freedom systems, balance requirements, and anthropomorphic design goals."}),"\n",(0,o.jsx)(e.h3,{id:"key-components-of-physical-ai-in-humanoid-robotics",children:"Key Components of Physical AI in Humanoid Robotics"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Perception Systems"}),": Visual, auditory, tactile, and proprioceptive sensing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Motion Planning"}),": Trajectory generation and path planning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Control Systems"}),": Low-level motor control and high-level behavior control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Learning Systems"}),": Adaptation and improvement through experience"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Interaction Systems"}),": Human-robot interaction and social understanding"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"22-theoretical-foundations",children:"2.2 Theoretical Foundations"}),"\n",(0,o.jsx)(e.h3,{id:"221-robot-kinematics-and-dynamics",children:"2.2.1 Robot Kinematics and Dynamics"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots operate in 3D space with complex kinematic chains. Understanding forward and inverse kinematics is crucial for controlling limb movements."}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Forward Kinematics"}),": Given joint angles, calculate the position and orientation of the end-effector.\n",(0,o.jsx)(e.strong,{children:"Inverse Kinematics"}),": Given desired end-effector pose, calculate required joint angles."]}),"\n",(0,o.jsx)(e.p,{children:"For humanoid systems, we often deal with redundant systems where multiple joint configurations can achieve the same end-effector pose. This redundancy can be exploited for secondary objectives like obstacle avoidance or energy efficiency."}),"\n",(0,o.jsx)(e.h3,{id:"222-control-theory-in-physical-ai",children:"2.2.2 Control Theory in Physical AI"}),"\n",(0,o.jsx)(e.p,{children:"Control systems in humanoid robotics must handle:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Feedback Control"}),": Real-time adjustments based on sensor data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Feedforward Control"}),": Predictive control based on desired trajectories"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Hierarchical Control"}),": Multi-level control architecture from high-level goals to low-level joint commands"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"223-sensor-fusion-and-state-estimation",children:"2.2.3 Sensor Fusion and State Estimation"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots utilize multiple sensors to estimate their state (position, velocity, orientation). Common approaches include:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Kalman Filters for linear systems"}),"\n",(0,o.jsx)(e.li,{children:"Extended Kalman Filters for non-linear systems"}),"\n",(0,o.jsx)(e.li,{children:"Particle Filters for multi-modal distributions"}),"\n",(0,o.jsx)(e.li,{children:"Complementary filters for combining different sensor types"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"224-motion-planning-for-humanoid-robots",children:"2.2.4 Motion Planning for Humanoid Robots"}),"\n",(0,o.jsx)(e.p,{children:"Motion planning in humanoid robotics involves:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Configuration Space"}),": The space of all possible robot configurations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Collision Avoidance"}),": Ensuring the robot doesn't collide with obstacles"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dynamic Balance"}),": Maintaining stability during movement"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Human-like Motion"}),": Generating natural, anthropomorphic movements"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"23-simple-examples",children:"2.3 Simple Examples"}),"\n",(0,o.jsx)(e.h3,{id:"example-1-balance-control-with-inverted-pendulum-model",children:"Example 1: Balance Control with Inverted Pendulum Model"}),"\n",(0,o.jsx)(e.p,{children:"A simple representation of humanoid balance can be modeled as an inverted pendulum where the center of mass is balanced above the support point."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSimple inverted pendulum model for humanoid balance control\nThis demonstrates basic PID control for maintaining balance\n"""\n\nimport numpy as np\nimport time\nfrom collections import deque\n\nclass BalanceController:\n    def __init__(self, kp=10.0, ki=0.1, kd=2.0):\n        self.kp = kp  # Proportional gain\n        self.ki = ki  # Integral gain  \n        self.kd = kd  # Derivative gain\n        \n        self.previous_error = 0.0\n        self.integral_error = 0.0\n        self.target_pose = 0.0  # Target balance position (upright)\n        \n    def compute_control(self, current_pose, dt):\n        # Calculate error\n        error = self.target_pose - current_pose\n        \n        # Update integral term\n        self.integral_error += error * dt\n        \n        # Calculate derivative term\n        derivative = (error - self.previous_error) / dt\n        \n        # Compute PID output\n        output = (self.kp * error) + (self.ki * self.integral_error) + (self.kd * derivative)\n        \n        self.previous_error = error\n        return output\n\nclass InvertedPendulum:\n    def __init__(self, length=1.0, mass=1.0, g=9.81):\n        self.length = length  # Length of pendulum\n        self.mass = mass      # Mass of pendulum\n        self.g = g            # Gravity\n        self.angle = 0.0      # Current angle (radians)\n        self.angular_velocity = 0.0  # Angular velocity (rad/s)\n        \n    def update(self, control_input, dt):\n        # Calculate angular acceleration based on gravity and control input\n        # For an inverted pendulum: angular_acc = (g/L) * sin(angle) + control_input\n        angular_acc = (self.g / self.length) * np.sin(self.angle) + control_input\n        \n        # Update angular velocity and angle\n        self.angular_velocity += angular_acc * dt\n        self.angle += self.angular_velocity * dt\n        \n        return self.angle, self.angular_velocity\n\ndef simulate_balance():\n    """Simulate balance control for an inverted pendulum"""\n    controller = BalanceController()\n    pendulum = InvertedPendulum(length=0.5, mass=10.0)\n    \n    dt = 0.01  # Time step (10ms)\n    duration = 10.0  # Simulation duration (seconds)\n    steps = int(duration / dt)\n    \n    # Simulate with some initial disturbance\n    pendulum.angle = 0.1  # Initial angle (10 degrees)\n    \n    print("Time (s)\\tAngle (rad)\\tControl Output")\n    print("--------\\t-----------\\t--------------")\n    \n    for i in range(steps):\n        current_time = i * dt\n        \n        # Get control output based on current angle\n        control_output = controller.compute_control(pendulum.angle, dt)\n        \n        # Update pendulum with control input\n        angle, angular_velocity = pendulum.update(control_output, dt)\n        \n        if i % 100 == 0:  # Print every second\n            print(f"{current_time:.2f}\\t\\t{angle:.4f}\\t\\t{control_output:.4f}")\n        \n        time.sleep(0.001)  # Slow down simulation for readability\n\nif __name__ == "__main__":\n    simulate_balance()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"example-2-basic-path-planning-with-potential-fields",children:"Example 2: Basic Path Planning with Potential Fields"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nBasic path planning using artificial potential fields\nDemonstrates obstacle avoidance and goal seeking behavior\n"""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass PotentialFieldPlanner:\n    def __init__(self, goal, obstacles, robot_pos, attractive_gain=1.0, repulsive_gain=1.0, obstacle_radius=1.0):\n        self.goal = np.array(goal)\n        self.obstacles = [np.array(obs) for obs in obstacles]\n        self.robot_pos = np.array(robot_pos)\n        self.attractive_gain = attractive_gain\n        self.repulsive_gain = repulsive_gain\n        self.obstacle_radius = obstacle_radius\n        \n    def attractive_force(self):\n        """Calculate attractive force towards goal"""\n        direction = self.goal - self.robot_pos\n        distance = np.linalg.norm(direction)\n        \n        if distance < 0.1:  # Near goal\n            return np.array([0.0, 0.0])\n        \n        # Linear attractive force\n        force = self.attractive_gain * direction\n        return force\n    \n    def repulsive_force(self):\n        """Calculate repulsive force from obstacles"""\n        total_force = np.array([0.0, 0.0])\n        \n        for obstacle in self.obstacles:\n            direction = self.robot_pos - obstacle\n            distance = np.linalg.norm(direction)\n            \n            if distance < self.obstacle_radius:\n                # Calculate repulsive force\n                if distance > 0.1:\n                    force_magnitude = self.repulsive_gain * (1.0/distance - 1.0/self.obstacle_radius)\n                    force_direction = direction / distance\n                    force = force_magnitude * force_direction\n                    total_force += force\n                else:\n                    # Avoid division by zero\n                    force = np.array([0.0, 0.0])\n                    total_force += force\n        \n        return total_force\n    \n    def total_force(self):\n        """Calculate total force (attractive + repulsive)"""\n        return self.attractive_force() + self.repulsive_force()\n    \n    def plan_step(self, step_size=0.1):\n        """Plan one step of movement"""\n        force = self.total_force()\n        direction = force / (np.linalg.norm(force) + 1e-8)  # Normalize\n        self.robot_pos += direction * step_size\n        return self.robot_pos.copy()\n\ndef visualize_path_planning():\n    """Visualize the potential field path planning"""\n    goal = [8, 8]\n    obstacles = [[3, 3], [5, 5], [6, 2]]\n    start = [1, 1]\n    \n    planner = PotentialFieldPlanner(goal, obstacles, start)\n    \n    # Store path\n    path = [start.copy()]\n    max_steps = 500\n    \n    for i in range(max_steps):\n        new_pos = planner.plan_step(step_size=0.1)\n        path.append(new_pos.copy())\n        \n        # Check if reached goal (within tolerance)\n        if np.linalg.norm(new_pos - goal) < 0.5:\n            break\n    \n    # Convert to numpy arrays for plotting\n    path = np.array(path)\n    \n    # Plot results\n    plt.figure(figsize=(10, 8))\n    \n    # Plot obstacles\n    for obs in obstacles:\n        circle = plt.Circle((obs[0], obs[1]), 0.5, color=\'red\', alpha=0.5)\n        plt.gca().add_patch(circle)\n    \n    # Plot path\n    plt.plot(path[:, 0], path[:, 1], \'b-\', linewidth=2, label=\'Path\')\n    plt.plot(path[0, 0], path[0, 1], \'go\', markersize=10, label=\'Start\')\n    plt.plot(path[-1, 0], path[-1, 1], \'ro\', markersize=10, label=\'Goal reached\')\n    plt.plot(goal[0], goal[1], \'rs\', markersize=12, label=\'Target\')\n    \n    plt.xlabel(\'X Position\')\n    plt.ylabel(\'Y Position\')\n    plt.title(\'Potential Field Path Planning\')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.axis(\'equal\')\n    plt.show()\n    \n    print(f"Path completed in {len(path)} steps")\n    print(f"Final position: {path[-1]}")\n    print(f"Distance to goal: {np.linalg.norm(path[-1] - goal):.2f}")\n\nif __name__ == "__main__":\n    visualize_path_planning()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"24-ros2-python-implementation-examples",children:"2.4 ROS2 Python Implementation Examples"}),"\n",(0,o.jsx)(e.h3,{id:"example-3-ros2-node-for-joint-control",children:"Example 3: ROS2 Node for Joint Control"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nROS2 node for controlling humanoid robot joints\nImplements trajectory execution with position, velocity, and acceleration\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom sensor_msgs.msg import JointState\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\n\nclass HumanoidJointController(Node):\n    def __init__(self):\n        super().__init__('humanoid_joint_controller')\n        \n        # Publisher for joint trajectory commands\n        self.trajectory_pub = self.create_publisher(\n            JointTrajectory, \n            '/joint_trajectory_controller/joint_trajectory', \n            10\n        )\n        \n        # Subscriber for current joint states\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n        \n        # Timer for periodic control updates\n        self.timer = self.create_timer(0.1, self.control_loop)\n        \n        # Joint names for humanoid (example: 24 DOF humanoid)\n        self.joint_names = [\n            'left_hip_roll', 'left_hip_yaw', 'left_hip_pitch',\n            'left_knee', 'left_ankle_pitch', 'left_ankle_roll',\n            'right_hip_roll', 'right_hip_yaw', 'right_hip_pitch', \n            'right_knee', 'right_ankle_pitch', 'right_ankle_roll',\n            'torso_yaw', 'torso_pitch', 'torso_roll',\n            'left_shoulder_pitch', 'left_shoulder_roll', 'left_shoulder_yaw',\n            'left_elbow', 'left_wrist_pitch', 'left_wrist_yaw',\n            'right_shoulder_pitch', 'right_shoulder_roll', 'right_shoulder_yaw',\n            'right_elbow', 'right_wrist_pitch', 'right_wrist_yaw',\n            'neck_yaw', 'neck_pitch'\n        ]\n        \n        # Current joint positions\n        self.current_positions = {name: 0.0 for name in self.joint_names}\n        \n        self.get_logger().info('Humanoid Joint Controller initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Callback to update current joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if name in self.current_positions:\n                self.current_positions[name] = msg.position[i]\n\n    def control_loop(self):\n        \"\"\"Main control loop\"\"\"\n        # Example: Send a simple trajectory for demonstration\n        self.send_joint_trajectory()\n\n    def send_joint_trajectory(self):\n        \"\"\"Send a joint trajectory message\"\"\"\n        trajectory_msg = JointTrajectory()\n        trajectory_msg.joint_names = self.joint_names\n        \n        # Create trajectory points\n        points = []\n        \n        # Start position (current)\n        start_point = JointTrajectoryPoint()\n        start_point.positions = [self.current_positions[name] for name in self.joint_names]\n        start_point.velocities = [0.0] * len(self.joint_names)\n        start_point.accelerations = [0.0] * len(self.joint_names)\n        start_point.time_from_start = Duration(sec=0, nanosec=0)\n        points.append(start_point)\n        \n        # Mid position (example: lifted leg)\n        mid_point = JointTrajectoryPoint()\n        mid_positions = []\n        for name in self.joint_names:\n            if 'left_knee' in name:\n                mid_positions.append(0.5)  # Lift knee\n            elif 'left_hip_pitch' in name:\n                mid_positions.append(0.2)  # Move hip forward\n            else:\n                mid_positions.append(self.current_positions[name])\n        \n        mid_point.positions = mid_positions\n        mid_point.velocities = [0.0] * len(self.joint_names)\n        mid_point.accelerations = [0.0] * len(self.joint_names)\n        mid_point.time_from_start = Duration(sec=2, nanosec=0)\n        points.append(mid_point)\n        \n        # End position (return to neutral)\n        end_point = JointTrajectoryPoint()\n        end_point.positions = [self.current_positions[name] for name in self.joint_names]\n        end_point.velocities = [0.0] * len(self.joint_names)\n        end_point.accelerations = [0.0] * len(self.joint_names)\n        end_point.time_from_start = Duration(sec=4, nanosec=0)\n        points.append(end_point)\n        \n        trajectory_msg.points = points\n        self.trajectory_pub.publish(trajectory_msg)\n        \n        self.get_logger().info('Sent joint trajectory command')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = HumanoidJointController()\n    \n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"example-4-ros2-perception-node-for-object-detection",children:"Example 4: ROS2 Perception Node for Object Detection"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nROS2 perception node for object detection and recognition\nUses camera data to detect objects and estimate their positions\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Point, PointStamped\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom std_msgs.msg import String\n\nclass HumanoidPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_perception\')\n        \n        # CV Bridge for image conversion\n        self.bridge = CvBridge()\n        \n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n        \n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n        \n        # Publishers\n        self.object_pub = self.create_publisher(PointStamped, \'/detected_objects\', 10)\n        self.status_pub = self.create_publisher(String, \'/perception_status\', 10)\n        \n        # Camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.camera_info_received = False\n        \n        # Object detection parameters\n        self.object_classifier = cv2.CascadeClassifier()\n        \n        # Initialize Haar cascade for simple object detection\n        # In practice, you\'d use a more sophisticated model\n        self.get_logger().info(\'Humanoid Perception Node initialized\')\n\n    def camera_info_callback(self, msg):\n        """Update camera parameters"""\n        if not self.camera_info_received:\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\n            self.distortion_coeffs = np.array(msg.d)\n            self.camera_info_received = True\n            self.get_logger().info(\'Camera parameters updated\')\n\n    def image_callback(self, msg):\n        """Process incoming image"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f\'Could not convert image: {e}\')\n            return\n        \n        # Convert to grayscale for object detection\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n        \n        # Simple circle detection as an example\n        circles = cv2.HoughCircles(\n            gray,\n            cv2.HOUGH_GRADIENT,\n            dp=1,\n            min_dist=50,\n            param1=50,\n            param2=30,\n            min_radius=10,\n            max_radius=100\n        )\n        \n        if circles is not None:\n            circles = np.round(circles[0, :]).astype("int")\n            \n            for (x, y, r) in circles:\n                # Draw detected circles\n                cv2.circle(cv_image, (x, y), r, (0, 255, 0), 2)\n                \n                # Publish object position as 3D point\n                if self.camera_info_received:\n                    object_3d = self.convert_2d_to_3d(x, y, depth=1.0)  # Assuming 1m depth\n                    self.publish_object_position(object_3d)\n                \n                # Annotate image\n                cv2.putText(cv_image, f\'Object at ({x}, {y})\', \n                           (x - 30, y - 30), \n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n        \n        # Publish status\n        status_msg = String()\n        status_msg.data = f"Objects detected: {len(circles) if circles is not None else 0}"\n        self.status_pub.publish(status_msg)\n        \n        # For visualization, we could publish the processed image\n        # (In practice, you\'d use an image publisher)\n\n    def convert_2d_to_3d(self, x, y, depth=1.0):\n        """Convert 2D image coordinates to 3D world coordinates"""\n        if self.camera_matrix is None:\n            return Point()\n        \n        # Convert pixel coordinates to normalized coordinates\n        x_norm = (x - self.camera_matrix[0, 2]) / self.camera_matrix[0, 0]\n        y_norm = (y - self.camera_matrix[1, 2]) / self.camera_matrix[1, 1]\n        \n        # Convert to 3D world coordinates\n        world_x = x_norm * depth\n        world_y = y_norm * depth\n        world_z = depth\n        \n        return Point(x=world_x, y=world_y, z=world_z)\n\n    def publish_object_position(self, point):\n        """Publish detected object position"""\n        point_stamped = PointStamped()\n        point_stamped.header = Header()\n        point_stamped.header.stamp = self.get_clock().now().to_msg()\n        point_stamped.header.frame_id = "camera_frame"\n        point_stamped.point = point\n        \n        self.object_pub.publish(point_stamped)\n        self.get_logger().info(f\'Published object at: ({point.x:.2f}, {point.y:.2f}, {point.z:.2f})\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = HumanoidPerceptionNode()\n    \n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"25-laboratory-exercises",children:"2.5 Laboratory Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-1-balance-control-implementation",children:"Lab Exercise 1: Balance Control Implementation"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Implement a balance controller for a humanoid robot using PID control principles."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Equipment Required"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Simulated humanoid robot (Gazebo + ROS2)"}),"\n",(0,o.jsx)(e.li,{children:"Computer with ROS2 installed"}),"\n",(0,o.jsx)(e.li,{children:"Basic programming environment"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Procedure"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Set up the simulation environment with a humanoid robot model"}),"\n",(0,o.jsx)(e.li,{children:"Implement a PID controller for balance using the example code as a starting point"}),"\n",(0,o.jsx)(e.li,{children:"Test the controller with different gain values (P, I, D)"}),"\n",(0,o.jsx)(e.li,{children:"Add external disturbances to test robustness"}),"\n",(0,o.jsx)(e.li,{children:"Analyze the response characteristics and stability"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Expected Outcomes"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Stable balance control of the simulated humanoid"}),"\n",(0,o.jsx)(e.li,{children:"Understanding of PID tuning for physical systems"}),"\n",(0,o.jsx)(e.li,{children:"Experience with sensor feedback and control loops"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-2-inverse-kinematics-for-arm-control",children:"Lab Exercise 2: Inverse Kinematics for Arm Control"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Implement inverse kinematics to control the humanoid's arm to reach specified positions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Theory Background"}),":\nInverse kinematics (IK) solves for joint angles required to achieve a desired end-effector position. For redundant systems, optimization criteria can be added."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Define the kinematic chain for the humanoid arm"}),"\n",(0,o.jsx)(e.li,{children:"Implement Jacobian-based inverse kinematics"}),"\n",(0,o.jsx)(e.li,{children:"Add constraints for joint limits and collision avoidance"}),"\n",(0,o.jsx)(e.li,{children:"Test with various target positions"}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nInverse Kinematics Implementation Lab Exercise\n"""\n\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass HumanoidArmIK:\n    def __init__(self, link_lengths=None):\n        if link_lengths is None:\n            # Example: simple 2-link arm (upper arm, lower arm)\n            self.link_lengths = [0.3, 0.3]  # meters\n        else:\n            self.link_lengths = link_lengths\n        \n        # Initial joint angles\n        self.joint_angles = [0.0, 0.0]\n        \n    def jacobian(self):\n        """Calculate the Jacobian matrix for the 2-DOF arm"""\n        theta1, theta2 = self.joint_angles\n        l1, l2 = self.link_lengths\n        \n        # Calculate Jacobian elements\n        J = np.array([\n            [-l1*np.sin(theta1) - l2*np.sin(theta1 + theta2), -l2*np.sin(theta1 + theta2)],\n            [l1*np.cos(theta1) + l2*np.cos(theta1 + theta2), l2*np.cos(theta1 + theta2)]\n        ])\n        \n        return J\n    \n    def forward_kinematics(self):\n        """Calculate end-effector position from joint angles"""\n        theta1, theta2 = self.joint_angles\n        l1, l2 = self.link_lengths\n        \n        x = l1*np.cos(theta1) + l2*np.cos(theta1 + theta2)\n        y = l1*np.sin(theta1) + l2*np.sin(theta1 + theta2)\n        \n        return np.array([x, y])\n    \n    def solve_ik(self, target_pos, max_iterations=100, tolerance=1e-4):\n        """Solve inverse kinematics using Jacobian transpose method"""\n        for i in range(max_iterations):\n            current_pos = self.forward_kinematics()\n            error = target_pos - current_pos\n            \n            if np.linalg.norm(error) < tolerance:\n                print(f"IK converged in {i+1} iterations")\n                return True\n            \n            # Calculate Jacobian\n            J = self.jacobian()\n            \n            # Update joint angles using Jacobian transpose\n            # J^T * error gives joint velocity in direction of error\n            delta_theta = 0.01 * J.T @ error\n            \n            # Apply joint limits (example: +/- 90 degrees)\n            for j in range(len(self.joint_angles)):\n                new_angle = self.joint_angles[j] + delta_theta[j]\n                # Limit to +/- 90 degrees\n                self.joint_angles[j] = np.clip(new_angle, -np.pi/2, np.pi/2)\n        \n        print(f"IK did not converge after {max_iterations} iterations")\n        return False\n\ndef test_ik():\n    """Test the inverse kinematics implementation"""\n    ik_solver = HumanoidArmIK()\n    \n    # Test positions\n    test_positions = [\n        [0.4, 0.2],  # Reach forward\n        [0.2, 0.4],  # Reach up\n        [0.1, 0.1],  # Reach down\n    ]\n    \n    for target in test_positions:\n        print(f"\\nAttempting to reach: {target}")\n        \n        # Reset to initial position\n        ik_solver.joint_angles = [0.0, 0.0]\n        \n        success = ik_solver.solve_ik(np.array(target))\n        \n        if success:\n            final_pos = ik_solver.forward_kinematics()\n            print(f"Final joint angles: {ik_solver.joint_angles}")\n            print(f"Target: {target}, Achieved: {final_pos}")\n            print(f"Error: {np.linalg.norm(np.array(target) - final_pos):.4f}")\n        else:\n            print("IK solution failed")\n\nif __name__ == "__main__":\n    test_ik()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-3-path-planning-and-navigation",children:"Lab Exercise 3: Path Planning and Navigation"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Implement and test a navigation system for humanoid robot path planning in an environment with obstacles."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Theory Background"}),":\nPath planning for humanoid robots must consider:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Kinematic constraints"}),"\n",(0,o.jsx)(e.li,{children:"Dynamic balance during movement"}),"\n",(0,o.jsx)(e.li,{children:"Obstacle avoidance"}),"\n",(0,o.jsx)(e.li,{children:"Energy efficiency"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement A* or D* path planning algorithm"}),"\n",(0,o.jsx)(e.li,{children:"Add humanoid-specific constraints (step size, balance)"}),"\n",(0,o.jsx)(e.li,{children:"Integrate with ROS2 navigation stack"}),"\n",(0,o.jsx)(e.li,{children:"Test in simulation with various obstacle configurations"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"lab-exercise-4-sensor-fusion-for-state-estimation",children:"Lab Exercise 4: Sensor Fusion for State Estimation"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Combine data from multiple sensors (IMU, encoders, cameras) to estimate the humanoid's state accurately."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Theory Background"}),":\nSensor fusion combines complementary information from multiple sensors to improve estimation accuracy and reliability."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement an Extended Kalman Filter (EKF)"}),"\n",(0,o.jsx)(e.li,{children:"Fuse IMU, encoder, and camera data"}),"\n",(0,o.jsx)(e.li,{children:"Test with simulated sensor noise"}),"\n",(0,o.jsx)(e.li,{children:"Compare fused estimate to ground truth"}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nExtended Kalman Filter for Humanoid State Estimation\n"""\n\nimport numpy as np\n\nclass HumanoidEKF:\n    def __init__(self):\n        # State vector: [x, y, theta, vx, vy, omega]\n        # x, y: position\n        # theta: orientation\n        # vx, vy: linear velocity\n        # omega: angular velocity\n        \n        self.state_dim = 6\n        self.state = np.zeros(self.state_dim)\n        \n        # Covariance matrix\n        self.P = np.eye(self.state_dim) * 0.1\n        \n        # Process noise\n        self.Q = np.eye(self.state_dim) * 0.01\n        \n        # Measurement noise (for different sensors)\n        self.R_imu = np.diag([0.01, 0.01, 0.001])  # [ax, ay, omega]\n        self.R_encoders = np.diag([0.05, 0.05])     # [vx, vy]\n        self.R_vision = np.diag([0.1, 0.1])         # [x, y]\n    \n    def predict(self, dt):\n        """Prediction step of EKF"""\n        # State transition model (simplified - assume constant velocity model)\n        F = np.eye(self.state_dim)\n        \n        # Add dynamics based on current state\n        theta = self.state[2]\n        vx = self.state[3]\n        vy = self.state[4]\n        \n        # Update position based on velocity\n        self.state[0] += (vx * np.cos(theta) - vy * np.sin(theta)) * dt\n        self.state[1] += (vx * np.sin(theta) + vy * np.cos(theta)) * dt\n        self.state[2] += self.state[5] * dt  # Update orientation\n        \n        # Jacobian of state transition\n        F[0, 3] = np.cos(theta) * dt\n        F[0, 4] = -np.sin(theta) * dt\n        F[0, 5] = (-vx * np.sin(theta) - vy * np.cos(theta)) * dt\n        \n        F[1, 3] = np.sin(theta) * dt\n        F[1, 4] = np.cos(theta) * dt\n        F[1, 5] = (vx * np.cos(theta) - vy * np.sin(theta)) * dt\n        \n        F[2, 5] = dt\n        \n        # Predict covariance\n        self.P = F @ self.P @ F.T + self.Q\n    \n    def update_with_imu(self, measurement):\n        """Update with IMU measurement [ax, ay, omega]"""\n        # Measurement model: directly measure acceleration and angular velocity\n        H = np.zeros((3, self.state_dim))\n        H[2, 5] = 1  # Measure angular velocity\n        \n        # Expected measurement (simplified model)\n        expected = np.array([0, 0, self.state[5]])\n        \n        # Innovation\n        y = measurement - expected\n        \n        # Innovation covariance\n        S = H @ self.P @ H.T + self.R_imu\n        \n        # Kalman gain\n        K = self.P @ H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.state += K @ y\n        self.P = (np.eye(self.state_dim) - K @ H) @ self.P\n    \n    def update_with_encoders(self, measurement):\n        """Update with encoder measurement [vx, vy]"""\n        H = np.zeros((2, self.state_dim))\n        H[0, 3] = 1  # Measure vx\n        H[1, 4] = 1  # Measure vy\n        \n        # Expected measurement\n        expected = self.state[3:5]\n        \n        # Innovation\n        y = measurement - expected\n        \n        # Innovation covariance\n        S = H @ self.P @ H.T + self.R_encoders\n        \n        # Kalman gain\n        K = self.P @ H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.state += K @ y\n        self.P = (np.eye(self.state_dim) - K @ H) @ self.P\n    \n    def update_with_vision(self, measurement):\n        """Update with vision measurement [x, y]"""\n        H = np.zeros((2, self.state_dim))\n        H[0, 0] = 1  # Measure x position\n        H[1, 1] = 1  # Measure y position\n        \n        # Expected measurement\n        expected = self.state[0:2]\n        \n        # Innovation\n        y = measurement - expected\n        \n        # Innovation covariance\n        S = H @ self.P @ H.T + self.R_vision\n        \n        # Kalman gain\n        K = self.P @ H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.state += K @ y\n        self.P = (np.eye(self.state_dim) - K @ H) @ self.P\n\ndef simulate_sensor_fusion():\n    """Simulate sensor fusion with multiple sensor types"""\n    ekf = HumanoidEKF()\n    \n    dt = 0.1\n    duration = 10.0\n    steps = int(duration / dt)\n    \n    print("Time\\tX\\tY\\tTheta\\tVx\\tVy\\tOmega")\n    print("-" * 50)\n    \n    for i in range(steps):\n        current_time = i * dt\n        \n        # Predict step\n        ekf.predict(dt)\n        \n        # Simulate sensor measurements with noise\n        # Ground truth (for comparison)\n        true_pos = [0.5 * current_time, 0.3 * current_time]\n        true_vel = [0.5, 0.3]\n        true_omega = 0.1\n        \n        # Add noise to measurements\n        imu_measurement = np.array([\n            np.random.normal(0, 0.01),  # ax\n            np.random.normal(0, 0.01),  # ay\n            np.random.normal(true_omega, 0.001)  # omega\n        ])\n        \n        encoder_measurement = np.array([\n            np.random.normal(true_vel[0], 0.02),  # vx\n            np.random.normal(true_vel[1], 0.02)   # vy\n        ])\n        \n        vision_measurement = np.array([\n            np.random.normal(true_pos[0], 0.05),  # x\n            np.random.normal(true_pos[1], 0.05)   # y\n        ])\n        \n        # Update with different sensors at different rates\n        if i % 2 == 0:  # Update with IMU every other step\n            ekf.update_with_imu(imu_measurement)\n        \n        if i % 5 == 0:  # Update with encoders every 5 steps\n            ekf.update_with_encoders(encoder_measurement)\n        \n        if i % 10 == 0:  # Update with vision every 10 steps\n            ekf.update_with_vision(vision_measurement)\n        \n        if i % 10 == 0:  # Print state every 10 steps\n            print(f"{current_time:.1f}\\t{ekf.state[0]:.2f}\\t{ekf.state[1]:.2f}\\t"\n                  f"{ekf.state[2]:.3f}\\t{ekf.state[3]:.2f}\\t{ekf.state[4]:.2f}\\t{ekf.state[5]:.3f}")\n\nif __name__ == "__main__":\n    simulate_sensor_fusion()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"26-summary",children:"2.6 Summary"}),"\n",(0,o.jsx)(e.p,{children:"Physical AI for humanoid robotics integrates multiple complex systems including perception, control, planning, and learning. The key challenges include real-time processing, handling of uncertainty, and ensuring safety and stability. This chapter has covered the theoretical foundations, practical examples, and laboratory exercises that form the basis for developing advanced humanoid robotic systems."}),"\n",(0,o.jsx)(e.p,{children:"The examples provided demonstrate how to implement balance control, path planning, joint control, and sensor fusion using ROS2 and Python. These concepts form the foundation for building more sophisticated humanoid robotic applications."}),"\n",(0,o.jsx)(e.p,{children:"In the next chapter, we will explore advanced topics in machine learning for humanoid robotics, including reinforcement learning, imitation learning, and adaptive control systems."}),"\n",(0,o.jsx)(e.h2,{id:"27-discussion-questions",children:"2.7 Discussion Questions"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"What are the main differences between physical AI and traditional AI approaches?"}),"\n",(0,o.jsx)(e.li,{children:"How do sensor fusion techniques improve the performance of humanoid robots?"}),"\n",(0,o.jsx)(e.li,{children:"What are the challenges of implementing real-time control in humanoid robotics?"}),"\n",(0,o.jsx)(e.li,{children:"How can learning algorithms be integrated with traditional control methods in humanoid robots?"}),"\n",(0,o.jsx)(e.li,{children:"Discuss the safety considerations that arise when humanoid robots operate in human environments."}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);
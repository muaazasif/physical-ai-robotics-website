"use strict";(globalThis.webpackChunkphysical_ai_robotics_website=globalThis.webpackChunkphysical_ai_robotics_website||[]).push([[531],{7842:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>_});const t=JSON.parse('{"id":"chapter_3_visualization_animation","title":"Chapter 3: Visualization and Animation for Humanoid Robotics AI","description":"3.1 Introduction to Visualization and Animation in Humanoid Robotics","source":"@site/docs/chapter_3_visualization_animation.md","sourceDirName":".","slug":"/chapter_3_visualization_animation","permalink":"/physical-ai-robotics-website/docs/chapter_3_visualization_animation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter_3_visualization_animation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Physical AI for Humanoid Robotics","permalink":"/physical-ai-robotics-website/docs/chapter_2_physical_ai"},"next":{"title":"Chapter 4: Machine Learning and AI Algorithms for Humanoid Robotics","permalink":"/physical-ai-robotics-website/docs/chapter_4_ml_algorithms"}}');var a=i(4848),s=i(8453);const o={},l="Chapter 3: Visualization and Animation for Humanoid Robotics AI",r={},_=[{value:"3.1 Introduction to Visualization and Animation in Humanoid Robotics",id:"31-introduction-to-visualization-and-animation-in-humanoid-robotics",level:2},{value:"Types of Visualization in Humanoid Robotics",id:"types-of-visualization-in-humanoid-robotics",level:3},{value:"3.2 Theoretical Foundations",id:"32-theoretical-foundations",level:2},{value:"3.2.1 3D Transformation Mathematics",id:"321-3d-transformation-mathematics",level:3},{value:"3.2.2 Kinematic Chain Visualization",id:"322-kinematic-chain-visualization",level:3},{value:"3.2.3 Animation Principles",id:"323-animation-principles",level:3},{value:"3.3 Visualization Frameworks and Tools",id:"33-visualization-frameworks-and-tools",level:2},{value:"3.3.1 RViz: ROS Visualization Tool",id:"331-rviz-ros-visualization-tool",level:3},{value:"3.3.2 Gazebo: Physics Simulation Environment",id:"332-gazebo-physics-simulation-environment",level:3},{value:"3.3.3 Blender: 3D Modeling and Animation",id:"333-blender-3d-modeling-and-animation",level:3},{value:"3.4 Simple Examples",id:"34-simple-examples",level:2},{value:"Example 1: Basic Robot State Visualization",id:"example-1-basic-robot-state-visualization",level:3},{value:"Example 2: Interactive Joint Control Visualization",id:"example-2-interactive-joint-control-visualization",level:3},{value:"Example 3: AI Behavior Visualization",id:"example-3-ai-behavior-visualization",level:3},{value:"3.4 ROS2 Visualization Examples",id:"34-ros2-visualization-examples",level:2},{value:"Example 4: RViz Visualization with Marker Arrays",id:"example-4-rviz-visualization-with-marker-arrays",level:3},{value:"Example 5: Animation of Walking Pattern",id:"example-5-animation-of-walking-pattern",level:3},{value:"3.5 Laboratory Exercises",id:"35-laboratory-exercises",level:2},{value:"Lab Exercise 1: Robot State Visualization Dashboard",id:"lab-exercise-1-robot-state-visualization-dashboard",level:3},{value:"Lab Exercise 2: Interactive Animation Editor",id:"lab-exercise-2-interactive-animation-editor",level:3},{value:"Lab Exercise 3: AI Behavior Tree Visualizer",id:"lab-exercise-3-ai-behavior-tree-visualizer",level:3},{value:"3.6 Advanced Visualization Techniques",id:"36-advanced-visualization-techniques",level:2},{value:"3.6.1 Real-Time Rendering with OpenGL",id:"361-real-time-rendering-with-opengl",level:3},{value:"3.6.2 Augmented Reality Integration",id:"362-augmented-reality-integration",level:3},{value:"3.6.3 Data-Driven Visualization",id:"363-data-driven-visualization",level:3},{value:"3.7 Best Practices for Visualization",id:"37-best-practices-for-visualization",level:2},{value:"3.8 Summary",id:"38-summary",level:2},{value:"3.9 Discussion Questions",id:"39-discussion-questions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3-visualization-and-animation-for-humanoid-robotics-ai",children:"Chapter 3: Visualization and Animation for Humanoid Robotics AI"})}),"\n",(0,a.jsx)(n.h2,{id:"31-introduction-to-visualization-and-animation-in-humanoid-robotics",children:"3.1 Introduction to Visualization and Animation in Humanoid Robotics"}),"\n",(0,a.jsx)(n.p,{children:"Visualization and animation play crucial roles in humanoid robotics, serving multiple purposes:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Development and debugging of robotic systems"}),"\n",(0,a.jsx)(n.li,{children:"Demonstration of capabilities to stakeholders"}),"\n",(0,a.jsx)(n.li,{children:"Educational tools for understanding robot behavior"}),"\n",(0,a.jsx)(n.li,{children:"Human-robot interaction enhancement"}),"\n",(0,a.jsx)(n.li,{children:"Validation of AI decision-making processes"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Effective visualization helps bridge the gap between abstract AI algorithms and tangible robot behaviors, making complex systems more understandable and accessible."}),"\n",(0,a.jsx)(n.h3,{id:"types-of-visualization-in-humanoid-robotics",children:"Types of Visualization in Humanoid Robotics"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulation Environments"}),": Physics-based worlds for testing and validation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robot State Visualization"}),": Display of joint angles, sensor readings, and internal states"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AI Decision Visualization"}),": Showcasing how AI algorithms make decisions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Behavior Animation"}),": Smooth transitions between robot poses and actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Visualization"}),": Graphical representation of sensor data and performance metrics"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"32-theoretical-foundations",children:"3.2 Theoretical Foundations"}),"\n",(0,a.jsx)(n.h3,{id:"321-3d-transformation-mathematics",children:"3.2.1 3D Transformation Mathematics"}),"\n",(0,a.jsx)(n.p,{children:"Understanding 3D transformations is essential for animating humanoid robots:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Translation"}),": Moving objects in 3D space"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rotation"}),": Orienting objects using rotation matrices or quaternions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scaling"}),": Changing object dimensions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Homogeneous Coordinates"}),": Representing transformations as 4x4 matrices"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"322-kinematic-chain-visualization",children:"3.2.2 Kinematic Chain Visualization"}),"\n",(0,a.jsx)(n.p,{children:"Humanoid robots consist of multiple connected rigid bodies forming kinematic chains. Visualizing these chains requires:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Forward kinematics computation for end-effector positioning"}),"\n",(0,a.jsx)(n.li,{children:"Joint coordinate frame representation"}),"\n",(0,a.jsx)(n.li,{children:"Link geometry visualization"}),"\n",(0,a.jsx)(n.li,{children:"Collision geometry display"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"323-animation-principles",children:"3.2.3 Animation Principles"}),"\n",(0,a.jsx)(n.p,{children:"Animation in robotics follows key principles:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Smooth Transitions"}),": Continuous motion without abrupt changes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Realistic Timing"}),": Natural acceleration and deceleration curves"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Balance Preservation"}),": Maintaining center of mass within support polygon"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physical Plausibility"}),": Movements respecting physical constraints"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"33-visualization-frameworks-and-tools",children:"3.3 Visualization Frameworks and Tools"}),"\n",(0,a.jsx)(n.h3,{id:"331-rviz-ros-visualization-tool",children:"3.3.1 RViz: ROS Visualization Tool"}),"\n",(0,a.jsx)(n.p,{children:"RViz is the primary visualization tool for ROS-based humanoid robotics projects, offering:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Real-time robot state visualization"}),"\n",(0,a.jsx)(n.li,{children:"Sensor data display (laser scans, images, point clouds)"}),"\n",(0,a.jsx)(n.li,{children:"Custom plugin development"}),"\n",(0,a.jsx)(n.li,{children:"TF tree visualization"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"332-gazebo-physics-simulation-environment",children:"3.3.2 Gazebo: Physics Simulation Environment"}),"\n",(0,a.jsx)(n.p,{children:"Gazebo provides:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Accurate physics simulation"}),"\n",(0,a.jsx)(n.li,{children:"Sensor simulation"}),"\n",(0,a.jsx)(n.li,{children:"Realistic environments"}),"\n",(0,a.jsx)(n.li,{children:"Integration with ROS controllers"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"333-blender-3d-modeling-and-animation",children:"3.3.3 Blender: 3D Modeling and Animation"}),"\n",(0,a.jsx)(n.p,{children:"Blender can be used for:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"High-quality robot modeling"}),"\n",(0,a.jsx)(n.li,{children:"Complex animations"}),"\n",(0,a.jsx)(n.li,{children:"Rendered demonstrations"}),"\n",(0,a.jsx)(n.li,{children:"Video creation for presentations"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"34-simple-examples",children:"3.4 Simple Examples"}),"\n",(0,a.jsx)(n.h3,{id:"example-1-basic-robot-state-visualization",children:"Example 1: Basic Robot State Visualization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nBasic robot state visualization using matplotlib\nShows joint positions and robot configuration in 3D space\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.animation import FuncAnimation\nimport time\n\nclass HumanoidVisualizer:\n    def __init__(self):\n        # Define humanoid skeleton structure\n        # Simplified for visualization: 6 DOF legs, 6 DOF arms, 3 DOF torso, 3 DOF head\n        self.joint_names = [\n            'left_hip_pitch', 'left_knee', 'left_ankle_pitch', \n            'right_hip_pitch', 'right_knee', 'right_ankle_pitch',\n            'left_shoulder_pitch', 'left_elbow', 'left_wrist_pitch',\n            'right_shoulder_pitch', 'right_elbow', 'right_wrist_pitch',\n            'torso_pitch', 'torso_yaw', 'torso_roll',\n            'neck_pitch', 'neck_yaw'\n        ]\n        \n        # Initialize with neutral pose\n        self.joint_angles = {name: 0.0 for name in self.joint_names}\n        \n        # Define kinematic chain for visualization\n        self.links = [\n            # Torso links\n            ('hip_center', 'torso', 0.5),\n            ('torso', 'head', 0.25),\n            \n            # Left leg\n            ('hip_center', 'left_hip', 0.05),\n            ('left_hip', 'left_knee', 0.4),\n            ('left_knee', 'left_ankle', 0.4),\n            ('left_ankle', 'left_foot', 0.1),\n            \n            # Right leg\n            ('hip_center', 'right_hip', -0.05),\n            ('right_hip', 'right_knee', 0.4),\n            ('right_knee', 'right_ankle', 0.4),\n            ('right_ankle', 'right_foot', 0.1),\n            \n            # Left arm\n            ('torso', 'left_shoulder', 0.15),\n            ('left_shoulder', 'left_elbow', 0.3),\n            ('left_elbow', 'left_wrist', 0.3),\n            ('left_wrist', 'left_hand', 0.1),\n            \n            # Right arm\n            ('torso', 'right_shoulder', -0.15),\n            ('right_shoulder', 'right_elbow', 0.3),\n            ('right_elbow', 'right_wrist', 0.3),\n            ('right_wrist', 'right_hand', 0.1),\n        ]\n        \n        # Joint positions (relative to parent)\n        self.joint_positions = {\n            'hip_center': (0, 0, 0),\n            'torso': (0, 0, 0.5),\n            'head': (0, 0, 0.75),\n            'left_hip': (-0.1, 0, 0),\n            'left_knee': (-0.1, 0, -0.4),\n            'left_ankle': (-0.1, 0, -0.8),\n            'left_foot': (-0.1, 0, -0.9),\n            'right_hip': (0.1, 0, 0),\n            'right_knee': (0.1, 0, -0.4),\n            'right_ankle': (0.1, 0, -0.8),\n            'right_foot': (0.1, 0, -0.9),\n            'left_shoulder': (-0.15, 0, 0.5),\n            'left_elbow': (-0.45, 0, 0.5),\n            'left_wrist': (-0.75, 0, 0.5),\n            'left_hand': (-0.85, 0, 0.5),\n            'right_shoulder': (0.15, 0, 0.5),\n            'right_elbow': (0.45, 0, 0.5),\n            'right_wrist': (0.75, 0, 0.5),\n            'right_hand': (0.85, 0, 0.5),\n        }\n        \n    def calculate_forward_kinematics(self):\n        \"\"\"Calculate 3D positions of all body parts\"\"\"\n        # For simplicity, using static positions\n        # In a real implementation, this would use actual FK calculations\n        positions = {}\n        \n        for joint, pos in self.joint_positions.items():\n            positions[joint] = np.array(pos)\n        \n        return positions\n    \n    def draw_humanoid(self, ax, positions):\n        \"\"\"Draw the humanoid skeleton on the given axis\"\"\"\n        ax.clear()\n        \n        # Draw links between joints\n        for start_joint, end_joint, length in self.links:\n            if start_joint in positions and end_joint in positions:\n                start_pos = positions[start_joint]\n                end_pos = positions[end_joint]\n                \n                # Draw line between joints\n                ax.plot([start_pos[0], end_pos[0]], \n                        [start_pos[1], end_pos[1]], \n                        [start_pos[2], end_pos[2]], \n                        'b-', linewidth=2)\n        \n        # Draw joints as spheres\n        for joint_name, pos in positions.items():\n            ax.scatter(pos[0], pos[1], pos[2], c='red', s=50)\n            ax.text(pos[0], pos[1], pos[2], joint_name.split('_')[0][:4], fontsize=8)\n        \n        # Set axis properties\n        ax.set_xlim([-1, 1])\n        ax.set_ylim([-1, 1])\n        ax.set_zlim([-1, 1])\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n        ax.set_zlabel('Z')\n        ax.set_title('Humanoid Robot Visualization')\n\ndef animate_humanoid():\n    \"\"\"Animate basic humanoid movements\"\"\"\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    visualizer = HumanoidVisualizer()\n    \n    def animate(frame):\n        # Simple animation: wave left arm\n        visualizer.joint_angles['left_shoulder_pitch'] = 0.5 * np.sin(frame * 0.1)\n        visualizer.joint_angles['left_elbow'] = 0.3 * np.sin(frame * 0.15)\n        \n        # Jumping motion\n        offset = 0.1 * np.sin(frame * 0.2)\n        # Adjust hip position for jumping effect\n        visualizer.joint_positions['hip_center'] = (0, 0, offset)\n        visualizer.joint_positions['torso'] = (0, 0, 0.5 + offset)\n        visualizer.joint_positions['head'] = (0, 0, 0.75 + offset)\n        \n        # Update leg positions to maintain balance\n        visualizer.joint_positions['left_knee'] = (-0.1, 0, -0.4 + offset)\n        visualizer.joint_positions['left_ankle'] = (-0.1, 0, -0.8 + offset)\n        visualizer.joint_positions['left_foot'] = (-0.1, 0, -0.9 + offset)\n        visualizer.joint_positions['right_knee'] = (0.1, 0, -0.4 + offset)\n        visualizer.joint_positions['right_ankle'] = (0.1, 0, -0.8 + offset)\n        visualizer.joint_positions['right_foot'] = (0.1, 0, -0.9 + offset)\n        \n        positions = visualizer.calculate_forward_kinematics()\n        visualizer.draw_humanoid(ax, positions)\n    \n    ani = FuncAnimation(fig, animate, frames=200, interval=50, blit=False)\n    plt.tight_layout()\n    plt.show()\n\nif __name__ == \"__main__\":\n    animate_humanoid()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"example-2-interactive-joint-control-visualization",children:"Example 2: Interactive Joint Control Visualization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nInteractive joint control visualization using PyQt\nAllows manual adjustment of joint angles and visualization of resulting pose\n\"\"\"\n\nimport sys\nimport numpy as np\nimport math\nfrom PyQt5.QtWidgets import (\n    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout, \n    QLabel, QSlider, QPushButton, QFrame\n)\nfrom PyQt5.QtCore import Qt, QTimer\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\nfrom matplotlib.figure import Figure\nfrom matplotlib.patches import Circle\nimport matplotlib.pyplot as plt\n\nclass MplCanvas(FigureCanvas):\n    \"\"\"Matplotlib canvas for drawing robot\"\"\"\n    def __init__(self, parent=None, width=5, height=5, dpi=100):\n        fig = Figure(figsize=(width, height), dpi=dpi)\n        self.axes = fig.add_subplot(111)\n        super(MplCanvas, self).__init__(fig)\n\nclass JointControlWidget(QWidget):\n    def __init__(self):\n        super().__init__()\n        self.init_ui()\n        self.init_robot()\n        \n    def init_ui(self):\n        layout = QVBoxLayout()\n        \n        # Canvas for robot visualization\n        self.canvas = MplCanvas(self, width=5, height=5, dpi=100)\n        layout.addWidget(self.canvas)\n        \n        # Sliders for joint control\n        joint_layout = QHBoxLayout()\n        \n        # Left shoulder pitch\n        left_shoulder_layout = QVBoxLayout()\n        left_shoulder_layout.addWidget(QLabel(\"Left Shoulder\"))\n        self.left_shoulder_slider = QSlider(Qt.Horizontal)\n        self.left_shoulder_slider.setRange(-90, 90)\n        self.left_shoulder_slider.setValue(0)\n        self.left_shoulder_slider.valueChanged.connect(lambda v: self.update_robot())\n        left_shoulder_layout.addWidget(self.left_shoulder_slider)\n        \n        # Left elbow\n        left_elbow_layout = QVBoxLayout()\n        left_elbow_layout.addWidget(QLabel(\"Left Elbow\"))\n        self.left_elbow_slider = QSlider(Qt.Horizontal)\n        self.left_elbow_slider.setRange(-90, 90)\n        self.left_elbow_slider.setValue(0)\n        self.left_elbow_slider.valueChanged.connect(lambda v: self.update_robot())\n        left_elbow_layout.addWidget(self.left_elbow_slider)\n        \n        # Right shoulder pitch\n        right_shoulder_layout = QVBoxLayout()\n        right_shoulder_layout.addWidget(QLabel(\"Right Shoulder\"))\n        self.right_shoulder_slider = QSlider(Qt.Horizontal)\n        self.right_shoulder_slider.setRange(-90, 90)\n        self.right_shoulder_slider.setValue(0)\n        self.right_shoulder_slider.valueChanged.connect(lambda v: self.update_robot())\n        right_shoulder_layout.addWidget(self.right_shoulder_slider)\n        \n        # Right elbow\n        right_elbow_layout = QVBoxLayout()\n        right_elbow_layout.addWidget(QLabel(\"Right Elbow\"))\n        self.right_elbow_slider = QSlider(Qt.Horizontal)\n        self.right_elbow_slider.setRange(-90, 90)\n        self.right_elbow_slider.setValue(0)\n        self.right_elbow_slider.valueChanged.connect(lambda v: self.update_robot())\n        right_elbow_layout.addWidget(self.right_elbow_slider)\n        \n        joint_layout.addLayout(left_shoulder_layout)\n        joint_layout.addLayout(left_elbow_layout)\n        joint_layout.addLayout(right_shoulder_layout)\n        joint_layout.addLayout(right_elbow_layout)\n        \n        layout.addLayout(joint_layout)\n        \n        self.setLayout(layout)\n        \n    def init_robot(self):\n        \"\"\"Initialize robot kinematic structure\"\"\"\n        # Robot dimensions (in arbitrary units)\n        self.body_height = 1.0\n        self.arm_length = 0.5\n        self.leg_length = 0.7\n        \n        # Initial joint angles (in degrees)\n        self.joint_angles = {\n            'left_shoulder': 0,\n            'left_elbow': 0,\n            'right_shoulder': 0,\n            'right_elbow': 0\n        }\n        \n        self.update_robot()\n        \n    def calculate_arm_positions(self, shoulder_angle, elbow_angle, side):\n        \"\"\"Calculate arm joint positions based on angles\"\"\"\n        # Convert to radians\n        shoulder_rad = math.radians(shoulder_angle)\n        elbow_rad = math.radians(elbow_angle)\n        \n        # Shoulder position (left or right)\n        if side == 'left':\n            shoulder_x = -0.15\n        else:\n            shoulder_x = 0.15\n        shoulder_y = self.body_height * 0.8  # Shoulders at 80% body height\n        \n        # Elbow position\n        elbow_x = shoulder_x + self.arm_length * math.sin(shoulder_rad)\n        elbow_y = shoulder_y + self.arm_length * math.cos(shoulder_rad)\n        \n        # Wrist position\n        wrist_x = elbow_x + self.arm_length * math.sin(shoulder_rad + elbow_rad)\n        wrist_y = elbow_y + self.arm_length * math.cos(shoulder_rad + elbow_rad)\n        \n        return (shoulder_x, shoulder_y), (elbow_x, elbow_y), (wrist_x, wrist_y)\n        \n    def update_robot(self):\n        \"\"\"Redraw the robot based on current joint angles\"\"\"\n        # Update joint angles from sliders\n        self.joint_angles['left_shoulder'] = self.left_shoulder_slider.value()\n        self.joint_angles['left_elbow'] = self.left_elbow_slider.value()\n        self.joint_angles['right_shoulder'] = self.right_shoulder_slider.value()\n        self.joint_angles['right_elbow'] = self.right_elbow_slider.value()\n        \n        # Clear the plot\n        self.canvas.axes.clear()\n        \n        # Draw body (simple rectangle)\n        self.canvas.axes.add_patch(plt.Rectangle((-0.15, 0), 0.3, self.body_height, \n                                                fill=True, color='lightblue', \n                                                edgecolor='black'))\n        \n        # Calculate and draw left arm\n        left_shoulder_pos, left_elbow_pos, left_wrist_pos = self.calculate_arm_positions(\n            self.joint_angles['left_shoulder'], \n            self.joint_angles['left_elbow'], \n            'left'\n        )\n        \n        # Left arm links\n        self.canvas.axes.plot([left_shoulder_pos[0], left_elbow_pos[0]], \n                             [left_shoulder_pos[1], left_elbow_pos[1]], \n                             'k-', linewidth=3)\n        self.canvas.axes.plot([left_elbow_pos[0], left_wrist_pos[0]], \n                             [left_elbow_pos[1], left_wrist_pos[1]], \n                             'k-', linewidth=2)\n        \n        # Left joints\n        self.canvas.axes.scatter(left_shoulder_pos[0], left_shoulder_pos[1], \n                                c='red', s=100, zorder=5)\n        self.canvas.axes.scatter(left_elbow_pos[0], left_elbow_pos[1], \n                                c='red', s=80, zorder=5)\n        self.canvas.axes.scatter(left_wrist_pos[0], left_wrist_pos[1], \n                                c='red', s=60, zorder=5)\n        \n        # Calculate and draw right arm\n        right_shoulder_pos, right_elbow_pos, right_wrist_pos = self.calculate_arm_positions(\n            self.joint_angles['right_shoulder'], \n            self.joint_angles['right_elbow'], \n            'right'\n        )\n        \n        # Right arm links\n        self.canvas.axes.plot([right_shoulder_pos[0], right_elbow_pos[0]], \n                             [right_shoulder_pos[1], right_elbow_pos[1]], \n                             'k-', linewidth=3)\n        self.canvas.axes.plot([right_elbow_pos[0], right_wrist_pos[0]], \n                             [right_elbow_pos[1], right_wrist_pos[1]], \n                             'k-', linewidth=2)\n        \n        # Right joints\n        self.canvas.axes.scatter(right_shoulder_pos[0], right_shoulder_pos[1], \n                                c='red', s=100, zorder=5)\n        self.canvas.axes.scatter(right_elbow_pos[0], right_elbow_pos[1], \n                                c='red', s=80, zorder=5)\n        self.canvas.axes.scatter(right_wrist_pos[0], right_wrist_pos[1], \n                                c='red', s=60, zorder=5)\n        \n        # Draw head\n        self.canvas.axes.add_patch(Circle((0, self.body_height + 0.1), 0.1, \n                                         fill=True, color='lightgray', \n                                         edgecolor='black'))\n        \n        # Draw legs (fixed for now)\n        # Left leg\n        self.canvas.axes.plot([-0.1, -0.1], [0, -self.leg_length], \n                             'k-', linewidth=3)\n        self.canvas.axes.scatter(-0.1, -self.leg_length, c='red', s=100, zorder=5)\n        \n        # Right leg\n        self.canvas.axes.plot([0.1, 0.1], [0, -self.leg_length], \n                             'k-', linewidth=3)\n        self.canvas.axes.scatter(0.1, -self.leg_length, c='red', s=100, zorder=5)\n        \n        # Set equal aspect ratio and limits\n        self.canvas.axes.set_aspect('equal')\n        self.canvas.axes.set_xlim(-0.5, 0.5)\n        self.canvas.axes.set_ylim(-0.8, 1.2)\n        self.canvas.axes.set_title('Humanoid Robot Joint Control')\n        \n        # Redraw\n        self.canvas.draw()\n\nclass JointControlWindow(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle('Humanoid Robot Joint Control')\n        self.setGeometry(100, 100, 800, 600)\n        \n        # Create central widget\n        central_widget = QWidget()\n        self.setCentralWidget(central_widget)\n        \n        layout = QVBoxLayout(central_widget)\n        self.joint_widget = JointControlWidget()\n        layout.addWidget(self.joint_widget)\n\ndef main():\n    app = QApplication(sys.argv)\n    window = JointControlWindow()\n    window.show()\n    sys.exit(app.exec_())\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"example-3-ai-behavior-visualization",children:"Example 3: AI Behavior Visualization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nVisualization of AI decision-making process in humanoid robotics\nShows how state machines and behavior trees guide robot actions\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.animation import FuncAnimation\nimport random\n\nclass BehaviorVisualizer:\n    def __init__(self):\n        # Define robot states\n        self.states = [\n            'Idle', 'Walking', 'Balancing', 'Interacting', 'Avoiding Obstacle', 'Planning'\n        ]\n        \n        # Define possible state transitions\n        self.transitions = {\n            'Idle': ['Walking', 'Interacting', 'Idle'],\n            'Walking': ['Balancing', 'Avoiding Obstacle', 'Idle'],\n            'Balancing': ['Walking', 'Idle'],\n            'Interacting': ['Idle'],\n            'Avoiding Obstacle': ['Walking', 'Planning'],\n            'Planning': ['Walking']\n        }\n        \n        # Current state\n        self.current_state = 'Idle'\n        self.state_history = [self.current_state]\n        \n        # AI decision confidence levels\n        self.confidence_levels = {}\n        for state in self.states:\n            self.confidence_levels[state] = 0.0\n        \n        # Robot position and velocity\n        self.robot_x = 0.0\n        self.robot_y = 0.0\n        self.robot_theta = 0.0  # Heading angle\n        self.velocity = 0.0\n        \n        # Environment obstacles\n        self.obstacles = [(2, 1), (3, -1), (4, 1.5), (5, -0.5)]\n        \n        # Sensory inputs\n        self.sensor_readings = {\n            'front_distance': 2.0,\n            'left_distance': 1.5,\n            'right_distance': 1.8,\n            'center_of_pressure_x': 0.0,\n            'center_of_pressure_y': 0.0\n        }\n    \n    def update_behavior(self):\n        \"\"\"Update robot behavior based on sensory inputs\"\"\"\n        # Get probabilities for each state based on sensor readings\n        probabilities = self.calculate_state_probabilities()\n        \n        # Select next state based on probabilities\n        states = list(probabilities.keys())\n        probs = list(probabilities.values())\n        \n        # Choose next state (with some randomness for realism)\n        next_state = np.random.choice(states, p=probs)\n        \n        self.current_state = next_state\n        self.state_history.append(next_state)\n        \n        # Update confidence levels\n        for state in self.states:\n            self.confidence_levels[state] = probabilities.get(state, 0.0)\n        \n        # Update robot position based on current state\n        self.update_robot_motion()\n        \n        # Update sensor readings\n        self.update_sensors()\n    \n    def calculate_state_probabilities(self):\n        \"\"\"Calculate probability of each state based on sensor inputs\"\"\"\n        probabilities = {}\n        \n        # Baseline probabilities\n        baseline = 0.1\n        \n        # Increase probability based on sensor conditions\n        if self.current_state == 'Avoiding Obstacle':\n            # High probability to stay in obstacle avoidance if front distance is low\n            probabilities['Avoiding Obstacle'] = 0.8 if self.sensor_readings['front_distance'] < 0.5 else 0.1\n            probabilities['Balancing'] = 0.1\n            probabilities['Idle'] = 0.05\n            probabilities['Walking'] = 0.05\n        elif self.current_state == 'Balancing':\n            # High probability to continue balancing if CoP is off-center\n            if abs(self.sensor_readings['center_of_pressure_x']) > 0.1:\n                probabilities['Balancing'] = 0.7\n                probabilities['Idle'] = 0.2\n                probabilities['Walking'] = 0.1\n            else:\n                probabilities['Idle'] = 0.4\n                probabilities['Walking'] = 0.4\n                probabilities['Balancing'] = 0.2\n        elif self.current_state == 'Walking':\n            # Probability based on path conditions\n            if self.sensor_readings['front_distance'] < 0.8:\n                probabilities['Avoiding Obstacle'] = 0.6\n                probabilities['Planning'] = 0.3\n                probabilities['Walking'] = 0.1\n            else:\n                probabilities['Walking'] = 0.7\n                probabilities['Balancing'] = 0.2\n                probabilities['Idle'] = 0.1\n        else:  # Idle state\n            if self.sensor_readings['front_distance'] > 1.0:\n                probabilities['Walking'] = 0.6\n                probabilities['Idle'] = 0.4\n            else:\n                probabilities['Idle'] = 0.6\n                probabilities['Planning'] = 0.2\n                probabilities['Avoiding Obstacle'] = 0.2\n        \n        # Fill in remaining probabilities\n        remaining_prob = 1.0 - sum(probabilities.values())\n        for state in self.states:\n            if state not in probabilities:\n                probabilities[state] = baseline * remaining_prob / (len(self.states) - len(probabilities))\n        \n        # Normalize probabilities\n        total_prob = sum(probabilities.values())\n        if total_prob > 0:\n            for state in probabilities:\n                probabilities[state] /= total_prob\n        \n        return probabilities\n    \n    def update_robot_motion(self):\n        \"\"\"Update robot position based on current state\"\"\"\n        dt = 0.1  # Time step\n        \n        if self.current_state == 'Walking':\n            self.velocity = 0.5  # m/s\n            self.robot_x += self.velocity * dt * np.cos(self.robot_theta)\n            self.robot_y += self.velocity * dt * np.sin(self.robot_theta)\n        elif self.current_state == 'Avoiding Obstacle':\n            # Turn away from obstacles\n            if self.sensor_readings['front_distance'] < 0.5:\n                if self.sensor_readings['left_distance'] > self.sensor_readings['right_distance']:\n                    self.robot_theta += 0.3 * dt  # Turn left\n                else:\n                    self.robot_theta -= 0.3 * dt  # Turn right\n            self.velocity = 0.2\n            self.robot_x += self.velocity * dt * np.cos(self.robot_theta)\n            self.robot_y += self.velocity * dt * np.sin(self.robot_theta)\n        elif self.current_state == 'Balancing':\n            # Small corrective motions\n            self.robot_x += 0.05 * np.sin(2 * np.pi * 0.5 * dt) * dt\n            self.robot_y += 0.05 * np.cos(2 * np.pi * 0.5 * dt) * dt\n        else:  # Idle\n            self.velocity = 0.0\n    \n    def update_sensors(self):\n        \"\"\"Update sensor readings\"\"\"\n        # Simulate sensor noise and dynamic obstacles\n        self.sensor_readings['front_distance'] = max(0.1, self.sensor_readings['front_distance'] + \n                                                   np.random.normal(0, 0.05))\n        self.sensor_readings['left_distance'] = max(0.1, self.sensor_readings['left_distance'] + \n                                                  np.random.normal(0, 0.05))\n        self.sensor_readings['right_distance'] = max(0.1, self.sensor_readings['right_distance'] + \n                                                   np.random.normal(0, 0.05))\n        \n        # Update center of pressure (CoP) with some drift\n        self.sensor_readings['center_of_pressure_x'] += np.random.normal(0, 0.01)\n        self.sensor_readings['center_of_pressure_y'] += np.random.normal(0, 0.01)\n        \n        # Bound CoP values\n        self.sensor_readings['center_of_pressure_x'] = np.clip(\n            self.sensor_readings['center_of_pressure_x'], -0.15, 0.15\n        )\n        self.sensor_readings['center_of_pressure_y'] = np.clip(\n            self.sensor_readings['center_of_pressure_y'], -0.15, 0.15\n        )\n\ndef visualize_behavior():\n    \"\"\"Animate the behavior visualization\"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n    \n    visualizer = BehaviorVisualizer()\n    \n    # Initialize plots\n    ax1.set_xlim(0, 10)\n    ax1.set_ylim(0, 10)\n    ax1.set_title('Robot Navigation Environment')\n    ax1.set_xlabel('X Position')\n    ax1.set_ylabel('Y Position')\n    \n    # Plot obstacles\n    for obs in visualizer.obstacles:\n        circle = patches.Circle(obs, 0.2, color='red', alpha=0.5)\n        ax1.add_patch(circle)\n    \n    # Robot patch\n    robot_patch = patches.Circle((visualizer.robot_x, visualizer.robot_y), 0.1, \n                                color='blue', alpha=0.7)\n    ax1.add_patch(robot_patch)\n    \n    # State probability bars\n    ax2.set_xlabel('Confidence Level')\n    ax2.set_ylabel('State')\n    ax2.set_xlim(0, 1)\n    bars = ax2.barh(range(len(visualizer.states)), [0]*len(visualizer.states))\n    \n    # State history plot\n    ax3.set_xlabel('Time Step')\n    ax3.set_ylabel('State')\n    ax3.set_ylim(-0.5, len(visualizer.states) - 0.5)\n    state_y_positions = {state: idx for idx, state in enumerate(visualizer.states)}\n    \n    # Sensor readings plot\n    ax4.set_xlabel('Time Step')\n    ax4.set_ylabel('Distance (m)')\n    ax4.set_ylim(0, 3)\n    \n    def animate(frame):\n        # Update behavior\n        visualizer.update_behavior()\n        \n        # Update robot position\n        robot_patch.center = (visualizer.robot_x, visualizer.robot_y)\n        \n        # Update state probability bars\n        probabilities = list(visualizer.confidence_levels.values())\n        for bar, prob in zip(bars, probabilities):\n            bar.set_width(prob)\n        \n        # Update state history plot\n        ax3.clear()\n        ax3.set_ylim(-0.5, len(visualizer.states) - 0.5)\n        ax3.set_xlim(0, max(10, len(visualizer.state_history)))\n        ax3.set_xlabel('Time Step')\n        ax3.set_ylabel('State')\n        \n        for t, state in enumerate(visualizer.state_history[-20:]):  # Last 20 states\n            y_pos = state_y_positions[state]\n            ax3.plot(t + len(visualizer.state_history) - 20, y_pos, 'o', color='green', markersize=8)\n        \n        ax3.set_yticks(list(range(len(visualizer.states))))\n        ax3.set_yticklabels(visualizer.states)\n        ax3.set_title('State History')\n        \n        # Update sensor readings plot\n        ax4.clear()\n        ax4.set_ylim(0, 3)\n        ax4.set_xlim(max(0, len(visualizer.state_history)-50), len(visualizer.state_history)+1)\n        ax4.set_xlabel('Time Step')\n        ax4.set_ylabel('Distance (m)')\n        \n        # Plot sensor readings\n        ax4.plot(range(len(visualizer.state_history)), \n                [reading['front_distance'] for i in range(len(visualizer.state_history))], \n                label='Front Distance', color='blue')\n        ax4.plot(range(len(visualizer.state_history)), \n                [reading['left_distance'] for i in range(len(visualizer.state_history))], \n                label='Left Distance', color='orange')\n        ax4.plot(range(len(visualizer.state_history)), \n                [reading['right_distance'] for i in range(len(visualizer.state_history))], \n                label='Right Distance', color='purple')\n        ax4.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Obstacle Threshold')\n        ax4.legend()\n        ax4.set_title('Sensor Readings Over Time')\n        \n        return robot_patch, bars\n    \n    ani = FuncAnimation(fig, animate, frames=200, interval=100, blit=False)\n    plt.tight_layout()\n    plt.show()\n\nif __name__ == \"__main__\":\n    visualize_behavior()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"34-ros2-visualization-examples",children:"3.4 ROS2 Visualization Examples"}),"\n",(0,a.jsx)(n.h3,{id:"example-4-rviz-visualization-with-marker-arrays",children:"Example 4: RViz Visualization with Marker Arrays"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nROS2 node for publishing visualization markers for RViz\nShows robot skeleton, paths, and AI decision visualizations\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import ColorRGBA\nfrom nav_msgs.msg import Path\nfrom geometry_msgs.msg import PoseStamped\nimport math\nimport numpy as np\n\nclass RobotVizNode(Node):\n    def __init__(self):\n        super().__init__('robot_visualization_node')\n        \n        # Publisher for visualization markers\n        self.marker_pub = self.create_publisher(MarkerArray, '/visualization_marker_array', 10)\n        self.path_pub = self.create_publisher(Path, '/robot_path', 10)\n        \n        # Timer for updating visualization\n        self.timer = self.create_timer(0.1, self.update_visualization)\n        \n        # Robot state variables\n        self.robot_x = 0.0\n        self.robot_y = 0.0\n        self.robot_theta = 0.0\n        self.path_points = []\n        \n        # Initialize robot skeleton points\n        self.initialize_robot_skeleton()\n        \n        self.get_logger().info('Robot Visualization Node Started')\n\n    def initialize_robot_skeleton(self):\n        \"\"\"Define robot skeleton structure\"\"\"\n        # Body parts dimensions\n        self.torso_height = 0.8\n        self.head_radius = 0.1\n        self.arm_length = 0.5\n        self.leg_length = 0.7\n        \n        # Define skeleton points relative to robot center\n        self.skeleton_points = {\n            'head_top': Point(x=0.0, y=0.0, z=self.torso_height + self.head_radius),\n            'head_center': Point(x=0.0, y=0.0, z=self.torso_height),\n            'torso_bottom': Point(x=0.0, y=0.0, z=0.0),\n            'left_shoulder': Point(x=-0.15, y=0.0, z=self.torso_height * 0.8),\n            'right_shoulder': Point(x=0.15, y=0.0, z=self.torso_height * 0.8),\n            'left_elbow': Point(x=-0.4, y=0.0, z=self.torso_height * 0.8),\n            'right_elbow': Point(x=0.4, y=0.0, z=self.torso_height * 0.8),\n            'left_wrist': Point(x=-0.65, y=0.0, z=self.torso_height * 0.8),\n            'right_wrist': Point(x=0.65, y=0.0, z=self.torso_height * 0.8),\n            'left_hip': Point(x=-0.1, y=0.0, z=0.0),\n            'right_hip': Point(x=0.1, y=0.0, z=0.0),\n            'left_knee': Point(x=-0.1, y=0.0, z=-0.35),\n            'right_knee': Point(x=0.1, y=0.0, z=-0.35),\n            'left_ankle': Point(x=-0.1, y=0.0, z=-0.7),\n            'right_ankle': Point(x=0.1, y=0.0, z=-0.7),\n            'left_foot': Point(x=-0.1, y=0.0, z=-0.75),\n            'right_foot': Point(x=0.1, y=0.0, z=-0.75)\n        }\n        \n        # Define skeleton connections (links)\n        self.skeleton_links = [\n            # Head to torso\n            ('head_center', 'head_top'),\n            ('torso_bottom', 'head_center'),\n            \n            # Left arm\n            ('head_center', 'left_shoulder'),\n            ('left_shoulder', 'left_elbow'),\n            ('left_elbow', 'left_wrist'),\n            \n            # Right arm\n            ('head_center', 'right_shoulder'),\n            ('right_shoulder', 'right_elbow'),\n            ('right_elbow', 'right_wrist'),\n            \n            # Legs\n            ('torso_bottom', 'left_hip'),\n            ('torso_bottom', 'right_hip'),\n            ('left_hip', 'left_knee'),\n            ('right_hip', 'right_knee'),\n            ('left_knee', 'left_ankle'),\n            ('right_knee', 'right_ankle'),\n            ('left_ankle', 'left_foot'),\n            ('right_ankle', 'right_foot')\n        ]\n\n    def get_world_coordinates(self, point):\n        \"\"\"Transform local point to world coordinates considering robot pose\"\"\"\n        cos_theta = math.cos(self.robot_theta)\n        sin_theta = math.sin(self.robot_theta)\n        \n        # Rotate point\n        rotated_x = point.x * cos_theta - point.y * sin_theta\n        rotated_y = point.x * sin_theta + point.y * cos_theta\n        \n        # Translate to robot position\n        world_x = self.robot_x + rotated_x\n        world_y = self.robot_y + rotated_y\n        world_z = point.z\n        \n        return Point(x=world_x, y=world_y, z=world_z)\n\n    def create_skeleton_marker(self):\n        \"\"\"Create marker for robot skeleton\"\"\"\n        marker = Marker()\n        marker.header.frame_id = \"map\"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = \"robot_skeleton\"\n        marker.id = 0\n        marker.type = Marker.LINE_LIST\n        marker.action = Marker.ADD\n        \n        # Set pose\n        marker.pose.orientation.w = 1.0\n        \n        # Set scale\n        marker.scale.x = 0.03  # Line width\n        \n        # Set color (blue for skeleton)\n        marker.color.r = 0.0\n        marker.color.g = 0.0\n        marker.color.b = 1.0\n        marker.color.a = 0.8\n        \n        # Add points to connect\n        for start_joint, end_joint in self.skeleton_links:\n            start_local = self.skeleton_points[start_joint]\n            end_local = self.skeleton_points[end_joint]\n            \n            start_world = self.get_world_coordinates(start_local)\n            end_world = self.get_world_coordinates(end_local)\n            \n            marker.points.append(start_world)\n            marker.points.append(end_world)\n        \n        return marker\n\n    def create_joints_marker(self):\n        \"\"\"Create markers for robot joints\"\"\"\n        marker_array = MarkerArray()\n        \n        id_counter = 1\n        for joint_name, local_point in self.skeleton_points.items():\n            marker = Marker()\n            marker.header.frame_id = \"map\"\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = \"robot_joints\"\n            marker.id = id_counter\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n            \n            # Transform to world coordinates\n            world_point = self.get_world_coordinates(local_point)\n            marker.pose.position = world_point\n            marker.pose.orientation.w = 1.0\n            \n            # Set scale\n            marker.scale.x = 0.05\n            marker.scale.y = 0.05\n            marker.scale.z = 0.05\n            \n            # Set color based on joint type\n            if 'head' in joint_name:\n                marker.color.r = 1.0\n                marker.color.g = 0.0\n                marker.color.b = 0.0\n            elif 'shoulder' in joint_name or 'elbow' in joint_name or 'wrist' in joint_name:\n                marker.color.r = 0.0\n                marker.color.g = 1.0\n                marker.color.b = 0.0\n            else:  # legs\n                marker.color.r = 1.0\n                marker.color.g = 1.0\n                marker.color.b = 0.0\n            \n            marker.color.a = 0.9\n            \n            # Add text label\n            text_marker = Marker()\n            text_marker.header.frame_id = \"map\"\n            text_marker.header.stamp = self.get_clock().now().to_msg()\n            text_marker.ns = \"joint_labels\"\n            text_marker.id = id_counter + 100\n            text_marker.type = Marker.TEXT_VIEW_FACING\n            text_marker.action = Marker.ADD\n            \n            # Position text slightly above joint\n            text_marker.pose.position = world_point\n            text_marker.pose.position.z += 0.08\n            text_marker.pose.orientation.w = 1.0\n            \n            text_marker.text = joint_name.replace('_', '\\n')\n            text_marker.scale.z = 0.08\n            text_marker.color.r = 1.0\n            text_marker.color.g = 1.0\n            text_marker.color.b = 1.0\n            text_marker.color.a = 1.0\n            \n            marker_array.markers.append(marker)\n            marker_array.markers.append(text_marker)\n            \n            id_counter += 1\n        \n        return marker_array\n\n    def create_path_marker(self):\n        \"\"\"Create path marker for robot trajectory\"\"\"\n        path_msg = Path()\n        path_msg.header.frame_id = \"map\"\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        \n        # Create path points\n        for i in range(max(0, len(self.path_points) - 20), len(self.path_points)):\n            pose = PoseStamped()\n            pose.header.frame_id = \"map\"\n            pose.header.stamp = self.get_clock().now().to_msg()\n            pose.pose.position.x = self.path_points[i][0]\n            pose.pose.position.y = self.path_points[i][1]\n            pose.pose.position.z = 0.0\n            pose.pose.orientation.w = 1.0\n            path_msg.poses.append(pose)\n        \n        return path_msg\n\n    def create_decision_markers(self):\n        \"\"\"Create markers for AI decisions and planning\"\"\"\n        marker_array = MarkerArray()\n        \n        # Goal marker\n        goal_marker = Marker()\n        goal_marker.header.frame_id = \"map\"\n        goal_marker.header.stamp = self.get_clock().now().to_msg()\n        goal_marker.ns = \"ai_goals\"\n        goal_marker.id = 200\n        goal_marker.type = Marker.SPHERE\n        goal_marker.action = Marker.ADD\n        \n        # Simple goal at (5, 0, 0)\n        goal_marker.pose.position.x = 5.0\n        goal_marker.pose.position.y = 0.0\n        goal_marker.pose.position.z = 0.1\n        goal_marker.pose.orientation.w = 1.0\n        \n        goal_marker.scale.x = 0.3\n        goal_marker.scale.y = 0.3\n        goal_marker.scale.z = 0.3\n        \n        goal_marker.color.r = 0.0\n        goal_marker.color.g = 1.0\n        goal_marker.color.b = 0.0\n        goal_marker.color.a = 0.7\n        \n        # Obstacle markers (simple simulation)\n        for i, (obs_x, obs_y) in enumerate([(2, 1), (3, -1), (4, 1.5)]):\n            obs_marker = Marker()\n            obs_marker.header.frame_id = \"map\"\n            obs_marker.header.stamp = self.get_clock().now().to_msg()\n            obs_marker.ns = \"obstacles\"\n            obs_marker.id = 300 + i\n            obs_marker.type = Marker.CYLINDER\n            obs_marker.action = Marker.ADD\n            \n            obs_marker.pose.position.x = obs_x\n            obs_marker.pose.position.y = obs_y\n            obs_marker.pose.position.z = 0.3\n            obs_marker.pose.orientation.w = 1.0\n            \n            obs_marker.scale.x = 0.4\n            obs_marker.scale.y = 0.4\n            obs_marker.scale.z = 0.6\n            \n            obs_marker.color.r = 1.0\n            obs_marker.color.g = 0.0\n            obs_marker.color.b = 0.0\n            obs_marker.color.a = 0.6\n        \n        marker_array.markers.extend([goal_marker, obs_marker])\n        return marker_array\n\n    def update_visualization(self):\n        \"\"\"Update all visualization markers\"\"\"\n        # Update robot position (simple circular motion for demo)\n        t = self.get_clock().now().nanoseconds / 1e9\n        self.robot_x = 2.0 * math.cos(0.5 * t)\n        self.robot_y = 2.0 * math.sin(0.5 * t)\n        self.robot_theta = 0.5 * t  # Robot heading\n        \n        # Add to path\n        self.path_points.append((self.robot_x, self.robot_y))\n        if len(self.path_points) > 200:  # Limit path length\n            self.path_points.pop(0)\n        \n        # Create and publish markers\n        marker_array = MarkerArray()\n        \n        # Add skeleton marker\n        skeleton_marker = self.create_skeleton_marker()\n        marker_array.markers.append(skeleton_marker)\n        \n        # Add joint markers\n        joint_markers = self.create_joints_marker()\n        marker_array.markers.extend(joint_markers.markers)\n        \n        # Add decision markers\n        decision_markers = self.create_decision_markers()\n        marker_array.markers.extend(decision_markers.markers)\n        \n        self.marker_pub.publish(marker_array)\n        \n        # Publish path\n        path_msg = self.create_path_marker()\n        self.path_pub.publish(path_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    viz_node = RobotVizNode()\n    \n    try:\n        rclpy.spin(viz_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        viz_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"example-5-animation-of-walking-pattern",children:"Example 5: Animation of Walking Pattern"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nAnimation of humanoid walking pattern\nDemonstrates gait cycles and coordination between limbs\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport math\n\nclass WalkingPatternVisualizer:\n    def __init__(self):\n        # Robot dimensions\n        self.body_height = 1.0\n        self.hip_width = 0.2\n        self.thigh_length = 0.4\n        self.shin_length = 0.4\n        self.upper_arm_length = 0.3\n        self.forearm_length = 0.3\n        \n        # Walking gait parameters\n        self.stride_length = 0.6\n        self.step_height = 0.1\n        self.gait_cycle_time = 2.0  # seconds per step cycle\n        \n        # Initialize joint angles\n        self.joint_angles = {\n            'left_hip_pitch': 0.0,\n            'left_knee': 0.0,\n            'left_ankle': 0.0,\n            'right_hip_pitch': 0.0,\n            'right_knee': 0.0,\n            'right_ankle': 0.0,\n            'left_shoulder_pitch': 0.0,\n            'left_elbow': 0.0,\n            'right_shoulder_pitch': 0.0,\n            'right_elbow': 0.0,\n            'torso_pitch': 0.0\n        }\n    \n    def calculate_leg_position(self, leg_side, phase, speed=0.5):\n        \"\"\"Calculate leg position based on gait phase\"\"\"\n        # Gait phase: 0 to 1, where 0 is heel strike, 0.5 is mid-stance, 1 is toe-off\n        if leg_side == 'left':\n            # Left leg swings forward during first half of cycle\n            swing_phase = phase if phase < 0.5 else 1.0 - phase\n        else:\n            # Right leg swings forward during second half of cycle\n            swing_phase = phase - 0.5 if phase >= 0.5 else phase + 0.5\n        \n        # Hip movement (forward/backward)\n        hip_offset_x = self.stride_length * (swing_phase - 0.5) * speed\n        \n        # Knee lift for swing phase\n        knee_lift = self.step_height * math.sin(math.pi * swing_phase) * 2.0  # Amplified for clear visualization\n        \n        # Calculate joint angles using inverse kinematics approximation\n        leg_length = self.thigh_length + self.shin_length\n        \n        # Simple sagittal plane gait\n        if leg_side == 'left':\n            hip_pitch = math.atan2(knee_lift, hip_offset_x + self.stride_length/2) if abs(knee_lift) > 0.01 else 0\n            knee_angle = -math.pi/3 + knee_lift * 0.5  # Knee flexion during swing\n            ankle_angle = -hip_pitch * 0.5  # Ankle compensation\n        else:\n            hip_pitch = math.atan2(-knee_lift, -(hip_offset_x + self.stride_length/2)) if abs(knee_lift) > 0.01 else 0\n            knee_angle = math.pi/3 - knee_lift * 0.5  # Opposite knee bend\n            ankle_angle = hip_pitch * 0.5  # Ankle compensation\n        \n        return hip_pitch, knee_angle, ankle_angle\n    \n    def calculate_arm_swing(self, phase, leg_side):\n        \"\"\"Calculate arm swing synchronized with leg movement\"\"\"\n        # Arms swing opposite to legs\n        if leg_side == 'left':\n            arm_phase = phase + 0.5 if phase < 0.5 else phase - 0.5\n        else:\n            arm_phase = phase\n        \n        # Shoulder and elbow movement\n        shoulder_swing = 0.2 * math.sin(2 * math.pi * arm_phase)\n        elbow_swing = 0.15 * math.sin(2 * math.pi * arm_phase + math.pi)  # Opposite to shoulder\n        \n        return shoulder_swing, elbow_swing\n    \n    def calculate_forward_kinematics(self, side, hip_angle, knee_angle, ankle_angle):\n        \"\"\"Calculate position of foot given joint angles\"\"\"\n        if side == 'left':\n            hip_x, hip_y = -self.hip_width/2, self.body_height\n        else:\n            hip_x, hip_y = self.hip_width/2, self.body_height\n        \n        # Thigh endpoint\n        thigh_x = hip_x + self.thigh_length * math.sin(hip_angle)\n        thigh_y = hip_y - self.thigh_length * math.cos(hip_angle)\n        \n        # Shin endpoint (knee to ankle)\n        knee_x = thigh_x + self.shin_length * math.sin(hip_angle + knee_angle)\n        knee_y = thigh_y - self.shin_length * math.cos(hip_angle + knee_angle)\n        \n        # Foot endpoint (ankle to toe)\n        foot_x = knee_x + 0.1 * math.sin(hip_angle + knee_angle + ankle_angle)\n        foot_y = knee_y - 0.1 * math.cos(hip_angle + knee_angle + ankle_angle)\n        \n        return (hip_x, hip_y), (thigh_x, thigh_y), (knee_x, knee_y), (foot_x, foot_y)\n    \n    def draw_humanoid(self, ax, phase):\n        \"\"\"Draw the humanoid in walking position\"\"\"\n        ax.clear()\n        \n        # Calculate leg positions\n        left_hip_ang, left_knee_ang, left_ankle_ang = self.calculate_leg_position('left', phase)\n        right_hip_ang, right_knee_ang, right_ankle_ang = self.calculate_leg_position('right', phase)\n        \n        # Calculate arm swings\n        left_shoulder_ang, left_elbow_ang = self.calculate_arm_swing(phase, 'left')\n        right_shoulder_ang, right_elbow_ang = self.calculate_arm_swing(phase, 'right')\n        \n        # Get leg positions using forward kinematics\n        left_hip_pos, left_knee_pos, left_ankle_pos, left_foot_pos = self.calculate_forward_kinematics(\n            'left', left_hip_ang, left_knee_ang, left_ankle_ang)\n        right_hip_pos, right_knee_pos, right_ankle_pos, right_foot_pos = self.calculate_forward_kinematics(\n            'right', right_hip_ang, right_knee_ang, right_ankle_ang)\n        \n        # Draw body (torso)\n        torso_x = 0\n        torso_y = self.body_height\n        neck_x = 0\n        neck_y = self.body_height + 0.2\n        head_x = 0\n        head_y = self.body_height + 0.3\n        \n        ax.plot([torso_x, neck_x], [torso_y, neck_y], 'k-', linewidth=4, label='Torso')\n        ax.plot([neck_x - 0.05, neck_x + 0.05], [neck_y, neck_y], 'k-', linewidth=2)  # Shoulders\n        ax.scatter(head_x, head_y, c='lightblue', s=100, zorder=5, label='Head')\n        \n        # Draw left leg\n        ax.plot([left_hip_pos[0], left_knee_pos[0], left_ankle_pos[0], left_foot_pos[0]],\n                [left_hip_pos[1], left_knee_pos[1], left_ankle_pos[1], left_foot_pos[1]], \n                'b-', linewidth=3, label='Left Leg')\n        \n        # Draw right leg\n        ax.plot([right_hip_pos[0], right_knee_pos[0], right_ankle_pos[0], right_foot_pos[0]],\n                [right_hip_pos[1], right_knee_pos[1], right_ankle_pos[1], right_foot_pos[1]], \n                'r-', linewidth=3, label='Right Leg')\n        \n        # Draw arms (simplified)\n        # Left arm\n        left_shoulder_x = -0.15\n        left_elbow_x = left_shoulder_x + self.upper_arm_length * math.sin(left_shoulder_ang)\n        left_elbow_y = neck_y - self.upper_arm_length * math.cos(left_shoulder_ang)\n        left_wrist_x = left_elbow_x + self.forearm_length * math.sin(left_shoulder_ang + left_elbow_ang)\n        left_wrist_y = left_elbow_y - self.forearm_length * math.cos(left_shoulder_ang + left_elbow_ang)\n        \n        ax.plot([left_shoulder_x, left_elbow_x, left_wrist_x],\n                [neck_y, left_elbow_y, left_wrist_y],\n                'g-', linewidth=2, label='Left Arm')\n        \n        # Right arm\n        right_shoulder_x = 0.15\n        right_elbow_x = right_shoulder_x + self.upper_arm_length * math.sin(right_shoulder_ang)\n        right_elbow_y = neck_y - self.upper_arm_length * math.cos(right_shoulder_ang)\n        right_wrist_x = right_elbow_x + self.forearm_length * math.sin(right_shoulder_ang + right_elbow_ang)\n        right_wrist_y = right_elbow_y - self.forearm_length * math.cos(right_shoulder_ang + right_elbow_ang)\n        \n        ax.plot([right_shoulder_x, right_elbow_x, right_wrist_x],\n                [neck_y, right_elbow_y, right_wrist_y],\n                'g-', linewidth=2, label='Right Arm')\n        \n        # Mark joints\n        ax.scatter(left_hip_pos[0], left_hip_pos[1], c='red', s=50, zorder=5)\n        ax.scatter(left_knee_pos[0], left_knee_pos[1], c='red', s=50, zorder=5)\n        ax.scatter(left_ankle_pos[0], left_ankle_pos[1], c='red', s=50, zorder=5)\n        ax.scatter(left_foot_pos[0], left_foot_pos[1], c='red', s=50, zorder=5)\n        \n        ax.scatter(right_hip_pos[0], right_hip_pos[1], c='red', s=50, zorder=5)\n        ax.scatter(right_knee_pos[0], right_knee_pos[1], c='red', s=50, zorder=5)\n        ax.scatter(right_ankle_pos[0], right_ankle_pos[1], c='red', s=50, zorder=5)\n        ax.scatter(right_foot_pos[0], right_foot_pos[1], c='red', s=50, zorder=5)\n        \n        ax.scatter([-0.15, 0.15], [neck_y, neck_y], c='red', s=50, zorder=5)  # Shoulders\n        ax.scatter([left_elbow_x, right_elbow_x], [left_elbow_y, right_elbow_y], c='red', s=40, zorder=5)\n        ax.scatter([left_wrist_x, right_wrist_x], [left_wrist_y, right_wrist_y], c='red', s=40, zorder=5)\n        \n        # Add gait phase indicator\n        ax.text(0.02, 0.98, f'Gait Phase: {phase:.2f}', transform=ax.transAxes, \n                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n        \n        # Set axis properties\n        ax.set_xlim(-1.0, 1.0)\n        ax.set_ylim(-0.2, 1.5)\n        ax.set_aspect('equal')\n        ax.set_title('Humanoid Walking Animation')\n        ax.grid(True, alpha=0.3)\n        \n        # Add legend\n        ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n\ndef animate_walking():\n    \"\"\"Create walking animation\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    walker = WalkingPatternVisualizer()\n    \n    def animate(frame):\n        # Calculate gait phase (0 to 1)\n        phase = (frame % 120) / 120.0  # 120 frames per gait cycle\n        walker.draw_humanoid(ax, phase)\n    \n    ani = FuncAnimation(fig, animate, frames=240, interval=100, blit=False)\n    plt.tight_layout()\n    plt.show()\n\nif __name__ == \"__main__\":\n    animate_walking()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"35-laboratory-exercises",children:"3.5 Laboratory Exercises"}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-1-robot-state-visualization-dashboard",children:"Lab Exercise 1: Robot State Visualization Dashboard"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Objective"}),": Create a comprehensive visualization dashboard that displays multiple aspects of robot state simultaneously."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Components to Implement"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Robot skeleton visualization in 3D space"}),"\n",(0,a.jsx)(n.li,{children:"Joint angle displays with gauges"}),"\n",(0,a.jsx)(n.li,{children:"Sensor readings visualization"}),"\n",(0,a.jsx)(n.li,{children:"Path planning and navigation display"}),"\n",(0,a.jsx)(n.li,{children:"AI decision confidence indicators"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Create a main display showing the robot's current pose"}),"\n",(0,a.jsx)(n.li,{children:"Add real-time joint angle displays using gauge widgets"}),"\n",(0,a.jsx)(n.li,{children:"Implement sensor fusion visualization showing multiple input sources"}),"\n",(0,a.jsx)(n.li,{children:"Create path planning visualization with cost maps"}),"\n",(0,a.jsx)(n.li,{children:"Add decision-making visualization showing state transitions"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-2-interactive-animation-editor",children:"Lab Exercise 2: Interactive Animation Editor"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Objective"}),": Develop an interactive tool for creating and editing robot animations."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Features to Include"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Timeline controls for animation sequences"}),"\n",(0,a.jsx)(n.li,{children:"Keyframe editor for major poses"}),"\n",(0,a.jsx)(n.li,{children:"Interpolation between keyframes"}),"\n",(0,a.jsx)(n.li,{children:"Preview playback functionality"}),"\n",(0,a.jsx)(n.li,{children:"Export to ROS trajectory format"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Design a timeline interface with frame-by-frame controls"}),"\n",(0,a.jsx)(n.li,{children:"Implement pose editor for defining key joint positions"}),"\n",(0,a.jsx)(n.li,{children:"Add interpolation algorithms (linear, cubic, B\xe9zier)"}),"\n",(0,a.jsx)(n.li,{children:"Create preview window with real-time animation"}),"\n",(0,a.jsx)(n.li,{children:"Implement trajectory export functionality"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-exercise-3-ai-behavior-tree-visualizer",children:"Lab Exercise 3: AI Behavior Tree Visualizer"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Objective"}),": Create a real-time visualizer for AI behavior trees and state machines."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Components to Implement"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Behavior tree graph visualization"}),"\n",(0,a.jsx)(n.li,{children:"Real-time node activation highlighting"}),"\n",(0,a.jsx)(n.li,{children:"Decision flow visualization"}),"\n",(0,a.jsx)(n.li,{children:"Performance metrics display"}),"\n",(0,a.jsx)(n.li,{children:"Debug information overlay"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Parse behavior tree structure from ROS parameters"}),"\n",(0,a.jsx)(n.li,{children:"Create graphical representation of tree nodes"}),"\n",(0,a.jsx)(n.li,{children:"Highlight active nodes during execution"}),"\n",(0,a.jsx)(n.li,{children:"Show transition arrows indicating decision flow"}),"\n",(0,a.jsx)(n.li,{children:"Add performance counters for each node"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"36-advanced-visualization-techniques",children:"3.6 Advanced Visualization Techniques"}),"\n",(0,a.jsx)(n.h3,{id:"361-real-time-rendering-with-opengl",children:"3.6.1 Real-Time Rendering with OpenGL"}),"\n",(0,a.jsx)(n.p,{children:"For high-performance visualization, OpenGL can be used to render complex robot models and environments with high fidelity."}),"\n",(0,a.jsx)(n.h3,{id:"362-augmented-reality-integration",children:"3.6.2 Augmented Reality Integration"}),"\n",(0,a.jsx)(n.p,{children:"AR technologies can overlay robot visualization onto real-world environments, enhancing teleoperation and debugging capabilities."}),"\n",(0,a.jsx)(n.h3,{id:"363-data-driven-visualization",children:"3.6.3 Data-Driven Visualization"}),"\n",(0,a.jsx)(n.p,{children:"Visualization systems that adapt based on incoming sensor data, showing relevant information dynamically as the robot operates."}),"\n",(0,a.jsx)(n.h2,{id:"37-best-practices-for-visualization",children:"3.7 Best Practices for Visualization"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance Optimization"}),": Keep visualization code efficient to avoid impacting robot control timing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Modular Design"}),": Separate visualization concerns from control logic"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scalability"}),": Ensure visualization scales appropriately with robot complexity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"User-Friendliness"}),": Design intuitive interfaces for various user expertise levels"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robustness"}),": Handle visualization failures gracefully without affecting robot operation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"38-summary",children:"3.8 Summary"}),"\n",(0,a.jsx)(n.p,{children:"Visualization and animation are essential tools for developing, debugging, and demonstrating humanoid robotics AI systems. This chapter has covered various techniques from basic 2D visualization to complex 3D simulation environments. The examples provided demonstrate how to visualize robot states, AI decision-making processes, and coordinated movements."}),"\n",(0,a.jsx)(n.p,{children:"Effective visualization bridges the gap between abstract AI algorithms and physical robot behaviors, making complex systems more transparent and controllable. As humanoid robotics continues to advance, sophisticated visualization tools will become increasingly important for both developers and end-users."}),"\n",(0,a.jsx)(n.h2,{id:"39-discussion-questions",children:"3.9 Discussion Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the key challenges in visualizing multi-degree-of-freedom humanoid robots?"}),"\n",(0,a.jsx)(n.li,{children:"How can real-time performance be maintained while providing rich visualization?"}),"\n",(0,a.jsx)(n.li,{children:"What role does visualization play in debugging AI decision-making systems?"}),"\n",(0,a.jsx)(n.li,{children:"How might AR technologies enhance humanoid robot visualization?"}),"\n",(0,a.jsx)(n.li,{children:"What are the most important elements to visualize when demonstrating robot capabilities to non-experts?"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var t=i(6540);const a={},s=t.createContext(a);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkphysical_ai_robotics_website=globalThis.webpackChunkphysical_ai_robotics_website||[]).push([[876],{3748:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>_,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter_6_sensors_perception","title":"Chapter 6: Sensors and Perception Systems for Humanoid Robotics","description":"6.1 Introduction to Sensor Systems in Humanoid Robotics","source":"@site/docs/chapter_6_sensors_perception.md","sourceDirName":".","slug":"/chapter_6_sensors_perception","permalink":"/physical-ai-robotics-website/docs/chapter_6_sensors_perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapter_6_sensors_perception.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Control Architectures and System Integration for Humanoid Robotics","permalink":"/physical-ai-robotics-website/docs/chapter_5_control_architecture"}}');var i=t(4848),a=t(8453);const o={},r="Chapter 6: Sensors and Perception Systems for Humanoid Robotics",l={},c=[{value:"6.1 Introduction to Sensor Systems in Humanoid Robotics",id:"61-introduction-to-sensor-systems-in-humanoid-robotics",level:2},{value:"Sensor Requirements for Humanoid Robots",id:"sensor-requirements-for-humanoid-robots",level:3},{value:"6.2 Proprioceptive Sensors",id:"62-proprioceptive-sensors",level:2},{value:"6.2.1 Joint Position and Velocity Sensors",id:"621-joint-position-and-velocity-sensors",level:3},{value:"6.2.2 Inertial Measurement Units (IMU)",id:"622-inertial-measurement-units-imu",level:3},{value:"6.2.3 Force and Torque Sensors",id:"623-force-and-torque-sensors",level:3},{value:"6.3 Exteroceptive Sensors",id:"63-exteroceptive-sensors",level:2},{value:"6.3.1 Vision Systems",id:"631-vision-systems",level:3},{value:"6.3.2 LIDAR Systems",id:"632-lidar-systems",level:3},{value:"6.4 Sensor Fusion and Integration",id:"64-sensor-fusion-and-integration",level:2},{value:"6.4.1 Kalman Filter for Sensor Fusion",id:"641-kalman-filter-for-sensor-fusion",level:3},{value:"6.5 ROS2 Integration for Sensor Systems",id:"65-ros2-integration-for-sensor-systems",level:2},{value:"6.5.1 Sensor Data Publishing",id:"651-sensor-data-publishing",level:3},{value:"6.6 Laboratory Exercises",id:"66-laboratory-exercises",level:2},{value:"Lab Exercise 1: IMU Calibration and Orientation Estimation",id:"lab-exercise-1-imu-calibration-and-orientation-estimation",level:3},{value:"Lab Exercise 2: Vision-Based Object Tracking",id:"lab-exercise-2-vision-based-object-tracking",level:3},{value:"Lab Exercise 3: LIDAR-based Navigation",id:"lab-exercise-3-lidar-based-navigation",level:3},{value:"6.7 Sensor Selection and Integration Guidelines",id:"67-sensor-selection-and-integration-guidelines",level:2},{value:"6.7.1 Requirements Analysis",id:"671-requirements-analysis",level:3},{value:"6.7.2 Sensor Redundancy and Safety",id:"672-sensor-redundancy-and-safety",level:3},{value:"6.8 Summary",id:"68-summary",level:2},{value:"6.9 Discussion Questions",id:"69-discussion-questions",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-6-sensors-and-perception-systems-for-humanoid-robotics",children:"Chapter 6: Sensors and Perception Systems for Humanoid Robotics"})}),"\n",(0,i.jsx)(e.h2,{id:"61-introduction-to-sensor-systems-in-humanoid-robotics",children:"6.1 Introduction to Sensor Systems in Humanoid Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Sensors form the foundation of autonomous behavior in humanoid robotics, enabling robots to perceive their environment, monitor their state, and interact safely with humans and objects. Unlike traditional industrial robots operating in structured environments, humanoid robots must navigate complex, dynamic environments while maintaining balance and safety."}),"\n",(0,i.jsx)(e.p,{children:"The sensor suite of a humanoid robot typically includes:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Proprioceptive sensors"}),": Monitor internal state (joint angles, motor currents, IMU)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Exteroceptive sensors"}),": Perceive external environment (cameras, LIDAR, force sensors)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cognitive sensors"}),": Assess interaction context (microphones, gesture recognition)"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"sensor-requirements-for-humanoid-robots",children:"Sensor Requirements for Humanoid Robots"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid robots have unique sensor requirements due to their dynamic nature:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"High-frequency sampling"}),": Balance control requires 1000Hz+ IMU data"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Robustness"}),": Sensors must withstand impacts and vibrations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Low latency"}),": Real-time control demands minimal sensor delay"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Redundancy"}),": Multiple sensors for critical functions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human-safe operation"}),": Sensors must not pose risks to humans"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"62-proprioceptive-sensors",children:"6.2 Proprioceptive Sensors"}),"\n",(0,i.jsx)(e.h3,{id:"621-joint-position-and-velocity-sensors",children:"6.2.1 Joint Position and Velocity Sensors"}),"\n",(0,i.jsx)(e.p,{children:"Joint encoders provide absolute or relative position feedback for each degree of freedom in the humanoid robot."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nJoint encoder interface for humanoid robotics\nDemonstrates position and velocity feedback processing\n"""\n\nimport numpy as np\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\n\n@dataclass\nclass JointState:\n    """Data structure for joint state information"""\n    name: str\n    position: float  # Radians\n    velocity: float  # Rad/s\n    effort: float    # N\xb7m\n    timestamp: float\n\nclass JointEncoderArray:\n    """Interface for managing multiple joint encoders"""\n    def __init__(self, joint_names: List[str], encoder_resolution: int = 4096):\n        self.joint_names = joint_names\n        self.encoder_resolution = encoder_resolution  # Counts per revolution\n        self.num_joints = len(joint_names)\n        \n        # Initialize with neutral position\n        self.current_positions = [0.0] * self.num_joints\n        self.last_positions = [0.0] * self.num_joints\n        self.velocities = [0.0] * self.num_joints\n        self.efforts = [0.0] * self.num_joints\n        \n        # For velocity calculation\n        self.last_timestamps = [time.time()] * self.num_joints\n        self.position_history = {name: deque(maxlen=10) for name in joint_names}\n        \n        # Velocity filtering\n        self.velocity_filters = {name: SimpleLowPassFilter() for name in joint_names}\n        \n    def update_reading(self, joint_name: str, encoder_value: int):\n        """Update with new encoder reading"""\n        if joint_name not in self.position_history:\n            raise ValueError(f"Unknown joint: {joint_name}")\n        \n        # Convert encoder value to radians\n        position = self._encoder_to_radians(encoder_value)\n        \n        # Calculate velocity\n        idx = self.joint_names.index(joint_name)\n        current_time = time.time()\n        dt = current_time - self.last_timestamps[idx]\n        \n        if dt > 0:\n            raw_velocity = (position - self.current_positions[idx]) / dt\n            # Apply low-pass filter\n            filtered_velocity = self.velocity_filters[joint_name].filter(raw_velocity)\n            \n            self.velocities[idx] = filtered_velocity\n            self.last_timestamps[idx] = current_time\n        \n        self.last_positions[idx] = self.current_positions[idx]\n        self.current_positions[idx] = position\n        self.position_history[joint_name].append(position)\n    \n    def _encoder_to_radians(self, encoder_value: int) -> float:\n        """Convert encoder counts to radians"""\n        # Assuming single revolution = encoder_resolution counts\n        return (encoder_value % self.encoder_resolution) * 2 * np.pi / self.encoder_resolution\n    \n    def get_joint_state(self, joint_name: str) -> JointState:\n        """Get current state of specified joint"""\n        if joint_name not in self.position_history:\n            raise ValueError(f"Unknown joint: {joint_name}")\n        \n        idx = self.joint_names.index(joint_name)\n        return JointState(\n            name=joint_name,\n            position=self.current_positions[idx],\n            velocity=self.velocities[idx],\n            effort=self.efforts[idx],\n            timestamp=time.time()\n        )\n    \n    def get_all_states(self) -> List[JointState]:\n        """Get state of all joints"""\n        all_states = []\n        current_time = time.time()\n        \n        for i, name in enumerate(self.joint_names):\n            state = JointState(\n                name=name,\n                position=self.current_positions[i],\n                velocity=self.velocities[i],\n                effort=self.efforts[i],\n                timestamp=current_time\n            )\n            all_states.append(state)\n        \n        return all_states\n\nclass SimpleLowPassFilter:\n    """Simple low-pass filter for velocity calculation"""\n    def __init__(self, alpha: float = 0.1):\n        self.alpha = alpha\n        self.last_output = 0.0\n        self.first_call = True\n    \n    def filter(self, input_value: float) -> float:\n        if self.first_call:\n            self.last_output = input_value\n            self.first_call = False\n            return input_value\n        \n        output = self.alpha * input_value + (1.0 - self.alpha) * self.last_output\n        self.last_output = output\n        return output\n\ndef simulate_joint_encoders():\n    """Simulate joint encoder operation"""\n    joint_names = [\n        \'left_hip_roll\', \'left_hip_pitch\', \'left_knee\', \'left_ankle_pitch\', \'left_ankle_roll\',\n        \'right_hip_roll\', \'right_hip_pitch\', \'right_knee\', \'right_ankle_pitch\', \'right_ankle_roll\'\n    ]\n    \n    encoders = JointEncoderArray(joint_names)\n    \n    print("Simulating joint encoder updates...")\n    \n    for step in range(100):\n        # Simulate encoder readings with some movement\n        for i, joint_name in enumerate(joint_names):\n            # Simulate sinusoidal motion\n            angle = 0.2 * np.sin(0.1 * step + i * 0.5)\n            encoder_counts = int((angle / (2 * np.pi)) * encoders.encoder_resolution)\n            # Add noise\n            encoder_counts += np.random.randint(-5, 5)\n            \n            encoders.update_reading(joint_name, encoder_counts)\n        \n        if step % 20 == 0:\n            # Print state of first few joints\n            for joint_name in joint_names[:3]:\n                state = encoders.get_joint_state(joint_name)\n                print(f"{state.name}: pos={state.position:.3f}, vel={state.velocity:.3f}, effort={state.effort:.3f}")\n    \n    print("Simulation completed.")\n\nif __name__ == "__main__":\n    simulate_joint_encoders()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"622-inertial-measurement-units-imu",children:"6.2.2 Inertial Measurement Units (IMU)"}),"\n",(0,i.jsx)(e.p,{children:"IMUs are critical for balance control, providing orientation, angular velocity, and acceleration data."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nIMU interface and processing for humanoid robotics\nDemonstrates orientation estimation and balance monitoring\n"""\n\nimport numpy as np\nimport time\nfrom dataclasses import dataclass\nfrom typing import Tuple\nimport math\n\n@dataclass\nclass IMUData:\n    """IMU data structure"""\n    timestamp: float\n    linear_acceleration: np.ndarray  # [ax, ay, az]\n    angular_velocity: np.ndarray     # [wx, wy, wz]\n    orientation: np.ndarray          # [x, y, z, w] - quaternion\n    magnetic_field: Optional[np.ndarray] = None\n\nclass IMUFilter:\n    """IMU data processing and filtering"""\n    def __init__(self, sample_rate: float = 1000.0):\n        self.sample_rate = sample_rate\n        self.dt = 1.0 / sample_rate\n        \n        # Initial state\n        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # [x, y, z, w]\n        self.gravity_vector = np.array([0.0, 0.0, -9.81])\n        \n        # Bias estimation\n        self.gyro_bias = np.zeros(3)\n        self.accel_bias = np.zeros(3)\n        \n        # For drift correction\n        self.accel_history = deque(maxlen=50)\n        self.mag_history = deque(maxlen=50)\n        \n    def process_raw_data(self, accel: np.ndarray, gyro: np.ndarray, mag: np.ndarray = None) -> IMUData:\n        """Process raw IMU data into filtered orientation"""\n        current_time = time.time()\n        \n        # Apply bias correction\n        corrected_gyro = gyro - self.gyro_bias\n        corrected_accel = accel - self.accel_bias\n        \n        # Update orientation using gyro integration\n        self._integrate_gyro(corrected_gyro)\n        \n        # Correct orientation using accelerometer\n        self._correct_with_accelerometer(corrected_accel)\n        \n        # Store for bias estimation\n        self.accel_history.append(corrected_accel)\n        if mag is not None:\n            self.mag_history.append(mag)\n        \n        # Update bias estimates periodically\n        if len(self.accel_history) == 50:\n            self._estimate_biases()\n        \n        return IMUData(\n            timestamp=current_time,\n            linear_acceleration=accel,\n            angular_velocity=gyro,\n            orientation=self.orientation.copy(),\n            magnetic_field=mag\n        )\n    \n    def _integrate_gyro(self, gyro: np.ndarray):\n        """Integrate gyroscope data to update orientation"""\n        # Convert angular velocity to quaternion derivative\n        omega_quat = np.array([gyro[0], gyro[1], gyro[2], 0.0])\n        omega_quat = self._quaternion_multiply(omega_quat, self.orientation) * 0.5\n        \n        # Integrate\n        new_orientation = self.orientation + omega_quat * self.dt\n        \n        # Normalize quaternion\n        self.orientation = new_orientation / np.linalg.norm(new_orientation)\n    \n    def _correct_with_accelerometer(self, accel: np.ndarray):\n        """Use accelerometer to correct orientation drift"""\n        # Normalize accelerometer reading\n        accel_norm = accel / np.linalg.norm(accel)\n        \n        # Convert reference gravity vector to body frame\n        gravity_body = self._rotate_vector_by_quaternion(self.gravity_vector, \n                                                        self._quaternion_conjugate(self.orientation))\n        \n        # Calculate correction quaternion\n        correction_angle = np.arccos(np.clip(np.dot(gravity_body, accel_norm), -1.0, 1.0))\n        \n        if correction_angle > 0.01:  # Only correct if significant difference\n            correction_axis = np.cross(gravity_body, accel_norm)\n            correction_axis = correction_axis / np.linalg.norm(correction_axis)\n            \n            # Create correction quaternion\n            correction_quat = np.array([\n                correction_axis[0] * np.sin(correction_angle / 2),\n                correction_axis[1] * np.sin(correction_angle / 2),\n                correction_axis[2] * np.sin(correction_angle / 2),\n                np.cos(correction_angle / 2)\n            ])\n            \n            # Apply correction with small gain\n            self.orientation = self._slerp(self.orientation, correction_quat, 0.01)\n    \n    def _estimate_biases(self):\n        """Estimate and update bias values"""\n        # Accelerometer bias: average should match gravity magnitude\n        avg_accel = np.mean(self.accel_history, axis=0)\n        gravity_magnitude = np.linalg.norm(avg_accel)\n        \n        if abs(gravity_magnitude - 9.81) < 0.5:  # Reasonable gravity reading\n            self.accel_bias = avg_accel - np.array([0.0, 0.0, -9.81])\n    \n    def _quaternion_multiply(self, q1: np.ndarray, q2: np.ndarray) -> np.ndarray:\n        """Multiply two quaternions"""\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n        \n        return np.array([\n            w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,\n            w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,\n            w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2,\n            w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2\n        ])\n    \n    def _quaternion_conjugate(self, q: np.ndarray) -> np.ndarray:\n        """Get quaternion conjugate"""\n        return np.array([-q[0], -q[1], -q[2], q[3]])\n    \n    def _rotate_vector_by_quaternion(self, v: np.ndarray, q: np.ndarray) -> np.ndarray:\n        """Rotate vector by quaternion"""\n        v_quat = np.array([v[0], v[1], v[2], 0.0])\n        q_conj = self._quaternion_conjugate(q)\n        \n        rotated = self._quaternion_multiply(self._quaternion_multiply(q, v_quat), q_conj)\n        return rotated[:3]\n    \n    def _slerp(self, q1: np.ndarray, q2: np.ndarray, t: float) -> np.ndarray:\n        """Spherical linear interpolation between quaternions"""\n        dot = np.dot(q1, q2)\n        \n        if dot < 0.0:\n            q2 = -q2\n            dot = -dot\n        \n        if dot > 0.9995:\n            result = q1 + t * (q2 - q1)\n            return result / np.linalg.norm(result)\n        \n        theta_0 = np.arccos(np.clip(dot, -1.0, 1.0))\n        sin_theta_0 = np.sin(theta_0)\n        theta = theta_0 * t\n        sin_theta = np.sin(theta)\n        \n        s0 = np.cos(theta) - dot * sin_theta / sin_theta_0\n        s1 = sin_theta / sin_theta_0\n        \n        result = s0 * q1 + s1 * q2\n        return result / np.linalg.norm(result)\n    \n    def get_euler_angles(self) -> np.ndarray:\n        """Convert quaternion to Euler angles [roll, pitch, yaw]"""\n        w, x, y, z = self.orientation\n        \n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\n        \n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = np.copysign(np.pi / 2, sinp)\n        else:\n            pitch = np.arcsin(sinp)\n        \n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\n        \n        return np.array([roll, pitch, yaw])\n\ndef simulate_imu_processing():\n    """Simulate IMU data processing"""\n    imu_filter = IMUFilter(sample_rate=500.0)  # 500 Hz\n    \n    print("Simulating IMU processing...")\n    \n    for step in range(500):\n        # Simulate IMU readings with some movement\n        current_time = step / 500.0\n        \n        # Accelerometer: gravity + small movement\n        accel = np.array([0.1 * np.sin(current_time), \n                         0.05 * np.cos(current_time), \n                         -9.81 + 0.1 * np.sin(2 * current_time)])\n        \n        # Gyro: rotation with drift\n        gyro = np.array([0.01 * np.cos(current_time), \n                        0.005 * np.sin(current_time), \n                        0.002 * np.sin(0.5 * current_time)])\n        \n        # Add noise\n        accel += np.random.normal(0, 0.01, 3)\n        gyro += np.random.normal(0, 0.001, 3)\n        \n        # Process data\n        imu_data = imu_filter.process_raw_data(accel, gyro)\n        \n        # Get Euler angles for balance monitoring\n        euler = imu_filter.get_euler_angles()\n        \n        if step % 50 == 0:\n            print(f"Time: {current_time:.2f}s")\n            print(f"  Roll: {np.degrees(euler[0]):.2f}\xb0, Pitch: {np.degrees(euler[1]):.2f}\xb0, Yaw: {np.degrees(euler[2]):.2f}\xb0")\n            print(f"  Bias est: gyro={imu_filter.gyro_bias} accel={imu_filter.accel_bias}")\n            print()\n\nif __name__ == "__main__":\n    from collections import deque  # Import here since it\'s used in the class\n    simulate_imu_processing()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"623-force-and-torque-sensors",children:"6.2.3 Force and Torque Sensors"}),"\n",(0,i.jsx)(e.p,{children:"Force/torque sensors enable precise interaction control and safety monitoring."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nForce/Torque sensor interface for humanoid robotics\nDemonstrates contact detection and force control\n"""\n\nimport numpy as np\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nimport threading\n\n@dataclass\nclass ForceTorqueData:\n    """Force/Torque sensor data"""\n    timestamp: float\n    forces: np.ndarray  # [fx, fy, fz]\n    torques: np.ndarray  # [tx, ty, tz]\n    contact_detected: bool\n\nclass ForceTorqueSensor:\n    """Force/Torque sensor interface"""\n    def __init__(self, sensor_location: str, force_threshold: float = 20.0):\n        self.sensor_location = sensor_location\n        self.force_threshold = force_threshold\n        self.torque_threshold = 5.0\n        \n        # Current sensor reading\n        self.forces = np.zeros(3)\n        self.torques = np.zeros(3)\n        self.contact_detected = False\n        \n        # Historical data for filtering\n        self.force_history = np.zeros((10, 3))\n        self.torque_history = np.zeros((10, 3))\n        \n        # Safety limits\n        self.max_force = 100.0\n        self.max_torque = 20.0\n        \n    def update_reading(self, raw_forces: np.ndarray, raw_torques: np.ndarray):\n        """Update with new raw sensor readings"""\n        # Apply basic filtering\n        self._update_history(raw_forces, raw_torques)\n        \n        # Calculate filtered values\n        self.forces = np.mean(self.force_history, axis=0)\n        self.torques = np.mean(self.torque_history, axis=0)\n        \n        # Detect contact\n        force_magnitude = np.linalg.norm(self.forces)\n        torque_magnitude = np.linalg.norm(self.torques)\n        \n        self.contact_detected = (force_magnitude > self.force_threshold or \n                                torque_magnitude > self.torque_threshold)\n        \n        # Check safety limits\n        if (np.any(np.abs(self.forces) > self.max_force) or \n            np.any(np.abs(self.torques) > self.max_torque)):\n            print(f"WARNING: Force/Torque limits exceeded at {self.sensor_location}")\n    \n    def _update_history(self, new_forces: np.ndarray, new_torques: np.ndarray):\n        """Update sensor history for filtering"""\n        # Roll the history arrays and add new values\n        self.force_history = np.roll(self.force_history, 1, axis=0)\n        self.force_history[0] = new_forces\n        \n        self.torque_history = np.roll(self.torque_history, 1, axis=0)\n        self.torque_history[0] = new_torques\n    \n    def get_data(self) -> ForceTorqueData:\n        """Get current sensor data"""\n        return ForceTorqueData(\n            timestamp=time.time(),\n            forces=self.forces.copy(),\n            torques=self.torques.copy(),\n            contact_detected=self.contact_detected\n        )\n    \n    def is_safe_to_proceed(self) -> bool:\n        """Check if forces are within safe limits"""\n        force_magnitude = np.linalg.norm(self.forces)\n        torque_magnitude = np.linalg.norm(self.torques)\n        \n        return (force_magnitude < self.max_force * 0.8 and \n                torque_magnitude < self.max_torque * 0.8)\n\nclass WholeBodyForceSensor:\n    """Interface for multiple force/torque sensors"""\n    def __init__(self):\n        self.sensors = {}\n        \n        # Initialize sensors at key locations\n        sensor_locations = [\n            \'left_foot\', \'right_foot\', \n            \'left_hand\', \'right_hand\',\n            \'left_ankle\', \'right_ankle\'\n        ]\n        \n        for location in sensor_locations:\n            self.sensors[location] = ForceTorqueSensor(\n                sensor_location=location\n            )\n        \n        # Zero reference values\n        self.zero_references = {loc: (np.zeros(3), np.zeros(3)) for loc in sensor_locations}\n    \n    def calibrate_sensor(self, location: str, num_samples: int = 100):\n        """Calibrate sensor to account for static loads"""\n        if location not in self.sensors:\n            raise ValueError(f"Unknown sensor location: {location}")\n        \n        print(f"Calibrating {location} sensor...")\n        \n        cumulative_forces = np.zeros(3)\n        cumulative_torques = np.zeros(3)\n        \n        for _ in range(num_samples):\n            # In real implementation, would read actual sensor values\n            # For simulation, we\'ll use nominal values\n            base_forces = np.array([0, 0, -50.0]) if \'foot\' in location else np.zeros(3)\n            base_torques = np.zeros(3)\n            \n            cumulative_forces += base_forces + np.random.normal(0, 0.1, 3)\n            cumulative_torques += base_torques + np.random.normal(0, 0.01, 3)\n            \n            time.sleep(0.001)  # Simulate sampling time\n        \n        avg_forces = cumulative_forces / num_samples\n        avg_torques = cumulative_torques / num_samples\n        \n        self.zero_references[location] = (avg_forces, avg_torques)\n        print(f"Calibration complete for {location}")\n    \n    def update_all_sensors(self, sensor_readings: dict):\n        """Update all sensors with new readings"""\n        for location, (raw_forces, raw_torques) in sensor_readings.items():\n            if location in self.sensors:\n                # Apply calibration\n                zero_forces, zero_torques = self.zero_references[location]\n                calibrated_forces = raw_forces - zero_forces\n                calibrated_torques = raw_torques - zero_torques\n                \n                self.sensors[location].update_reading(calibrated_forces, calibrated_torques)\n    \n    def get_contact_status(self) -> dict:\n        """Get contact status for all sensors"""\n        return {loc: sensor.contact_detected for loc, sensor in self.sensors.items()}\n    \n    def get_support_polygon(self) -> np.ndarray:\n        """Calculate support polygon from contact points"""\n        contact_points = []\n        \n        for location, sensor in self.sensors.items():\n            if sensor.contact_detected and \'foot\' in location:\n                # Convert sensor location to world coordinates\n                # In real implementation, would use forward kinematics\n                if location == \'left_foot\':\n                    contact_points.append([-0.1, 0.1, 0.0])  # Example position\n                elif location == \'right_foot\':\n                    contact_points.append([0.1, -0.1, 0.0])\n        \n        if contact_points:\n            return np.array(contact_points)\n        else:\n            return np.array([])\n\ndef simulate_force_torque_sensing():\n    """Simulate force/torque sensor operation"""\n    print("Simulating whole-body force/torque sensing...")\n    \n    # Initialize sensor system\n    ft_system = WholeBodyForceSensor()\n    \n    # Calibrate sensors\n    for location in [\'left_foot\', \'right_foot\', \'left_hand\', \'right_hand\']:\n        ft_system.calibrate_sensor(location)\n    \n    print("\\nStarting sensor simulation...")\n    \n    for step in range(100):\n        # Simulate sensor readings with some contact events\n        sensor_readings = {}\n        \n        # Simulate normal walking pattern\n        if step < 50:\n            # Standing phase\n            left_foot_forces = np.array([0, 0, -400]) + np.random.normal(0, 5, 3)\n            right_foot_forces = np.array([0, 0, -400]) + np.random.normal(0, 5, 3)\n        else:\n            # Single support phase\n            left_foot_forces = np.array([0, 0, -800]) + np.random.normal(0, 5, 3)\n            right_foot_forces = np.array([0, 0, -100]) + np.random.normal(0, 2, 3)\n        \n        # Hand forces (simulating object holding)\n        left_hand_forces = np.array([10, 5, -20]) + np.random.normal(0, 1, 3)\n        right_hand_forces = np.array([5, -10, 15]) + np.random.normal(0, 1, 3)\n        \n        # Torques (small values)\n        zero_torques = np.random.normal(0, 0.1, 3)\n        \n        sensor_readings[\'left_foot\'] = (left_foot_forces, zero_torques)\n        sensor_readings[\'right_foot\'] = (right_foot_forces, zero_torques)\n        sensor_readings[\'left_hand\'] = (left_hand_forces, zero_torques)\n        sensor_readings[\'right_hand\'] = (right_hand_forces, zero_torques)\n        \n        # Update all sensors\n        ft_system.update_all_sensors(sensor_readings)\n        \n        # Check contact status\n        contact_status = ft_system.get_contact_status()\n        \n        if step % 20 == 0:\n            print(f"Step {step}:")\n            for location, contacted in contact_status.items():\n                if contacted:\n                    sensor = ft_system.sensors[location]\n                    print(f"  {location}: Contact! F={sensor.forces[:2]} N")\n            \n            support_polygon = ft_system.get_support_polygon()\n            if len(support_polygon) > 0:\n                print(f"  Support polygon: {len(support_polygon)} contact points")\n            print()\n\nif __name__ == "__main__":\n    simulate_force_torque_sensing()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"63-exteroceptive-sensors",children:"6.3 Exteroceptive Sensors"}),"\n",(0,i.jsx)(e.h3,{id:"631-vision-systems",children:"6.3.1 Vision Systems"}),"\n",(0,i.jsx)(e.p,{children:"Vision systems enable humanoid robots to perceive and understand their 3D environment."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nVision system interface for humanoid robotics\nDemonstrates object detection, tracking, and 3D reconstruction\n"""\n\nimport numpy as np\nimport cv2\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\nimport threading\nfrom collections import deque\n\n@dataclass\nclass ImageData:\n    """Image data structure"""\n    timestamp: float\n    image: np.ndarray\n    camera_intrinsic: np.ndarray  # 3x3 intrinsic matrix\n    distortion_coeffs: np.ndarray  # 4x1 distortion coefficients\n\n@dataclass\nclass DetectedObject:\n    """Detected object information"""\n    class_name: str\n    confidence: float\n    bbox: Tuple[int, int, int, int]  # x, y, w, h\n    center_3d: Optional[np.ndarray]  # [x, y, z] in world coordinates\n\nclass VisionSystem:\n    """Main vision system for humanoid robot"""\n    def __init__(self, camera_matrix: np.ndarray = None, dist_coeffs: np.ndarray = None):\n        # Camera parameters\n        if camera_matrix is None:\n            # Default camera matrix (simulated)\n            self.camera_matrix = np.array([\n                [600, 0, 320],\n                [0, 600, 240],\n                [0, 0, 1]\n            ], dtype=np.float32)\n        else:\n            self.camera_matrix = camera_matrix\n            \n        if dist_coeffs is None:\n            self.distortion_coeffs = np.zeros(4, dtype=np.float32)\n        else:\n            self.distortion_coeffs = dist_coeffs\n        \n        # Object detection parameters\n        self.confidence_threshold = 0.5\n        self.nms_threshold = 0.4  # Non-maximum suppression\n        \n        # Feature tracking\n        self.feature_params = dict(\n            maxCorners=100,\n            qualityLevel=0.3,\n            minDistance=7,\n            blockSize=7\n        )\n        \n        self.lk_params = dict(\n            winSize=(15, 15),\n            maxLevel=2,\n            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n        )\n        \n        # Previous frame for optical flow\n        self.prev_gray = None\n        self.prev_features = None\n        \n        # Object tracking\n        self.tracked_objects = {}\n        self.object_id_counter = 0\n        \n        # Thread safety\n        self.lock = threading.Lock()\n    \n    def process_image(self, image: np.ndarray) -> List[DetectedObject]:\n        """Process single image to detect and track objects"""\n        with self.lock:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            \n            # Update feature tracking\n            self._update_feature_tracking(gray, image)\n            \n            # Detect objects\n            detected_objects = self._detect_objects(image)\n            \n            # Track objects across frames\n            self._track_objects(detected_objects)\n            \n            # Estimate 3D positions\n            for obj in detected_objects:\n                obj.center_3d = self._estimate_3d_position(obj, image)\n            \n            return detected_objects\n    \n    def _detect_objects(self, image: np.ndarray) -> List[DetectedObject]:\n        """Detect objects using classical computer vision methods"""\n        # Simple color-based object detection (for demonstration)\n        # In real implementation, would use deep learning models\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        \n        detected_objects = []\n        \n        # Detect red objects\n        lower_red = np.array([0, 100, 100])\n        upper_red = np.array([10, 255, 255])\n        mask_red = cv2.inRange(hsv, lower_red, upper_red)\n        \n        # Find contours\n        contours, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 100:  # Minimum area threshold\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n                \n                # Calculate confidence based on area\n                confidence = min(0.9, area / 1000)\n                \n                if confidence > self.confidence_threshold:\n                    detected_objects.append(DetectedObject(\n                        class_name="red_object",\n                        confidence=confidence,\n                        bbox=(x, y, w, h),\n                        center_3d=None\n                    ))\n        \n        # Detect blue objects\n        lower_blue = np.array([100, 100, 100])\n        upper_blue = np.array([130, 255, 255])\n        mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n        \n        contours, _ = cv2.findContours(mask_blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 100:  # Minimum area threshold\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n                \n                # Calculate confidence based on area\n                confidence = min(0.9, area / 1000)\n                \n                if confidence > self.confidence_threshold:\n                    detected_objects.append(DetectedObject(\n                        class_name="blue_object",\n                        confidence=confidence,\n                        bbox=(x, y, w, h),\n                        center_3d=None\n                    ))\n        \n        return detected_objects\n    \n    def _update_feature_tracking(self, gray: np.ndarray, image: np.ndarray):\n        """Update feature tracking using optical flow"""\n        if self.prev_gray is None:\n            # Initialize features\n            self.prev_features = cv2.goodFeaturesToTrack(\n                gray, mask=None, **self.feature_params\n            )\n            self.prev_gray = gray.copy()\n            return\n        \n        # Calculate optical flow\n        if len(self.prev_features) > 0:\n            new_features, status, error = cv2.calcOpticalFlowPyrLK(\n                self.prev_gray, gray, self.prev_features, None, **self.lk_params\n            )\n            \n            # Select good points\n            good_new = new_features[status == 1]\n            good_old = self.prev_features[status == 1]\n            \n            # Draw optical flow\n            for i, (new, old) in enumerate(zip(good_new, good_old)):\n                a, b = new.ravel()\n                c, d = old.ravel()\n                cv2.line(image, (int(a), int(b)), (int(c), int(d)), (0, 255, 0), 2)\n                cv2.circle(image, (int(a), int(b)), 5, (0, 0, 255), -1)\n        \n        # Update for next frame\n        self.prev_gray = gray.copy()\n        self.prev_features = cv2.goodFeaturesToTrack(\n            gray, mask=None, maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7\n        )\n    \n    def _track_objects(self, detected_objects: List[DetectedObject]):\n        """Track objects across frames using simple tracking"""\n        # Simple tracking by position matching\n        for obj in detected_objects:\n            # Calculate center of bounding box\n            x, y, w, h = obj.bbox\n            center = np.array([x + w/2, y + h/2])\n            \n            # Try to match with existing tracked objects\n            matched = False\n            for tracked_id, tracked_info in self.tracked_objects.items():\n                prev_center = tracked_info[\'center\']\n                distance = np.linalg.norm(center - prev_center)\n                \n                if distance < 50:  # Threshold for matching\n                    # Update tracked object\n                    self.tracked_objects[tracked_id][\'center\'] = center\n                    self.tracked_objects[tracked_id][\'bbox\'] = obj.bbox\n                    self.tracked_objects[tracked_id][\'confidence\'] = obj.confidence\n                    matched = True\n                    break\n            \n            if not matched:\n                # Create new tracked object\n                obj_id = self.object_id_counter\n                self.object_id_counter += 1\n                self.tracked_objects[obj_id] = {\n                    \'center\': center,\n                    \'bbox\': obj.bbox,\n                    \'confidence\': obj.confidence,\n                    \'class\': obj.class_name\n                }\n    \n    def _estimate_3d_position(self, obj: DetectedObject, image: np.ndarray) -> np.ndarray:\n        """Estimate 3D position of object using camera parameters"""\n        # Calculate center of bounding box\n        x, y, w, h = obj.bbox\n        center_2d = np.array([x + w/2, y + h/2])\n        \n        # In a real implementation, would use:\n        # 1. Stereo vision or depth camera\n        # 2. Structure from motion\n        # 3. Assumptions about object size\n        \n        # For simulation, assume fixed distance and convert 2D to 3D\n        # using camera intrinsic parameters\n        z_distance = 1.0  # Fixed distance assumption\n        \n        # Convert 2D image coordinates to 3D world coordinates\n        x_world = (center_2d[0] - self.camera_matrix[0, 2]) * z_distance / self.camera_matrix[0, 0]\n        y_world = (center_2d[1] - self.camera_matrix[1, 2]) * z_distance / self.camera_matrix[1, 1]\n        \n        return np.array([x_world, y_world, z_distance])\n    \n    def get_tracked_objects_3d(self) -> dict:\n        """Get all tracked objects with 3D positions"""\n        with self.lock:\n            result = {}\n            for obj_id, obj_info in self.tracked_objects.items():\n                # Estimate 3D position using the same method as detection\n                x, y, w, h = obj_info[\'bbox\']\n                center_2d = np.array([x + w/2, y + h/2])\n                \n                z_distance = 1.0  # Fixed distance assumption\n                x_world = (center_2d[0] - self.camera_matrix[0, 2]) * z_distance / self.camera_matrix[0, 0]\n                y_world = (center_2d[1] - self.camera_matrix[1, 2]) * z_distance / self.camera_matrix[1, 1]\n                \n                result[obj_id] = {\n                    \'position\': np.array([x_world, y_world, z_distance]),\n                    \'confidence\': obj_info[\'confidence\'],\n                    \'class\': obj_info[\'class\'],\n                    \'bbox\': obj_info[\'bbox\']\n                }\n            return result\n\nclass StereoVisionSystem:\n    """Stereo vision system for depth perception"""\n    def __init__(self, baseline: float = 0.12, focal_length: float = 600.0):\n        self.baseline = baseline  # Distance between cameras (meters)\n        self.focal_length = focal_length  # Pixel focal length\n        \n        # Stereo matching parameters\n        self.stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=96,\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n    \n    def compute_depth_map(self, left_image: np.ndarray, right_image: np.ndarray) -> np.ndarray:\n        """Compute depth map from stereo pair"""\n        # Convert to grayscale\n        gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n        gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n        \n        # Compute disparity\n        disparity = self.stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0\n        \n        # Convert disparity to depth\n        # Depth = (baseline * focal_length) / disparity\n        with np.errstate(divide=\'ignore\'):\n            depth = (self.baseline * self.focal_length) / (disparity + 1e-6)\n        \n        # Set invalid values to 0\n        depth[disparity <= 0] = 0\n        \n        return depth\n    \n    def get_3d_point(self, u: int, v: int, depth_map: np.ndarray) -> np.ndarray:\n        """Convert 2D image coordinates + depth to 3D world coordinates"""\n        if v >= depth_map.shape[0] or u >= depth_map.shape[1]:\n            return np.array([0, 0, 0])\n        \n        z = depth_map[v, u]\n        if z <= 0:\n            return np.array([0, 0, 0])\n        \n        # Convert to 3D coordinates using camera parameters\n        x = (u - 320) * z / self.focal_length  # Assuming cx = 320\n        y = (v - 240) * z / self.focal_length  # Assuming cy = 240\n        \n        return np.array([x, y, z])\n\ndef simulate_vision_system():\n    """Simulate vision system operation"""\n    print("Setting up vision system...")\n    vision_system = VisionSystem()\n    \n    print("Simulating vision processing...")\n    \n    for step in range(100):\n        # Create a synthetic image with some objects\n        image = np.zeros((480, 640, 3), dtype=np.uint8)\n        \n        # Add some colored objects\n        cv2.circle(image, (200, 200), 30, (0, 0, 255), -1)  # Red circle\n        cv2.rectangle(image, (300, 150), (350, 200), (255, 0, 0), -1)  # Blue rectangle\n        cv2.circle(image, (400, 300), 20, (0, 255, 0), -1)  # Green circle\n        \n        # Add some noise\n        noise = np.random.randint(0, 20, image.shape, dtype=np.uint8)\n        image = cv2.add(image, noise)\n        \n        # Process image\n        detected_objects = vision_system.process_image(image)\n        \n        if step % 20 == 0:\n            print(f"Step {step}: Detected {len(detected_objects)} objects")\n            \n            if detected_objects:\n                for i, obj in enumerate(detected_objects[:3]):  # Show first 3\n                    print(f"  Object {i+1}: {obj.class_name} at {obj.center_3d}")\n            \n            # Show tracked objects\n            tracked = vision_system.get_tracked_objects_3d()\n            print(f"  Tracked objects: {len(tracked)}")\n    \n    print("Vision system simulation completed.")\n\nif __name__ == "__main__":\n    simulate_vision_system()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"632-lidar-systems",children:"6.3.2 LIDAR Systems"}),"\n",(0,i.jsx)(e.p,{children:"LIDAR provides precise 3D mapping and obstacle detection for humanoid robots."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nLIDAR interface for humanoid robotics\nDemonstrates 3D mapping and obstacle detection\n"""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\nimport time\nfrom collections import deque\n\n@dataclass\nclass LIDARData:\n    """LIDAR data structure"""\n    timestamp: float\n    ranges: np.ndarray  # Distance measurements\n    intensities: np.ndarray  # Intensity values\n    angles: np.ndarray  # Measurement angles\n    point_cloud: np.ndarray  # 3D point cloud [x, y, z]\n\nclass LIDARSystem:\n    """LIDAR system interface"""\n    def __init__(self, fov: float = 2 * np.pi, resolution: float = 0.01, max_range: float = 10.0):\n        self.fov = fov  # Field of view in radians\n        self.resolution = resolution  # Angular resolution in radians\n        self.max_range = max_range  # Maximum detection range\n        \n        # Calculate number of beams\n        self.num_beams = int(fov / resolution)\n        self.angles = np.linspace(0, fov, self.num_beams, endpoint=False)\n        \n        # Previous scans for filtering\n        self.scan_history = deque(maxlen=5)\n        \n        # Transformation parameters (robot to LIDAR)\n        self.lidar_offset = np.array([0.0, 0.0, 1.0])  # LIDAR is 1m above ground\n        \n        # Obstacle detection\n        self.obstacle_threshold = 0.5  # Minimum distance for obstacle\n        self.min_obstacle_size = 0.3   # Minimum size to be considered obstacle\n    \n    def process_scan(self, ranges: np.ndarray, intensities: np.ndarray = None) -> LIDARData:\n        """Process LIDAR scan data"""\n        if len(ranges) != self.num_beams:\n            raise ValueError(f"Expected {self.num_beams} beams, got {len(ranges)}")\n        \n        # Apply noise filtering to ranges\n        filtered_ranges = self._filter_scan(ranges)\n        \n        # Convert to 3D point cloud\n        point_cloud = self._ranges_to_point_cloud(filtered_ranges)\n        \n        # Store in history for temporal filtering\n        self.scan_history.append(filtered_ranges.copy())\n        \n        # Perform obstacle detection\n        obstacles = self._detect_obstacles(filtered_ranges)\n        \n        # Create result data\n        lidar_data = LIDARData(\n            timestamp=time.time(),\n            ranges=filtered_ranges,\n            intensities=intensities if intensities is not None else np.zeros_like(ranges),\n            angles=self.angles,\n            point_cloud=point_cloud\n        )\n        \n        return lidar_data, obstacles\n    \n    def _filter_scan(self, ranges: np.ndarray) -> np.ndarray:\n        """Apply basic filtering to LIDAR scan"""\n        # Remove readings beyond max range\n        ranges = np.where(ranges > self.max_range, self.max_range, ranges)\n        \n        # Simple median filtering for noise reduction\n        filtered_ranges = np.zeros_like(ranges)\n        \n        for i in range(len(ranges)):\n            # Get neighborhood\n            start_idx = max(0, i - 1)\n            end_idx = min(len(ranges), i + 2)\n            neighborhood = ranges[start_idx:end_idx]\n            \n            # Apply median filter\n            filtered_ranges[i] = np.median(neighborhood)\n        \n        return filtered_ranges\n    \n    def _ranges_to_point_cloud(self, ranges: np.ndarray) -> np.ndarray:\n        """Convert range measurements to 3D point cloud"""\n        # Convert polar to Cartesian coordinates\n        x = ranges * np.cos(self.angles)\n        y = ranges * np.sin(self.angles)\n        z = np.zeros_like(x) + self.lidar_offset[2]  # Fixed height\n        \n        # Stack to create point cloud\n        point_cloud = np.column_stack([x, y, z])\n        \n        return point_cloud\n    \n    def _detect_obstacles(self, ranges: np.ndarray) -> List[Tuple[int, float, float]]:\n        """Detect obstacles from LIDAR data"""\n        obstacles = []\n        \n        # Find consecutive points that are close (potential obstacles)\n        distance_threshold = self.obstacle_threshold\n        current_obstacle = []\n        \n        for i, distance in enumerate(ranges):\n            if distance < distance_threshold and distance > 0.1:  # Valid range\n                current_obstacle.append((i, distance))\n            else:\n                # End of potential obstacle\n                if len(current_obstacle) >= 3:  # At least 3 consecutive points\n                    # Calculate obstacle center and size\n                    angles = [self.angles[obs[0]] for obs in current_obstacle]\n                    avg_angle = np.mean(angles)\n                    avg_distance = np.mean([obs[1] for obs in current_obstacle])\n                    \n                    # Estimate obstacle size\n                    size = len(current_obstacle) * distance_threshold  # Rough estimate\n                    \n                    if size >= self.min_obstacle_size:\n                        obstacles.append((int(avg_angle / self.resolution), avg_distance, size))\n                \n                current_obstacle = []\n        \n        return obstacles\n    \n    def create_occupancy_grid(self, lidar_data: LIDARData, resolution: float = 0.1) -> np.ndarray:\n        """Create occupancy grid from LIDAR data"""\n        # Determine grid size based on max range\n        grid_size = int(2 * self.max_range / resolution)\n        grid = np.zeros((grid_size, grid_size))\n        \n        # Center of grid corresponds to robot position\n        grid_center = grid_size // 2\n        \n        # Convert point cloud to grid coordinates\n        x_points = lidar_data.point_cloud[:, 0]\n        y_points = lidar_data.point_cloud[:, 1]\n        \n        x_grid = ((x_points / resolution) + grid_center).astype(int)\n        y_grid = ((y_points / resolution) + grid_center).astype(int)\n        \n        # Mark obstacle cells\n        valid_indices = (\n            (x_grid >= 0) & (x_grid < grid_size) & \n            (y_grid >= 0) & (y_grid < grid_size)\n        )\n        \n        x_valid = x_grid[valid_indices]\n        y_valid = y_grid[valid_indices]\n        \n        grid[x_valid, y_valid] = 1.0  # Occupied\n        \n        return grid\n    \n    def get_free_space(self, lidar_data: LIDARData) -> Tuple[float, float, float]:\n        """Get available free space in front, left, right directions"""\n        front_idx = slice(int(0.4 * self.num_beams), int(0.6 * self.num_beams))\n        left_idx = slice(int(0.15 * self.num_beams), int(0.35 * self.num_beams))\n        right_idx = slice(int(0.65 * self.num_beams), int(0.85 * self.num_beams))\n        \n        front_distances = lidar_data.ranges[front_idx]\n        left_distances = lidar_data.ranges[left_idx]\n        right_distances = lidar_data.ranges[right_idx]\n        \n        front_free = np.min(front_distances)\n        left_free = np.min(left_distances)\n        right_free = np.min(right_distances)\n        \n        return front_free, left_free, right_free\n\nclass HumanoidNavigationSystem:\n    """Navigation system using LIDAR data"""\n    def __init__(self):\n        self.lidar_system = LIDARSystem()\n        self.path_history = deque(maxlen=100)\n        self.goal_position = None\n        \n    def set_goal(self, x: float, y: float):\n        """Set navigation goal position"""\n        self.goal_position = np.array([x, y])\n    \n    def process_navigation(self, lidar_data: LIDARData):\n        """Process LIDAR data for navigation"""\n        if self.goal_position is None:\n            return "stop"  # No goal set\n        \n        # Get free space information\n        front_free, left_free, right_free = self.lidar_system.get_free_space(lidar_data)\n        \n        # Calculate direction to goal\n        current_pos = np.array([0, 0])  # Robot is at origin in LIDAR frame\n        goal_direction = self.goal_position - current_pos\n        goal_angle = np.arctan2(goal_direction[1], goal_direction[0])\n        \n        # Simple navigation logic\n        if front_free < 0.8:\n            # Obstacle ahead\n            if left_free > right_free:\n                return "turn_left"\n            else:\n                return "turn_right"\n        elif abs(goal_angle) > 0.3:\n            # Need to turn toward goal\n            if goal_angle > 0:\n                return "turn_left"\n            else:\n                return "turn_right"\n        else:\n            # Clear path toward goal\n            return "forward"\n\ndef simulate_lidar_navigation():\n    """Simulate LIDAR-based navigation"""\n    print("Setting up LIDAR navigation system...")\n    nav_system = HumanoidNavigationSystem()\n    nav_system.set_goal(5.0, 0.0)  # Goal 5m ahead\n    \n    print("Simulating LIDAR navigation...")\n    \n    for step in range(100):\n        # Simulate LIDAR scan with some obstacles\n        ranges = np.full(628, 10.0)  # 628 beams for 2*pi fov at 0.01 rad resolution\n        \n        # Add some obstacles\n        if 250 < step < 350:  # Simulate hallway with obstacle\n            # Add obstacle on the right\n            obstacle_start = int(471)  # ~3*pi/2 angle\n            obstacle_end = int(524)    # ~5*pi/3 angle\n            ranges[obstacle_start:obstacle_end] = 0.5  # 0.5m distance\n        \n        # Add some random noise\n        ranges += np.random.normal(0, 0.02, len(ranges))\n        ranges = np.clip(ranges, 0.1, 10.0)  # Valid range 0.1-10m\n        \n        # Process scan\n        lidar_data, obstacles = nav_system.lidar_system.process_scan(ranges)\n        \n        # Get navigation command\n        command = nav_system.process_navigation(lidar_data)\n        \n        if step % 20 == 0:\n            front, left, right = nav_system.lidar_system.get_free_space(lidar_data)\n            print(f"Step {step}: Command={command}, Free space - F:{front:.2f}, L:{left:.2f}, R:{right:.2f}")\n            print(f"  Detected obstacles: {len(obstacles)}")\n    \n    print("LIDAR navigation simulation completed.")\n\ndef visualize_lidar_data():\n    """Visualize LIDAR point cloud data"""\n    lidar_system = LIDARSystem()\n    \n    # Create sample scan with obstacle\n    ranges = np.full(628, 5.0)  # 5m in all directions\n    \n    # Add an obstacle\n    obstacle_start = 300\n    obstacle_end = 320\n    ranges[obstacle_start:obstacle_end] = 1.0  # 1m distance\n    \n    # Process scan\n    lidar_data, obstacles = lidar_system.process_scan(ranges)\n    \n    # Create visualization\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Plot point cloud\n    ax1.scatter(lidar_data.point_cloud[:, 0], lidar_data.point_cloud[:, 1], s=1, alpha=0.6)\n    ax1.set_xlim(-6, 6)\n    ax1.set_ylim(-6, 6)\n    ax1.set_aspect(\'equal\')\n    ax1.grid(True, alpha=0.3)\n    ax1.set_title(\'LIDAR Point Cloud\')\n    ax1.set_xlabel(\'X (m)\')\n    ax1.set_ylabel(\'Y (m)\')\n    \n    # Plot range scan\n    ax2.plot(np.degrees(lidar_data.angles), lidar_data.ranges)\n    ax2.set_xlabel(\'Angle (degrees)\')\n    ax2.set_ylabel(\'Distance (m)\')\n    ax2.set_title(\'LIDAR Range Scan\')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nif __name__ == "__main__":\n    simulate_lidar_navigation()\n    visualize_lidar_data()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"64-sensor-fusion-and-integration",children:"6.4 Sensor Fusion and Integration"}),"\n",(0,i.jsx)(e.h3,{id:"641-kalman-filter-for-sensor-fusion",children:"6.4.1 Kalman Filter for Sensor Fusion"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSensor fusion using Extended Kalman Filter for humanoid robotics\nCombines IMU, vision, and encoders for robust state estimation\n"""\n\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Tuple\nimport time\n\n@dataclass\nclass RobotState:\n    """Robot state vector: [x, y, theta, vx, vy, omega]"""\n    position: np.ndarray  # [x, y]\n    orientation: float    # theta\n    velocity: np.ndarray  # [vx, vy]\n    angular_velocity: float  # omega\n    covariance: np.ndarray  # State covariance matrix\n\nclass ExtendedKalmanFilter:\n    """Extended Kalman Filter for robot state estimation"""\n    def __init__(self):\n        # State vector: [x, y, theta, vx, vy, omega]\n        self.state_dim = 6\n        self.state = np.zeros(self.state_dim)\n        \n        # Initial covariance\n        self.P = np.eye(self.state_dim) * 0.1\n        \n        # Process noise\n        self.Q = np.diag([0.01, 0.01, 0.001, 0.1, 0.1, 0.01])\n        \n        # Measurement noise for different sensors\n        self.R_imu = np.diag([0.01, 0.01, 0.001])  # [ax, ay, omega]\n        self.R_vision = np.diag([0.1, 0.1, 0.01])  # [x, y, theta]\n        self.R_encoders = np.diag([0.05, 0.05])     # [vx, vy]\n    \n    def predict(self, dt: float, control_input: np.ndarray = None):\n        """Prediction step of EKF"""\n        # State transition model (constant velocity model with orientation)\n        theta = self.state[2]\n        vx = self.state[3]\n        vy = self.state[4]\n        \n        # Update state based on motion model\n        self.state[0] += (vx * np.cos(theta) - vy * np.sin(theta)) * dt\n        self.state[1] += (vx * np.sin(theta) + vy * np.cos(theta)) * dt\n        self.state[2] += self.state[5] * dt  # Update orientation from angular velocity\n        # Velocities remain approximately constant in simple model\n        \n        # Jacobian of state transition\n        F = np.eye(self.state_dim)\n        F[0, 2] = (-vx * np.sin(theta) - vy * np.cos(theta)) * dt\n        F[0, 3] = np.cos(theta) * dt\n        F[0, 4] = -np.sin(theta) * dt\n        \n        F[1, 2] = (vx * np.cos(theta) - vy * np.sin(theta)) * dt\n        F[1, 3] = np.sin(theta) * dt\n        F[1, 4] = np.cos(theta) * dt\n        \n        F[2, 5] = dt\n        \n        # Predict covariance\n        self.P = F @ self.P @ F.T + self.Q\n    \n    def update_with_imu(self, measurement: np.ndarray):\n        """Update with IMU measurement [ax, ay, omega]"""\n        # Measurement model: extract acceleration and angular velocity from state\n        # For this example, we\'ll use a simplified model\n        H = np.zeros((3, self.state_dim))\n        H[2, 5] = 1  # Measure angular velocity directly\n        \n        # Expected measurement\n        expected = np.array([0, 0, self.state[5]])  # [ax_est, ay_est, omega]\n        \n        # Innovation\n        y = measurement - expected\n        \n        # Innovation covariance\n        S = H @ self.P @ H.T + self.R_imu\n        \n        # Kalman gain\n        K = self.P @ H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.state += K @ y\n        self.P = (np.eye(self.state_dim) - K @ H) @ self.P\n    \n    def update_with_vision(self, measurement: np.ndarray):\n        """Update with vision measurement [x, y, theta] - position and orientation"""\n        H = np.zeros((3, self.state_dim))\n        H[0, 0] = 1  # Measure x position\n        H[1, 1] = 1  # Measure y position  \n        H[2, 2] = 1  # Measure orientation\n        \n        # Expected measurement\n        expected = self.state[0:3]  # [x, y, theta]\n        \n        # Innovation\n        y = measurement - expected\n        \n        # Innovation covariance\n        S = H @ self.P @ H.T + self.R_vision\n        \n        # Kalman gain\n        K = self.P @ H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.state += K @ y\n        self.P = (np.eye(self.state_dim) - K @ H) @ self.P\n    \n    def update_with_encoders(self, measurement: np.ndarray):\n        """Update with encoder measurement [vx, vy] - linear velocities"""\n        H = np.zeros((2, self.state_dim))\n        H[0, 3] = 1  # Measure vx\n        H[1, 4] = 1  # Measure vy\n        \n        # Expected measurement\n        expected = self.state[3:5]  # [vx, vy]\n        \n        # Innovation\n        y = measurement - expected\n        \n        # Innovation covariance\n        S = H @ self.P @ H.T + self.R_encoders\n        \n        # Kalman gain\n        K = self.P @ H.T @ np.linalg.inv(S)\n        \n        # Update state and covariance\n        self.state += K @ y\n        self.P = (np.eye(self.state_dim) - K @ H) @ self.P\n    \n    def get_robot_state(self) -> RobotState:\n        """Get current robot state"""\n        return RobotState(\n            position=self.state[0:2],\n            orientation=self.state[2],\n            velocity=self.state[3:4],\n            angular_velocity=self.state[5],\n            covariance=self.P\n        )\n\nclass MultiSensorFusion:\n    """Multi-sensor fusion system for humanoid robot"""\n    def __init__(self):\n        self.ekf = ExtendedKalmanFilter()\n        self.last_update_time = time.time()\n        \n        # Sensor data buffers\n        self.imu_buffer = []\n        self.vision_buffer = []\n        self.encoder_buffer = []\n        \n        # Update rates\n        self.imu_rate = 100  # Hz\n        self.vision_rate = 30  # Hz\n        self.encoder_rate = 50  # Hz\n    \n    def process_imu_data(self, accel: np.ndarray, gyro: np.ndarray, dt: float):\n        """Process IMU data and update state estimate"""\n        current_time = time.time()\n        \n        # Combine accelerometer and gyroscope data\n        measurement = np.concatenate([accel[0:2], [gyro[2]]])  # [ax, ay, omega_z]\n        \n        # Update EKF\n        self.ekf.predict(dt)\n        self.ekf.update_with_imu(measurement)\n        \n        self.last_update_time = current_time\n    \n    def process_vision_data(self, position: np.ndarray, orientation: float, dt: float):\n        """Process vision data and update state estimate"""\n        current_time = time.time()\n        \n        # Create measurement vector\n        measurement = np.array([position[0], position[1], orientation])\n        \n        # Update EKF\n        self.ekf.predict(dt)\n        self.ekf.update_with_vision(measurement)\n        \n        self.last_update_time = current_time\n    \n    def process_encoder_data(self, velocity: np.ndarray, dt: float):\n        """Process encoder data and update state estimate"""\n        current_time = time.time()\n        \n        # Create measurement vector\n        measurement = velocity  # [vx, vy]\n        \n        # Update EKF\n        self.ekf.predict(dt)\n        self.ekf.update_with_encoders(measurement)\n        \n        self.last_update_time = current_time\n    \n    def get_fused_state(self) -> RobotState:\n        """Get the fused state estimate"""\n        return self.ekf.get_robot_state()\n\ndef simulate_sensor_fusion():\n    """Simulate multi-sensor fusion"""\n    print("Simulating sensor fusion...")\n    \n    fusion_system = MultiSensorFusion()\n    \n    dt = 0.01  # 100Hz simulation\n    \n    # Simulate robot moving in a circle\n    for step in range(1000):\n        current_time = step * dt\n        \n        # Simulate robot motion (circular path)\n        true_pos = np.array([2 * np.cos(0.5 * current_time), 2 * np.sin(0.5 * current_time)])\n        true_vel = np.array([-np.sin(0.5 * current_time), np.cos(0.5 * current_time)]) * 1.0\n        true_theta = 0.5 * current_time  # Heading angle\n        true_omega = 0.5  # Angular velocity\n        \n        # Add noise to simulate real sensors\n        imu_accel = np.array([\n            -0.5 * np.sin(0.5 * current_time), \n            0.5 * np.cos(0.5 * current_time)\n        ]) + np.random.normal(0, 0.01, 2)\n        \n        imu_gyro = np.array([0, 0, true_omega]) + np.random.normal(0, 0.001, 3)\n        \n        vision_pos = true_pos + np.random.normal(0, 0.05, 2)\n        vision_theta = true_theta + np.random.normal(0, 0.01)\n        \n        encoder_vel = true_vel + np.random.normal(0, 0.02, 2)\n        \n        # Process sensor data\n        fusion_system.process_imu_data(imu_accel, imu_gyro, dt)\n        if step % 3 == 0:  # Vision at 33Hz\n            fusion_system.process_vision_data(vision_pos, vision_theta, dt)\n        if step % 2 == 0:  # Encoders at 50Hz\n            fusion_system.process_encoder_data(encoder_vel, dt)\n        \n        # Get fused state\n        fused_state = fusion_system.get_fused_state()\n        \n        if step % 100 == 0:\n            pos_error = np.linalg.norm(fused_state.position - true_pos)\n            vel_error = np.linalg.norm(fused_state.velocity - true_vel)\n            \n            print(f"Step {step}:")\n            print(f"  True: pos={true_pos}, vel={true_vel}")\n            print(f"  Fused: pos={fused_state.position}, vel={fused_state.velocity}")\n            print(f"  Error: pos={pos_error:.3f}, vel={vel_error:.3f}")\n            print()\n\nif __name__ == "__main__":\n    simulate_sensor_fusion()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"65-ros2-integration-for-sensor-systems",children:"6.5 ROS2 Integration for Sensor Systems"}),"\n",(0,i.jsx)(e.h3,{id:"651-sensor-data-publishing",children:"6.5.1 Sensor Data Publishing"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nROS2 sensor interface for humanoid robotics\nPublishes sensor data for the perception pipeline\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Imu, PointCloud2, PointField\nfrom geometry_msgs.msg import Vector3\nfrom std_msgs.msg import Header\nimport numpy as np\nimport time\nfrom collections import deque\n\nclass HumanoidSensorNode(Node):\n    def __init__(self):\n        super().__init__('humanoid_sensor_node')\n        \n        # Publishers\n        self.joint_state_pub = self.create_publisher(JointState, '/joint_states', 10)\n        self.imu_pub = self.create_publisher(Imu, '/imu/data', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, '/laser_cloud', 10)\n        \n        # Timer for publishing\n        self.timer = self.create_timer(0.01, self.publish_sensor_data)  # 100Hz\n        \n        # Joint names for humanoid robot\n        self.joint_names = [\n            'left_hip_roll', 'left_hip_yaw', 'left_hip_pitch',\n            'left_knee', 'left_ankle_pitch', 'left_ankle_roll',\n            'right_hip_roll', 'right_hip_yaw', 'right_hip_pitch',\n            'right_knee', 'right_ankle_pitch', 'right_ankle_roll',\n            'torso_yaw', 'torso_pitch', 'torso_roll',\n            'left_shoulder_pitch', 'left_shoulder_roll', 'left_shoulder_yaw',\n            'left_elbow', 'left_wrist_pitch', 'left_wrist_yaw',\n            'right_shoulder_pitch', 'right_shoulder_roll', 'right_shoulder_yaw',\n            'right_elbow', 'right_wrist_pitch', 'right_wrist_yaw',\n            'neck_yaw', 'neck_pitch'\n        ]\n        \n        # Initialize joint positions\n        self.joint_positions = np.zeros(len(self.joint_names))\n        self.joint_velocities = np.zeros(len(self.joint_names))\n        self.joint_efforts = np.zeros(len(self.joint_names))\n        \n        # IMU state\n        self.orientation = [0.0, 0.0, 0.0, 1.0]  # [x, y, z, w]\n        self.angular_velocity = [0.0, 0.0, 0.0]\n        self.linear_acceleration = [0.0, 0.0, 0.0]\n        \n        # Point cloud simulation\n        self.point_cloud = self._generate_sample_pointcloud()\n        \n        self.get_logger().info('Humanoid Sensor Node Started')\n\n    def _generate_sample_pointcloud(self):\n        \"\"\"Generate sample point cloud data\"\"\"\n        # Create a simple point cloud with some objects\n        points = []\n        \n        # Ground plane\n        for i in range(10):\n            for j in range(10):\n                x = i * 0.5 - 2.5\n                y = j * 0.5 - 2.5\n                z = -1.0\n                points.extend([x, y, z])\n        \n        # Add an obstacle\n        for i in range(5):\n            for j in range(5):\n                x = 2.0\n                y = i * 0.2 - 0.4\n                z = j * 0.2\n                points.extend([x, y, z])\n        \n        return np.array(points).astype(np.float32)\n\n    def publish_sensor_data(self):\n        \"\"\"Publish all sensor data\"\"\"\n        # Update joint positions with some movement\n        t = self.get_clock().now().nanoseconds / 1e9\n        for i in range(len(self.joint_names)):\n            self.joint_positions[i] = 0.1 * np.sin(t + i * 0.1)\n            self.joint_velocities[i] = 0.1 * np.cos(t + i * 0.1)\n            self.joint_efforts[i] = 0.05 * np.sin(2 * t + i * 0.1)\n        \n        # Update IMU data\n        self.orientation[3] = np.cos(t * 0.5)  # w component\n        self.angular_velocity[2] = 0.5 * np.sin(t * 0.5)  # z rotation\n        self.linear_acceleration[0] = 0.5 * np.cos(t)  # x acceleration\n        \n        # Publish joint states\n        joint_msg = JointState()\n        joint_msg.header = Header()\n        joint_msg.header.stamp = self.get_clock().now().to_msg()\n        joint_msg.header.frame_id = 'base_link'\n        joint_msg.name = self.joint_names\n        joint_msg.position = self.joint_positions.tolist()\n        joint_msg.velocity = self.joint_velocities.tolist()\n        joint_msg.effort = self.joint_efforts.tolist()\n        \n        self.joint_state_pub.publish(joint_msg)\n        \n        # Publish IMU data\n        imu_msg = Imu()\n        imu_msg.header = Header()\n        imu_msg.header.stamp = self.get_clock().now().to_msg()\n        imu_msg.header.frame_id = 'imu_link'\n        imu_msg.orientation.x = self.orientation[0]\n        imu_msg.orientation.y = self.orientation[1]\n        imu_msg.orientation.z = self.orientation[2]\n        imu_msg.orientation.w = self.orientation[3]\n        imu_msg.angular_velocity.x = self.angular_velocity[0]\n        imu_msg.angular_velocity.y = self.angular_velocity[1]\n        imu_msg.angular_velocity.z = self.angular_velocity[2]\n        imu_msg.linear_acceleration.x = self.linear_acceleration[0]\n        imu_msg.linear_acceleration.y = self.linear_acceleration[1]\n        imu_msg.linear_acceleration.z = self.linear_acceleration[2]\n        \n        self.imu_pub.publish(imu_msg)\n        \n        # Publish point cloud\n        pc_msg = PointCloud2()\n        pc_msg.header = Header()\n        pc_msg.header.stamp = self.get_clock().now().to_msg()\n        pc_msg.header.frame_id = 'laser_frame'\n        pc_msg.height = 1\n        pc_msg.width = len(self.point_cloud) // 3\n        pc_msg.fields = [\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n        pc_msg.is_bigendian = False\n        pc_msg.point_step = 12\n        pc_msg.row_step = 12 * len(self.point_cloud) // 3\n        pc_msg.is_dense = True\n        pc_msg.data = self.point_cloud.tobytes()\n        \n        self.pointcloud_pub.publish(pc_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_node = HumanoidSensorNode()\n    \n    try:\n        rclpy.spin(sensor_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"66-laboratory-exercises",children:"6.6 Laboratory Exercises"}),"\n",(0,i.jsx)(e.h3,{id:"lab-exercise-1-imu-calibration-and-orientation-estimation",children:"Lab Exercise 1: IMU Calibration and Orientation Estimation"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Objective"}),": Calibrate IMU sensors and implement orientation estimation using sensor fusion."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Equipment Required"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Inertial Measurement Unit (IMU)"}),"\n",(0,i.jsx)(e.li,{children:"Microcontroller or single-board computer"}),"\n",(0,i.jsx)(e.li,{children:"Computer with visualization software"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement accelerometer bias estimation"}),"\n",(0,i.jsx)(e.li,{children:"Implement gyroscope drift correction"}),"\n",(0,i.jsx)(e.li,{children:"Develop a complementary filter for orientation estimation"}),"\n",(0,i.jsx)(e.li,{children:"Test with different movement patterns"}),"\n",(0,i.jsx)(e.li,{children:"Analyze accuracy and drift over time"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"lab-exercise-2-vision-based-object-tracking",children:"Lab Exercise 2: Vision-Based Object Tracking"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Objective"}),": Implement real-time object tracking using camera vision."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Set up camera interface with proper calibration"}),"\n",(0,i.jsx)(e.li,{children:"Implement color-based object detection"}),"\n",(0,i.jsx)(e.li,{children:"Develop tracking algorithm using optical flow"}),"\n",(0,i.jsx)(e.li,{children:"Test with moving objects in various lighting conditions"}),"\n",(0,i.jsx)(e.li,{children:"Measure tracking accuracy and latency"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"lab-exercise-3-lidar-based-navigation",children:"Lab Exercise 3: LIDAR-based Navigation"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Objective"}),": Implement obstacle detection and path planning using LIDAR data."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Implementation Steps"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Interface with LIDAR sensor to acquire scan data"}),"\n",(0,i.jsx)(e.li,{children:"Implement point cloud processing for obstacle detection"}),"\n",(0,i.jsx)(e.li,{children:"Create occupancy grid mapping"}),"\n",(0,i.jsx)(e.li,{children:"Implement simple navigation algorithm"}),"\n",(0,i.jsx)(e.li,{children:"Test navigation in simulated or real environment"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"67-sensor-selection-and-integration-guidelines",children:"6.7 Sensor Selection and Integration Guidelines"}),"\n",(0,i.jsx)(e.h3,{id:"671-requirements-analysis",children:"6.7.1 Requirements Analysis"}),"\n",(0,i.jsx)(e.p,{children:"When selecting sensors for humanoid robots, consider:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Performance Requirements"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Update rate and latency requirements"}),"\n",(0,i.jsx)(e.li,{children:"Accuracy and precision needs"}),"\n",(0,i.jsx)(e.li,{children:"Operating range and field of view"}),"\n",(0,i.jsx)(e.li,{children:"Environmental conditions (temperature, humidity, lighting)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Integration Considerations"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Mounting location and accessibility"}),"\n",(0,i.jsx)(e.li,{children:"Wiring and communication requirements"}),"\n",(0,i.jsx)(e.li,{children:"Power consumption and thermal management"}),"\n",(0,i.jsx)(e.li,{children:"Safety and reliability requirements"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"672-sensor-redundancy-and-safety",children:"6.7.2 Sensor Redundancy and Safety"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Critical functions should have redundant sensors"}),"\n",(0,i.jsx)(e.li,{children:"Cross-validate sensor readings for consistency"}),"\n",(0,i.jsx)(e.li,{children:"Implement sensor health monitoring"}),"\n",(0,i.jsx)(e.li,{children:"Design graceful degradation when sensors fail"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"68-summary",children:"6.8 Summary"}),"\n",(0,i.jsx)(e.p,{children:"Sensor systems are fundamental to humanoid robotics, providing the information necessary for perception, navigation, and interaction. This chapter has covered:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Proprioceptive sensors (encoders, IMU, force/torque) for state monitoring"}),"\n",(0,i.jsx)(e.li,{children:"Exteroceptive sensors (vision, LIDAR) for environment perception"}),"\n",(0,i.jsx)(e.li,{children:"Sensor fusion techniques for robust state estimation"}),"\n",(0,i.jsx)(e.li,{children:"Integration approaches using ROS2 frameworks"}),"\n",(0,i.jsx)(e.li,{children:"Practical implementation examples with code"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"The quality and integration of sensors directly impacts the robot's ability to operate safely and effectively in human environments."}),"\n",(0,i.jsx)(e.h2,{id:"69-discussion-questions",children:"6.9 Discussion Questions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"How does sensor redundancy improve the reliability of humanoid robots?"}),"\n",(0,i.jsx)(e.li,{children:"What are the key challenges in fusing data from multiple sensor types?"}),"\n",(0,i.jsx)(e.li,{children:"How do environmental conditions affect sensor performance in humanoid robotics?"}),"\n",(0,i.jsx)(e.li,{children:"What safety considerations are important when designing sensor systems?"}),"\n",(0,i.jsx)(e.li,{children:"How might emerging sensor technologies change humanoid robotics in the future?"}),"\n"]})]})}function _(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);